[
  {
    "objectID": "containers/index.html",
    "href": "containers/index.html",
    "title": "Containers",
    "section": "",
    "text": "Similar to virtual machines, containers are a form of virtualization that package software and the multiple dependencies required to run that software into a single, distributable environment. That container can then be run on any system that is running the appropriate engine (Docker, Podman, etc.). They are often lighter and more agile than virtual machines but can include increased security risks if not deployed correctly. In general, virtual environments managed using tools like Conda as discussed in ‘Getting started with Conda, Virtual Environments, and Python’ should provide sufficient isolation for most geospatial computing cases without the need to familiarize oneself with container technologies, but containers can be a helpful tool in cases when neither virtual environments or virtual machines are feasible.\nFor examples on leveraging containers to run highly-customized development environments with Jupyter, RStudio, or VS Code, see Using Containers for Development Environments.\nIt is important to note that most containers can only run in Linux environments. Each of the container systems listed below include tools and steps for setting up simple, Linux environments from which containers can be ran on either Windows or Mac machines.\nAdditionally, a large majority of container images are built with a simplified Linux distribution (often Ubuntu) as their base image. While some images have been built with Windows Server as their base image, it is important to note that these images cannot and likely never will run a desktop environment and/or graphical user interface (GUI)."
  },
  {
    "objectID": "containers/index.html#container-images",
    "href": "containers/index.html#container-images",
    "title": "Containers",
    "section": "Container Images",
    "text": "Container Images\nThere are many open container registries, where you can find publicly shared container images. Docker Hub is one of the most popular options and can be a useful tool when searching for images. In general, you should be careful when selecting images and be sure to select images that are well-maintained. If you need a very minimal environment with Conda/Mamba, Conda-forge’s mambaforge image can be a good place to start as you’ll be able to install packages with APT or Mamba.\n\nPython\n\nJupyter - Minimal Notebook: A Jupyter server running JupyterLab and a Python kernel. No geospatial packages are included in this environment, but it is an excellent base image for creating customized Jupyter environments.\nGDS_ENV - GDS_PY: Built on top of Jupyter - Minimal Notebook, this image includes a large selection of Python geospatial packages.\n\n\n\nR\n\nJupyter - R Notebook: Similarly built from Jupyter - Minimal Notebook, this image includes an R kernel and a few R packages. Custom R environments with a preference towards JupyterLab should use this as their base image.\nGDS_ENV - GDS: Built on top of GDS_ENV - GDS_PY, this image includes an R kernel and a wide selection of R geospatial packages.\nRocker - Geospatial: Swapping JupyterLab for RStudio, this container image includes a range of R packages with a heavy focus on geospatial research.\n\n\n\nOthers\n\nPostGIS\nGeoServer\nQGIS Server"
  },
  {
    "objectID": "containers/index.html#engines",
    "href": "containers/index.html#engines",
    "title": "Containers",
    "section": "Engines",
    "text": "Engines\n\nApptainer (formerly Singularity)\nDeveloped specifically for academic and high performance computing, Apptainer avoids some of the security pitfalls of other engines, like Docker, at the expense of certain functionalities. Apptainer can only run a specific container format, but it provides all the necessary tools to complete conversions.\n\nApptainer Documentation\nRunning Apptainer on an Alliance cluster\nRunning Apptainer on UBC ARCSockeye\n\nFor a few simple examples on using and running Apptainer, see High Performance Computing (HPC).\n\n\nPodman\nSimilar to Apptainer, Podman avoids some of the security pitfalls found in Docker, but unlike Apptainer, Podman provides a more general-purpose container engine. When developing containers, Podman can provide a more flexible and feature-rich set of tooling compared to Apptainer. A common workflow may include building containers with Podman and then converting them to Apptainer containers prior to running in an HPC environment.\n\nPodman Documentation\nPodman in Action\n\n\n\nDocker\nDocker is by far the most popular container system, so similar to Podman, it can provide a helpful tool for developing and running containers on your local machine. If you are still new to managing and running containers, Docker can be a good system to get started with because of the large collection of resources and tools that have been built around it.\n\nUBC Library Research Commons - Introduction to Docker\nDocker Documentation\nDocker: Up and Running\nPractical Docker with Python\nDocker Deep Dive"
  },
  {
    "objectID": "high-performance-computing/index.html",
    "href": "high-performance-computing/index.html",
    "title": "High Performance Computing",
    "section": "",
    "text": "High performance computing can provide a powerful solution when working with incredibly large datasets as they allow you and your collaborators to run scripts and programs over those datasets without facing limitations in your hardware and network speeds. UBC’s Advanced Research Computing provides access to high performance computing via their Sockeye compute cluster along with secure and redundant digital object storage via Chinook. Additionally, the Canada-based organization, Digital Research Alliance, provides compute clusters across Canada, including one located at Simon Fraser University."
  },
  {
    "objectID": "high-performance-computing/index.html#ubc-advanced-research-computing-sockeye",
    "href": "high-performance-computing/index.html#ubc-advanced-research-computing-sockeye",
    "title": "High Performance Computing",
    "section": "UBC Advanced Research Computing Sockeye",
    "text": "UBC Advanced Research Computing Sockeye\nDocumentation: https://confluence.it.ubc.ca/display/UARC/Using+Sockeye\nSupported Software: https://confluence.it.ubc.ca/display/UARC/Software\n\nNote: Linux (CentOS) software can be installed along with conda packages or any software available within an Apptainer container"
  },
  {
    "objectID": "high-performance-computing/index.html#digital-research-alliance-clusters",
    "href": "high-performance-computing/index.html#digital-research-alliance-clusters",
    "title": "High Performance Computing",
    "section": "Digital Research Alliance Clusters",
    "text": "Digital Research Alliance Clusters\nDocumentation: https://docs.alliancecan.ca/wiki/Getting_started\nSupported Software: https://docs.alliancecan.ca/wiki/Available_software\n\nNote: Additionally, any software that is supported on Linux can run within an Apptainer container"
  },
  {
    "objectID": "high-performance-computing/index.html#qgis",
    "href": "high-performance-computing/index.html#qgis",
    "title": "High Performance Computing",
    "section": "QGIS",
    "text": "QGIS\nQGIS can run on either UBC Sockeye or Alliance clusters. To interact with QGIS through a graphical user interface, you’ll need to run in it within an interactive job with X11 forwarding enabled. You can find more information on creating graphical interactive jobs on UBC Sockeye here, while documentation for Alliance clusters is listed below.\n\nAlliance Documentation - QGIS"
  },
  {
    "objectID": "high-performance-computing/index.html#jupyter",
    "href": "high-performance-computing/index.html#jupyter",
    "title": "High Performance Computing",
    "section": "Jupyter",
    "text": "Jupyter\n\nAlliance Documentation - JupyterHub\nClusters Running JupyterHub (requires an Alliance account):\n\nCedar (SFU)\nGraham (UW)\nBéluga (McGill)\nNarval (UQ)\n\nSupported Kernels (Programming Languages):\n\nPython 2.7, 3.6, 3.7, 3.8, 3.9 (default), 3.10, and 3.11\nR 4.2\nJulia 1.5 & 1.8\nNote: Other kernels can be supported using Apptainer containers.\n\nServer Resource Options:\n\nTime (Session Length): 30 minutes - 5 hours\nNumber of (CPU) Cores: 1 - 8\nMemory (Total Session Limit): 1000MB - 63000MB\n(Optional) GPU Configuration: none - 4 x V100L\n\nNote: Nodes do not have internet access, so installing packages from external sources, cloning repositories from GitHub, or running code that downloads data from external sources will not work. You can only install packages from Alliance’s maintained set of Python wheels. To run JupyterLab with conda or external libraries held on PYPI, you’ll need to run it using an Apptainer container as documented in the alternative approaches below.\nAlternatives Approaches for Running JupyterLab:\n\nAlliance - Instructions for running JupyterLab via a virtual environment\nUBC ARC Sockeye - Instructions for running JupyterLab via an Apptainer container\n\n\n\nRunning a Jupyter Kernel from a Container\nContainers enable you to pre-build customized environments from which you can run software that is not supported on the Alliance clusters, and they can be leveraged to extend the Alliance’s JupyterHub clusters by running them as a custom Jupyter kernels. This can be extraordinarily useful considering Conda is not supported on the cluster.\nBefore using this approach, double-check that the software you need is not already available on the cluster either as modules or Python wheels. Software installed within a container is going to perform less efficiently than equivalent modules on Alliance, which have been optimized for HPC.\n\nCreating the container environment with Mamba (Conda)\nWhile this can be done within the Alliance login node, it is highly recommended that the containers are built from another Linux machine and copied onto the cluster. This will ensure the build runs a bit faster and you aren’t competing for resources with other researchers on the login node.\nNote: Whenever you start a new Jupyter server on the cluster, you’ll need to load the apptainer module again in order for the kernel to run.\nIf you need flexibility in your container, you might be able to create a sandbox environment from your uploaded container and modify the sandbox container from your terminal with conda/mamba.\n\nEnsure Apptainer is installed on the Linux machine\n$ apptainer --version\nIf Apptainer is not installed, you can use the following steps on Debian or Ubuntu\n$ sudo apt update\n$ sudo apt install -y software-properties-common\n$ sudo add-apt-repository -y ppa:apptainer/ppa\n$ sudo apt update\n$ sudo apt install -y apptainer\nPull the conda-forge/mambaforge Docker container from Docker Hub and build a sandbox container from it. This will enable you to use Mamba, an alternative to Conda that runs more efficiently in environments where resources are limited.\n$ apptainer build --sandbox &lt;container_name&gt;/ docker://condaforge/mambaforge\nStart a Bash shell within the sandbox container with write privileges.\n$ apptainer shell --shell /bin/bash --writable &lt;container_name&gt;\nUpdate Mamba.\nApptainer&gt; mamba update mamba -y\nUse Mamba to install the kernel package needed for your preferred programming language (Python: ipykernel; R: r-irkernel) and any other packages you need.\nApptainer&gt; mamba install &lt;kernel_package&gt; &lt;other_packages&gt; -y\nExit the sandbox container shell\nApptainer&gt; exit\nBuild your Apptainer container from the sandbox container. Including a time stamp with &lt;container_name&gt; is recommended when building a container as you may want to rebuild the container with updated packages in the future.\n$ apptainer build &lt;container_name&gt;_&lt;timestamp&gt;.sif &lt;container_name&gt;\n\n\n\nInstalling a Python-Based Container as a Kernel\n\nStart a server on the JupyterHub cluster.\nStart a new terminal.\nLoad the apptainer module.\n$ load module apptainer\nClose the terminal and start a new one.\nInstall the container as a Python kernel\n$ python -m ipykernel install --user --name &lt;container_name&gt; --display-name=\"Python (&lt;container_name&gt;)\"\nThe previous step provided an initial configuration for the kernel, but we will need to slightly modify it to ensure the kernel executes from the container.\n$ nano /home/&lt;username&gt;/.local/share/jupyter/kernels/&lt;container_name&gt;/kernel.json\nModify kernel.json file to match the following:\n{\n  \"argv\": [\n    \"apptainer\",\n    \"exec\",\n    \"/home/&lt;username&gt;/&lt;container_name&gt;.sif\",\n    \"python\",\n    \"-m\",\n    \"ipykernel_launcher\",\n    \"-f\",\n    \"{connection_file}\"\n  ],\n  \"display_name\": \"Python (&lt;container_name&gt;)\",\n  \"language\": \"python\",\n  \"metadata\": {\n    \"debugger\": true\n  }\n}\nClose the terminal and wait a few seconds for the kernel to register in the JupyterLab launcher.\n\n\n\nInstalling an R-Based Container as a Kernel\n\nStart a server on the JupyterHub cluster.\nStart a new terminal.\nLoad the apptainer module.\n$ load module apptainer\nClose the terminal and start a new one.\nUsing the nano text editor, create a new directory and file to store the custom Jupyter kernel configuration\n$ nano &lt;container_name&gt;/kernel.json\nWithin the kernel.json file, enter the following:\n{\n  \"argv\": [\n    \"apptainer\",\n    \"exec\",\n    \"/home/&lt;username&gt;/&lt;container_name&gt;.sif\",\n    \"R\",\n    \"--slave\",\n    \"-e\",\n    \"IRkernel::main()\",\n    \"--args\",\n    \"{connection_file}\"\n  ],\n  \"display_name\": \"R (&lt;container_name&gt;)\",\n  \"language\": \"R\",\n  \"metadata\": {\n    \"debugger\": true\n  }\n}\nInstall the container as a custom kernel using the kernel.json file.\n$ jupyter kernelspec install &lt;container_name&gt; --user\nClose the terminal and wait a few seconds for the kernel to register in the JupyterLab launcher."
  },
  {
    "objectID": "high-performance-computing/index.html#rstudio-server",
    "href": "high-performance-computing/index.html#rstudio-server",
    "title": "High Performance Computing",
    "section": "RStudio Server",
    "text": "RStudio Server\n\nvia JupyterHub\nThe easiest way to start an RStudio session in a HPC environment is to launch it within a JupyterLab session. See the entry above for session limits and documentation on starting a JupyterLab session from a JupyterHub cluster. Once your JupyterLab session is running, select the Software tab in the sidebar and find/load the rstudio-server module. You can then click the RStudio launcher, to open a new RStudio session. While you can install any R library from CRAN, you can’t use tools like conda and can only use software loaded as modules from the Alliance. If you need more flexibility for your RStudio session, setup and run your environment within an Apptainer container as documented below.\n\n\nvia Container\nNote: the linked instructions above cover running RStudio Server on the UBC ARC Sockeye cluster using Apptainer, but the instructions should be extremely similar for running on an Alliance cluster as you would just need to convert the job script from PBS to SLURM.\n\nInstructions for creating an Apptainer container with pre-installed dependencies:\n\nNote: You can either build your container on a local Linux system and copy the container to the cluster or build it directly on a login node.\nExample of building an RStudio container on a local Linux (Ubuntu) system\n# Install Apptainer if not already done so\n$ sudo apt update\n$ sudo apt install -y software-properties-common\n$ sudo add-apt-repository -y ppa:apptainer/ppa\n$ sudo apt update\n$ sudo apt install -y apptainer\n# Build a new sandbox container from the Rocker Project RStudio Server Docker container\n$ apptainer build --sandbox rstudio/ docker://rocker/rstudio\n# Run a shell within the sandbox container with write and sudo privileges\n$ apptainer shell --writable --fakeroot rstudio/\n# Start R and install packages\nApptainer&gt; R\nR version 4.3.1 (2023-06-16) -- \"Beagle Scounts\"\n...\n&gt; install.packages('ggplot2')\n...\n&gt; q()\nApptainer&gt; exit\n# Convert the sandbox container into a SIF container to be transferred to the HPC cluster\n$ apptainer build rstudio.sif rstudio"
  },
  {
    "objectID": "high-performance-computing/index.html#python",
    "href": "high-performance-computing/index.html#python",
    "title": "High Performance Computing",
    "section": "Python",
    "text": "Python\n\nAlliance Documentation - Python\nSupported Versions: 2.7, 3.6, 3.7, 3.8, 3.9, 3.10, and 3.11\nSupported Virtual Environment Managers:\n\nUBC ARC Sockeye:\n\nconda, virtualenv, and venv\nUsing Virtual Environments on Sockeye\n\nAlliance: virtualenv or venv. conda is not supported, but it can be run using an Apptainer container. In general, Alliance recommends avoiding conda and Anaconda packages if possible. Go here for more details. See instructions below for creating and running conda from an Apptainer container:\n\nCreating an Apptainer container with conda and Anaconda packages:\n# Pull the miniconda3 Docker container and create a new sandbox container from it\n$ apptainer build --sandbox conda/ docker://continuumio/miniconda3\n# Run a shell with write access in the sandbox container\n$ apptainer shell --shell /bin/bash --writable conda\n# Update conda\nApptainer&gt; conda update conda\n# Create a conda environment and install some conda packages\nApptainer&gt; conda create --name &lt;env_name&gt; &lt;package_names&gt;\nApptainer&gt; exit\n# Convert the sandbox container into a SIF container\n$ apptainer build conda.sif conda\nSLURM job script for running a Python script within the container created above:\n#!/bin/bash\n# Include the shebang at the top of the file, to clearly identify that the following script is meant for the Bash Shell\n# Provide flags as comments in the script. These will be read by SLURM to set option flags\n# Identify your user account\n#SBATCH --account=def-someuser\n# Identify the amount of memory to use per CPU\n#SBATCH --mem-per-cpu=1.5G  # In this case the job will only use one CPU with 1.5 GB of memory\n#SBATCH --time=1:00:00 # And I expect that the job will take a little less than an hour\n# At the beginning of your job load the Apptainer module\n$ module load apptainer\n# Run a Python script using an environment within your conda container\n$ apptainer run -C -B &lt;directory holding Python script&gt; -W $SLURM_TMPDIR conda.sif conda run -n &lt;env_name&gt; python &lt;script&gt;\n\n\nSupported Packages (Alliance): https://docs.alliancecan.ca/wiki/Available_Python_wheels\n\nNote: include --no-index flag with pip to only install Alliance wheels. The Alliance Python wheels have been specifically compiled and optimized to run as effectively on HPC clusters as possible.\n\nExample Job Scripts:\n\nSLURM (Alliance):\n#!/bin/bash\n# Include the shebang at the top of the file, to clearly identify that the following script is meant for the Bash Shell\n# Provide flags as comments in the script. These will be read by SLURM to set option flags\n# Identify your user account\n#SBATCH --account=def-someuser\n# Identify the amount of memory to use per CPU\n#SBATCH --mem-per-cpu=1.5G  # In this case the job will only use one CPU with 1.5 GB of memory\n#SBATCH --time=1:00:00 # And I expect that the job will take a little less than an hour\n# At the beginning of your job load any software dependencies needed for your job\nmodule load python/3.10\n# If running Python, create and activate a virual environment\nvirtualenv --no-download $SLURM_TMPDIR/venv\nsource $SLURM_TMPDIR/venv/bin/activate\n# Upgrade pip and install Python dependencies using Alliance wheels listed as dependencies in requirements.txt\npython -m pip install --no-index --upgrade pip\npython -m pip install --no-index -r requirements.txt\n# Run the script\npython -m script.py\n# Deactivate the virtual environment\ndeactivate\nPBS (UBC ARC Sockeye with conda):\n\nPrior to running your job, pre-install conda into your job directory and recreate your virtual environment\n\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-py310_23.3.1-0-Linux-x86_64.sh\n$ bash Miniconda3-py310_23.3.1-0-Linux-x86_64.sh\nWelcome to Miniconda3 py310_23.3.1-0\n...\n$ source ~/.bashrc\n$ conda env create -n venv -f environment.yml\n\nCreate your PBS job script\n\n#!/bin/bash\n# Include the shebang at the top of the file, to clearly identify that the following script is meant for the Bash Shell\n# Provide flags as comments in the script. These will be read by PBS to set option flags\n# Identify the resource required to run the job\n#PBS -l walltime=1:00:00,select=1:ncpus=1:mem=1.5gb # In this case, I anticipate that the job will take a little less than an hour and that I will only need to use one cluster node and within that node I will need one CPU core with 1.5 GB of memory dedicated to it\n# Identify a name that you can use to identify this job\n#PBS -N my_conda_job\n# Identify your project account\n#PBS -A proj-name\n# Request that the scheduler update you about the status of your job\n#PBS -m abe\n# Provide the scheduler an email address to recieve updates\n#PBS -M your.name@ubc.ca\n######################################################################################\n# At the beginning of your job, enter your job directory\ncd $PBS_O_WORKDIR\n# Load conda environment\nsource ~/.bashrc\n# Activate conda environment\nconda activate venv\n# Add your commands here\npython script.py\n# Deactivate environment\nconda deactivate\n\nInteractive Sessions:\n\nSockeye\nAlliance\n\nUse the Slurm allocation (salloc) command\nExample command for starting a single core IPython session on Alliance:\n$ salloc --time=00:15:00\nsalloc: Pending job allocation 1234567\n...\nsalloc: Nodes cdr&lt;###&gt; are ready for your job\n$ module load gcc/9.3.0 python/3.10\n$ virtualenv --no-download venv\n$ source venv/bin/activate\n(venv) $ python -m pip install --no-index ipython\n(venv) $ ipython\nPython 3.10.2 ...\n...\nIn [1]: &lt;python code&gt;\n...\nIn [2]: exit\n(venv) $ deactivate\n$ exit\nsalloc: Relinquishing job allocation 1234567\n\n\n\n\nOther Resources\n\nWorking with the Python Dask (Parallelization) Library Video"
  },
  {
    "objectID": "high-performance-computing/index.html#r",
    "href": "high-performance-computing/index.html#r",
    "title": "High Performance Computing",
    "section": "R",
    "text": "R\n\nAlliance Documentation - R\nSupported Versions: 4.0, 4.1 & 4.2\nExample Job:\n#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --mem-per-cpu=1.5G\n#SBATCH --time=1:00:00\n####################################################\nmodule load gcc/9.3.0 r/4.2.2\nRscript script.r\nInteractive Sessions:\n\nSockeye\nAlliance\n\nUse the Slurm allocation (salloc) command\nExample command for starting session:\n$ salloc --time=00:15:00\nsalloc: Pending job allocation 1234567\n...\nsalloc: Nodes cdr&lt;###&gt; are ready for your job\n$ module load gcc/9.3.0 r/4.2.2\n$ R\nR version 4.2.2 (2022-10-31) -- \"Innocent and Trusting\"\n...\n&gt; &lt;r code here&gt;\n&gt; q()\nSave workspace image? [y/n/c]:\n$ exit\nsalloc: Relinquishing job allocation 1234567\n\n\n\n\nOther Resources:\n\nIntroduction to HPC in R Webinar\nHigh-performance R Tutorial"
  },
  {
    "objectID": "high-performance-computing/index.html#other-supported-languages",
    "href": "high-performance-computing/index.html#other-supported-languages",
    "title": "High Performance Computing",
    "section": "Other Supported Languages",
    "text": "Other Supported Languages\n\nC, C++, Objective-C, and other GCC supported languages\nElixir (Alliance)\nGo (Alliance)\nJava\nJavaScript/TypeScript via node.js\nJulia\nMATLAB\nOctave\nPerl\nRuby (Alliance)\nRust (Alliance)"
  },
  {
    "objectID": "high-performance-computing/index.html#containers-on-hpc",
    "href": "high-performance-computing/index.html#containers-on-hpc",
    "title": "High Performance Computing",
    "section": "Containers on HPC",
    "text": "Containers on HPC\n\nUse Cases:\n\nRunning software that is not already included as an HPC module\nBuilding runtime environments from existing projects and easily reproducing research\n\nDocumentation\n\nAlliance\nSockeye\n\nDocumentation for HPC Container Platform - Apptainer"
  },
  {
    "objectID": "high-performance-computing/index.html#photogrammetry",
    "href": "high-performance-computing/index.html#photogrammetry",
    "title": "High Performance Computing",
    "section": "Photogrammetry",
    "text": "Photogrammetry\nPhotogrammetry software can be incredibly resource intensive, so running it on an HPC cluster can save you a lot of time and resources when working with extraordinarily large datasets.\nFor an assortment of reasons, most HPC clusters only install free and open-source software. This includes UBC ARC’s Sockeye cluster and the Digital Research Alliance’s clusters. This means that software like Agisoft Metashape, ArcGIS Drone2Map, and Pix4D are unfortunately not available for use on HPC clusters.\nTo leverage HPC clusters for processing drone images or video into orthophotos, 3D models, point clouds, or elevation models, Open Drone Map (ODM) will likely be your best option.\nODM is most often distributed within a Docker container, and it can just as easily run within alternative container engines, like Apptainer or Podman. Prior to running ODM on an HPC cluster, it is highly recommended that you take a few test runs on a local machine with a subset of your data. This will give you an opportunity to explore and optimize any option flags to produce the best results for your dataset. You can find information on installing Apptainer here.\nTo test ODM on a local machine that has Apptainer installed, make a directory that will be bound to the container for ODM’s input and output. Then nest another directory named ‘images’ with a subset of your data stored within it. The example below uses a directory named ‘odm_test’.\n$ apptainer run -B odm_test:/project/code docker://opendronemap/odm:latest --project-path /project\nAfter processing your dataset, ODM creates a report at ‘odm_test/odm_report/report.pdf’, which can be used to quickly analyze your results and start calculating a total job time estimate for your HPC job script.\nOnce you’ve identified any flags that you want to add to the command, you will need to convert the ODM Docker container into an Apptainer SIF container with the following command on your local machine:\n$ apptainer build odm_latest.sif docker://opendronemap/odm:latest\n# For a container with GPU support, run:\n# apptainer build odm_gpu.sif docker://opendronemap/odm:gpu\nThen copy the Apptainer SIF file from your local machine to your HPC home directory and ensure your dataset is stored in a project directory.\nyou can use the following in a SLURM job script on the HPC cluster:\n#!/bin/bash\n# Include the shebang at the top of the file, to clearly identify that the following script is meant for the Bash Shell\n# Provide flags as comments in the script. These will be read by SLURM to set option flags\n# Identify your user account\n#SBATCH --account=def-someuser\n# Identify the amount of memory to use per CPU\n#SBATCH --mem-per-cpu=1.5G  # In this case the job will only use one CPU with 1.5 GB of memory, adjust as needed\n#SBATCH --time=1:00:00 # You can use the test runs on your local machine to help you estimate this\n# At the beginning of your job load the Apptainer module\nmodule load apptainer\n# CPU Only\napptainer run -C -W $SLURM_TMPDIR -B /project/images:/project/code/images,/scratch:/project odm_latest.sif --project-path /project\n# For a container with GPU support, run:\n# apptainer run -C -W $SLURM_TMPDIR -B /project/images:/project/code/images,/scratch:/project --nv odm_gpu.sif --project-path /project\nThe command in the above job script may need to be slightly adjusted based on the structure of the HPC cluster that you are using with the job script parameters being also being modified to account for the overall size of your dataset.\nBased on best practices, you will want to store your dataset within your project folder and bind the directory holding your dataset to a directory within the container with the path ‘/project/code/images’. You will also want to set ODM to output to your scratch directory by binding it to the ODM project directory. Finally, flag ODM to use the bound scratch directory as the project output directory.\nOnce the job has been completed, you will need to review ODMs output within the scratch directory and transfer any relevant files back into your project directory before cleaning up the scratch directory."
  },
  {
    "objectID": "databases/index.html",
    "href": "databases/index.html",
    "title": "Databases",
    "section": "",
    "text": "How to create and access MySQL and PostgreSQL databases on DRI systems - Video\nGeospatial Analysis with SQL\nPractical SQL"
  },
  {
    "objectID": "databases/index.html#sqlite",
    "href": "databases/index.html#sqlite",
    "title": "Databases",
    "section": "SQLite",
    "text": "SQLite\n\nGIS Extensions / Libraries\n\nSpatiaLite"
  },
  {
    "objectID": "databases/index.html#postgresql",
    "href": "databases/index.html#postgresql",
    "title": "Databases",
    "section": "PostgreSQL",
    "text": "PostgreSQL\n\nLearn PostgreSQL\n\n\nHosting\n\nAlliance\n\nDatabase Servers – Cedar PostreSQL\n\n\n\nCloud / SaaS Providers\n\nMicrosoftAzure\nAWS\nGoogle Cloud\nSupabase\nRailway\n\n\n\n\nGIS Extensions / Libraries\n\nPostGIS\n\nPostGIS in Action\nPostGIS Cookbook\nMastering PostGIS\n\npgRouting"
  },
  {
    "objectID": "databases/index.html#mysql",
    "href": "databases/index.html#mysql",
    "title": "Databases",
    "section": "MySQL",
    "text": "MySQL\n\nHosting\n\nAlliance\n\nDatabase Servers – Cedar MySQL\n\n\n\nCloud / SaaS Providers\n\nMicrosoft Azure\nAWS\nGoogle Cloud\nPlanetScale\nRailway\n\n\n\n\nGIS Extensions / Libraries\n\nMySQL Spatial"
  },
  {
    "objectID": "r/index.html",
    "href": "r/index.html",
    "title": "R",
    "section": "",
    "text": "R in Action\nR for Data Science\nR 4 Data Science Quick Reference"
  },
  {
    "objectID": "r/index.html#geospatial-books",
    "href": "r/index.html#geospatial-books",
    "title": "R",
    "section": "Geospatial Books",
    "text": "Geospatial Books\n\nGeographic Data Science with R: Visualizing and Analyzing Environmental Change\nLearning R for Geospatial Analysis\nIntroduction to R for Spatial Data Science\nSpatial Data Science with Applications in R\nGeocomputation with R\nRemote Sensing and Digital Image Processing with R\n\n\nExtending GIS Software\n\nHands-On Geospatial Analysis with R and QGIS"
  },
  {
    "objectID": "r/index.html#geospatial-librariespackages",
    "href": "r/index.html#geospatial-librariespackages",
    "title": "R",
    "section": "Geospatial Libraries/Packages",
    "text": "Geospatial Libraries/Packages\n\nCRAN Task View: Analysis of Spatial Data\nAwesome Geospatial - R"
  },
  {
    "objectID": "r/index.html#formatting",
    "href": "r/index.html#formatting",
    "title": "R",
    "section": "Formatting",
    "text": "Formatting\nSometimes writing code can get a bit messy. Formatters, like styler, can automatically reformat your code to make it cleaner and easier to read while following a set of standards and best practices.\nNote: RStudio includes a formatting tool via Code &gt; Reformat Code\n\nstyler Documentation"
  },
  {
    "objectID": "r/index.html#linting",
    "href": "r/index.html#linting",
    "title": "R",
    "section": "Linting",
    "text": "Linting\nUsing a static analysis tool, or linter, is a common best practice among programmers that helps identify mistakes when writing code by ensuring that you follow the correct syntax and a guiding set of best practices.\n\nlintr Documentation"
  },
  {
    "objectID": "r/index.html#testing-framework",
    "href": "r/index.html#testing-framework",
    "title": "R",
    "section": "Testing Framework",
    "text": "Testing Framework\nSimilar to type checking, unit testing can be a helpful tool when writing large and complex scripts or programs. Testing frameworks enable you to define tests that can run over your functions and ensure they are following expected behavior in a range of practical scenarios.\n\ntestthat Documentation"
  },
  {
    "objectID": "r/index.html#benchmarking",
    "href": "r/index.html#benchmarking",
    "title": "R",
    "section": "Benchmarking",
    "text": "Benchmarking\nWhen writing code, there is often a wide array of ways in which a task can be completed. Benchmarking your functions can help you find the most efficient approach possible. This is particularly important when writing scripts and programs that will be used on large datasets and/or on high performance computers.\n\nbench Documentation"
  },
  {
    "objectID": "r/index.html#other-related-librariespackages",
    "href": "r/index.html#other-related-librariespackages",
    "title": "R",
    "section": "Other Related Libraries/Packages",
    "text": "Other Related Libraries/Packages\n\ntrajectories\n\ntrajectories: Classes and Methods for Trajectory Data\n\nxts\n\nxts: Extensible Time Series\n\nzoo\n\nzoo: An S3 Class and Methods for Indexed Totally Ordered Observations\n\ntidygraph\nproj4\ndpylr\n\nIntroduction to dplyr\ndpylr Reference Manual\n\ntidyr\n\ntidyr Reference Manual\n\nCAST\n\nIntroduction to CAST\n\nmlr3Spatial\n\nSpatial Data in the mlr3 Ecosystem\n\nperformanceEstimation\n\nperformanceEstimation Reference Manual\n\nplotly Reference Manual\n\nInteractive web-based data visualization with R, plotly, and shiny\n\nreticulate"
  },
  {
    "objectID": "development-environments/index.html",
    "href": "development-environments/index.html",
    "title": "Development Environments",
    "section": "",
    "text": "Virtual environments are an important tool for isolating multiple Python projects on a single machine, and their use is highly recommended when starting new projects. By isolating your projects, you can ensure that each project is only using the specific Python packages that were identified and installed for running it. This makes the project easier to reproduce while also reducing errors caused by clashing package dependencies.\n\nGetting Started with Conda, Virtual Environments, and Python\n\n\n\nConda is an extremely powerful tool for both managing environments and packages. Using conda install provides the ability to install a range of powerful packages that aren’t available via Python’s built-in package manager, Pip. Additionally, Conda enables you to install and manage multiple versions of Python on a single machine using virtual environments.\nIf you need to install Conda on your device, we suggest using the link below to install it using the Miniconda installer.\n\nMiniconda, a minimal installer for conda\nconda User Guide\n\n\n\n\nvenv is a lightweight module that has been included with base installations of Python since version 3.3 and enables you to quickly setup and manage virtual environments for your project. virtualenv is a more powerful alternative to venv, but unlike venv it needs to be installed separately via pip. Both modules lack the same level of functionality as conda, but they can be helpful tools for those who are still getting comfortable with Python or don’t need conda’s added functionalities.\n\nvenv Documentation\nvirtualenv Documentation\n\n\n\n\nR’s equivalent to virtualenv is renv, which itself is a successor to another R-based virtual environment manager, packrat. renv has been developed by the same team behind RStudio and it’s usage is recommended within the RStudio user guide.\n\nIntroduction to renv\nRStudio User Guide - renv"
  },
  {
    "objectID": "development-environments/index.html#virtual-environment-managers",
    "href": "development-environments/index.html#virtual-environment-managers",
    "title": "Development Environments",
    "section": "",
    "text": "Virtual environments are an important tool for isolating multiple Python projects on a single machine, and their use is highly recommended when starting new projects. By isolating your projects, you can ensure that each project is only using the specific Python packages that were identified and installed for running it. This makes the project easier to reproduce while also reducing errors caused by clashing package dependencies.\n\nGetting Started with Conda, Virtual Environments, and Python\n\n\n\nConda is an extremely powerful tool for both managing environments and packages. Using conda install provides the ability to install a range of powerful packages that aren’t available via Python’s built-in package manager, Pip. Additionally, Conda enables you to install and manage multiple versions of Python on a single machine using virtual environments.\nIf you need to install Conda on your device, we suggest using the link below to install it using the Miniconda installer.\n\nMiniconda, a minimal installer for conda\nconda User Guide\n\n\n\n\nvenv is a lightweight module that has been included with base installations of Python since version 3.3 and enables you to quickly setup and manage virtual environments for your project. virtualenv is a more powerful alternative to venv, but unlike venv it needs to be installed separately via pip. Both modules lack the same level of functionality as conda, but they can be helpful tools for those who are still getting comfortable with Python or don’t need conda’s added functionalities.\n\nvenv Documentation\nvirtualenv Documentation\n\n\n\n\nR’s equivalent to virtualenv is renv, which itself is a successor to another R-based virtual environment manager, packrat. renv has been developed by the same team behind RStudio and it’s usage is recommended within the RStudio user guide.\n\nIntroduction to renv\nRStudio User Guide - renv"
  },
  {
    "objectID": "development-environments/index.html#integrated-development-environments-ide",
    "href": "development-environments/index.html#integrated-development-environments-ide",
    "title": "Development Environments",
    "section": "Integrated Development Environments (IDE)",
    "text": "Integrated Development Environments (IDE)\n\nJupyter\nProject Jupyter consists of an ecosystem of open-source projects that support the creation and distribution of computational notebooks. While Jupyter supports computational notebooks with kernels from a wide range of programming languages, it specializes in Python followed by R and Julia. Much of the Jupyter ecosystem has been written in and/or runs on Python.\n\nJupyter Notebooks\nThe term Jupyter Notebooks is used interchangeably to refer to the Jupyter Notebook file format and the user interface that has most commonly been to used to author and edit them. In recent years, the Project Jupyter team has slowly encouraged users to migrate from the classic Jupyter Notebook interface to JupyterLab, its more feature-rich successor.\n\nJupyter Notebook (File) Format Documentation\nJupyter Notebook (User Interface) Documentation\n\n\n\nJupyterLab\nYou can install and run JupyterLab on your local machine in two separate ways. The traditional and most flexible approach is to install JupyterLab as a Python package using Pip or Conda and starting it from the Jupyter command line interface. This approach uses a locally installed version of Python to start a basic server on your machine and then navigates to that server through your preferred web browser. For those who may be uncomfortable with installing Python and/or using command line interfaces, Project Jupyter has also released a simple-to-use desktop application, named JupyterLab Desktop. This application comes packaged with Python and a default set of packages that are frequently used in scientific computing.\n\nJupyterLab Documentation\nJupyterLab Desktop - Installation\nJupyterLab Desktop User Guide\n\n\n\nJupyterHub\nThe easiest way to get started with creating and editing notebooks is through a JupyterHub service, like UBC LT’s Open Jupyter or UBC Syzygy, which enable you to connect to and interact with Jupyter servers without having to install anything on your machine. While convenient, these services come with significant drawbacks in computing power and will not work well in cases where computations are running on large datasets.\n\nUBC LT - JupyterHub Instructor Guide\nIntroduction to Syzygy - Getting Started\n\n\n\nHelpful Extensions\n\nRendering\n\nGeoJSON Extension\n\n\n\nGit & GitHub\n\nGit Extension\nGitHub Extension\nGit and Jupyter Notebooks: The Ultimate Guide\n\n\n\nLinting, Formatting, and Benchmarking\n\nLanguage Server Protocol Extension\nCode Formatter Extension\nExecution Time Extension\nSystem Monitor Extension\n\n\n\nSpellchecking\n\nSpellchecker Extension\n\n\n\nAI\n\nGenerative AI Extension\n\n\n\n\nGeospatial Libraries\n\nipyleaflet Documentation\n\n\n\n\nRStudio\nWhile JupyterLab excels at providing an intuitive environment for creating and editing Python-based computational notebooks, RStudio provides a similarly intuitive environment for working with the R programming language. It provides useful tools for writing R scripts, interacting with the R console, or developing R-based computational notebooks with Quarto or R Markdown.\n\nRStudio UserGuide\n\n\n\nVisual Studio Code\nVisual Studio Code (VS Code) is a more general-purpose programming environment compared to JupyterLab and RStudio that excels for writing scripts and programs in Python, JavaScript, Julia, and a broad range of other programming languages. VS Code is also capable of running computational notebooks through well-supported extensions, but it can feel less intuitive for some users.\n\nUBC Library Research Commons - VS Code Overview in Setting Up a Dev Environment\nVS Code Documentation\nUsing Git source control in VSCode\n\n\nHelpful Extensions\n\nRemote Development\n\nRemote Development Extension Overview\n\n\n\nJupyter\n\nJupyter Extension Overview\n\n\n\nQuarto\n\nQuarto Extension Overview\n\n\n\nPython\n\nPython in VS Code Documentation\nPython Extension Overview\n\n\n\nR\n\nR in VS Code Documentation\nR Extension Overview"
  },
  {
    "objectID": "gis-software/index.html",
    "href": "gis-software/index.html",
    "title": "GIS Software",
    "section": "",
    "text": "Practical GIS\nGIS Fundamentals\nGIS: An Introduction to Mapping Technologies"
  },
  {
    "objectID": "gis-software/index.html#arcgis",
    "href": "gis-software/index.html#arcgis",
    "title": "GIS Software",
    "section": "ArcGIS",
    "text": "ArcGIS\n\nUBC Library Research Commons - Understanding Map Projections\nLearning ArcGIS Pro 2"
  },
  {
    "objectID": "gis-software/index.html#qgis",
    "href": "gis-software/index.html#qgis",
    "title": "GIS Software",
    "section": "QGIS",
    "text": "QGIS\n\nUBC Library Research Commons - Map Production with QGIS\nUBC Library Research Commons - Network Analysis with QGIS\nQGIS Quick Start Guide\nLearn QGIS\nQGIS: Becoming a GIS Power User"
  },
  {
    "objectID": "gis-software/index.html#geoserver",
    "href": "gis-software/index.html#geoserver",
    "title": "GIS Software",
    "section": "GeoServer",
    "text": "GeoServer\n\nGeoServer Beginner’s Guide\nExpert GeoServer"
  },
  {
    "objectID": "gis-software/index.html#mapkurator",
    "href": "gis-software/index.html#mapkurator",
    "title": "GIS Software",
    "section": "MapKurator",
    "text": "MapKurator\n\nMapKurator Documentation"
  },
  {
    "objectID": "digital-communications/index.html",
    "href": "digital-communications/index.html",
    "title": "Digital Communications",
    "section": "",
    "text": "Open-source content management system software that is used to host a wide range of websites and web applications. Both UBC Blogs and UBC CMS are built on top of this software, so UBC provides a large collection of documentation to help faculty and students in getting started with using it. The wide adoption of this software also means there is plenty of documentation for self-hosting a WordPress server, but strong caution should be taken as security vulnerabilities are frequently identified and taken advantage of by bad actors.\n\nUBC Arts ISIT Video Tutorials\nIThemes Tutorials\nUBC Arts ISIT WordPress Resources\nWordPress Clinics at UBC CTLT\nWordPress Documentation\nWordPress 101 – Video Course\nWordPress: The Missing Manual\n\n\n\n\nUBC Blogs vs. UBC CMS\nUBC Blogs FAQ\nUBC Blogs Video Tutorials\nUBC Blogs Instructor Guide\n\n\n\n\n\nUBC CMS Manual\n\n\n\n\n\nUBC IT Shared Hosting\nDigital Research Alliance - Cloud\nReclaim Hosting"
  },
  {
    "objectID": "digital-communications/index.html#wordpress",
    "href": "digital-communications/index.html#wordpress",
    "title": "Digital Communications",
    "section": "",
    "text": "Open-source content management system software that is used to host a wide range of websites and web applications. Both UBC Blogs and UBC CMS are built on top of this software, so UBC provides a large collection of documentation to help faculty and students in getting started with using it. The wide adoption of this software also means there is plenty of documentation for self-hosting a WordPress server, but strong caution should be taken as security vulnerabilities are frequently identified and taken advantage of by bad actors.\n\nUBC Arts ISIT Video Tutorials\nIThemes Tutorials\nUBC Arts ISIT WordPress Resources\nWordPress Clinics at UBC CTLT\nWordPress Documentation\nWordPress 101 – Video Course\nWordPress: The Missing Manual\n\n\n\n\nUBC Blogs vs. UBC CMS\nUBC Blogs FAQ\nUBC Blogs Video Tutorials\nUBC Blogs Instructor Guide\n\n\n\n\n\nUBC CMS Manual\n\n\n\n\n\nUBC IT Shared Hosting\nDigital Research Alliance - Cloud\nReclaim Hosting"
  },
  {
    "objectID": "digital-communications/index.html#static-site-generators-ssg-web-publishing",
    "href": "digital-communications/index.html#static-site-generators-ssg-web-publishing",
    "title": "Digital Communications",
    "section": "Static Site Generators (SSG) / Web Publishing",
    "text": "Static Site Generators (SSG) / Web Publishing\nAs alternatives to WordPress, static site generators give up easy-to-use interfaces via a content management system and a range of other functionalities to create faster, more reliable, and more secure websites, where content is managed via a set of Markdown files. Additionally, these tools often have smaller sets of themes that users can install to customize the styling of their website, thus requiring a better understanding of HTML, CSS, and JavaScript to develop a more customized look and feel to a website.\nStatically generated sites can be hosted nearly anywhere on the web, including:\n\nDigital Research Alliance - Cloud / Arbutus Object Storage\nGitHub Pages - 1 GB Limit\n\n\nQuarto\nDeveloped by the same company working on RStudio, Quarto provides a powerful tool for publishing scientific and technical work in a range of formats and acts as a successor to R Markdown. Code in Python, R, Julia, and JavaScript can all be represented and rendered along with Pandoc-style Markdown. Interactive visualizations can also be embedded on supported formats using R Shiny, Observable JS, or Jupyter Widgets. Note: If rendering and exporting HTML files in RStudio using Quarto, ensure you also export the quarto_files directory, which holds necessary CSS and JavaScript files.\n\nQuarto Webinars\nQuarto Documentation\n\n\n\nR Markdown (R)\nDeveloped by the company behind RStudio, R Markdown provides an authoring framework that enables code to be rendered alongside Markdown and published in multiple formats. See Quarto for a successor to R Markdown. Note: HTML files rendered via R Markdown use inline scripts and CSS, which can create sometimes unnecessarily large files, but they ensure that the file displays consistently without external files.\n\nR Markdown Documentation\n\n\n\nJekyll (Ruby)\nOne of the most used static site generators. This SSG integrates smoothly with GitHub Pages to quickly render websites directly from a GitHub repository. Due to its wide adoption, it also includes a wide range of open-source themes that can be used to customize the style of a website.\n\nUBC Library Research Commons - Introduction to Jekyll\nUBC Library Research Commons - Intermediate Jekyll\nUBC Library Research Commons - Building a project website with Jekyll and GitHub Pages"
  },
  {
    "objectID": "digital-communications/index.html#digital-exhibit-tools",
    "href": "digital-communications/index.html#digital-exhibit-tools",
    "title": "Digital Communications",
    "section": "Digital Exhibit Tools",
    "text": "Digital Exhibit Tools\nStoring and presenting digital assets, like images, audio, and video files, can be easily managed via content management systems and static site generators that specialize in digital exhibits.\n\nUBC Library Research Commons - Survey of Digital Exhibit Tools\n\n\nOmeka Classic & Omeka-S\nOmeka Classic is targeted towards small, individually developed projects, while Omeka-S focuses on developing larger scale, institutional-wide repositories with multiple collections or projects. Both are considered web applications as they include content management systems that provide an easy-to-use interface for developing and managing digital exhibits.\n\nUBC Library Research Commons - Building Digital Exhibits with Omeka\nOmeka: A User’s Guide: Introduction\n\n\nCloud Hosting\n\nUBC IT Shared Hosting\nOmeka Classic Documentation\nOmeka-S Documentation\n\n\n\n\nCollectionBuilder\nLacking a content management system, CollectionBuilder centers the development and management of digital exhibits around CSVs and Markdown. Built as a theme around a static site generator (Jekyll), CollectionBuilder creates digital exhibits as fast and easy-to-preserve websites.\n\nCollectionBuilder Documentation"
  },
  {
    "objectID": "version-control-systems/jupyterhub-private-repo.html",
    "href": "version-control-systems/jupyterhub-private-repo.html",
    "title": "Working with Private GitHub Repositories from JupyterHub",
    "section": "",
    "text": "This is the recommended method for cloning and pushing commits to a private GitHub repository while working within a JupyterHub server. A fine-grained personal access token, acts as a password for your account and provides a limited set of functionality when working with your GitHub repositories.\nYou can have multiple personal access tokens associated with your GitHub account with various levels of access to your account and specific repositories associated with it. You can generate tokens from within the developer settings of your personal account.\nWhen creating a new token, ensure that you provided it with the minimum privileges necessary to interact with your private repositories via JupyterHub. This includes setting a short expiration date for the token and restricting the token to only work with a specific repository.\n\n\n\nGitHub fine-grained personal access token - name, expiration, description, and repository access\n\n\nAlso set restrictive permissions for the token. If you only need to clone a private repository from JupyterHub, set the ‘Contents’ permissions to ‘Read-only’. Otherwise, if you plan to make changes to any files in the repository and push those changes to GitHub, set the ‘Contents’ permissions to ‘Read and write’.\n\n\n\nGitHub fine-grained personal access token - permissions\n\n\nOnce you click ‘Generate token’, GitHub will display your new personal access token. Be sure to copy and store the token in a secure place. From JupyterLab’s Git extension, you can now provide your GitHub username and your token in place of a password to clone and push changes to token’s associated repository."
  },
  {
    "objectID": "version-control-systems/jupyterhub-private-repo.html#using-fine-grained-personal-access-tokens",
    "href": "version-control-systems/jupyterhub-private-repo.html#using-fine-grained-personal-access-tokens",
    "title": "Working with Private GitHub Repositories from JupyterHub",
    "section": "",
    "text": "This is the recommended method for cloning and pushing commits to a private GitHub repository while working within a JupyterHub server. A fine-grained personal access token, acts as a password for your account and provides a limited set of functionality when working with your GitHub repositories.\nYou can have multiple personal access tokens associated with your GitHub account with various levels of access to your account and specific repositories associated with it. You can generate tokens from within the developer settings of your personal account.\nWhen creating a new token, ensure that you provided it with the minimum privileges necessary to interact with your private repositories via JupyterHub. This includes setting a short expiration date for the token and restricting the token to only work with a specific repository.\n\n\n\nGitHub fine-grained personal access token - name, expiration, description, and repository access\n\n\nAlso set restrictive permissions for the token. If you only need to clone a private repository from JupyterHub, set the ‘Contents’ permissions to ‘Read-only’. Otherwise, if you plan to make changes to any files in the repository and push those changes to GitHub, set the ‘Contents’ permissions to ‘Read and write’.\n\n\n\nGitHub fine-grained personal access token - permissions\n\n\nOnce you click ‘Generate token’, GitHub will display your new personal access token. Be sure to copy and store the token in a secure place. From JupyterLab’s Git extension, you can now provide your GitHub username and your token in place of a password to clone and push changes to token’s associated repository."
  },
  {
    "objectID": "version-control-systems/jupyterhub-private-repo.html#using-ssh",
    "href": "version-control-systems/jupyterhub-private-repo.html#using-ssh",
    "title": "Working with Private GitHub Repositories from JupyterHub",
    "section": "Using SSH",
    "text": "Using SSH\nWarning: This method is not recommended. While there are multiple precautions taken by UBC LT and Syzygy to isolate and secure your JupyterHub environment, if for any reason your environment were to be compromised, a bad actor could gain full access to your GitHub account.\nIf you must use this method, ensure you generate a new SSH key on JupyterHub with a strong passphrase. Never reuse/store an SSH private key that you have generated on your local machine within a JupyterHub environment.\nFrom a Terminal in your JupyterHub server, run the following command to generate a new SSH key.\njovyan@jupyter-&lt;your_cwl&gt;:~$ ssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/jovyan/.ssh/id_rsa):\nCreated directory '/home/jovyan/.ssh'.\nEnter passphrase (empty for no passphrase): &lt;enter_a_strong_passphrase_here&gt;\nEnter same passphrase again:\nYour identification has been saved in /home/jovyan/.ssh/id_rsa\nYour public key has been saved in /home/jovyan/.ssh/id_rsa.pub\nTo use your new SSH key, add a public key to your GitHub personal account from the SSH Key Settings page by copying the contents of /home/jovyan/.ssh/id_rsa.pub into the key field.\nFrom JupyterHub, you’ll then be able to clone and push changes to all of your private repositories within GitHub using the SSH protocol."
  },
  {
    "objectID": "development-environments/jupyterhub-installing-packages.html",
    "href": "development-environments/jupyterhub-installing-packages.html",
    "title": "Installing Packages in UBC LT’s Jupyter Open or UBC Syzygy",
    "section": "",
    "text": "When using one of UBC’s remote Jupyter services, you will likely find that JupyterHub’s default environment does not include a range of packages needed to complete your research. While it is possible to install more packages within the default environment using package managers like, Pip, Conda, or Mamba, you may find yourself running into instances where you need to run an unsupported version of Python or R and/or need to install packages that conflict with the packages already included in the default environment. The following instructions provide walkthrough documentation for installing new packages and creating customized kernels within a JupyterHub environment.\nIf you need to share a reproducible Jupyter environment or would like to automate the creation and management of your kernels, copy the Makefile from this repository into your project and follow the instructions found in the repository’s README."
  },
  {
    "objectID": "development-environments/jupyterhub-installing-packages.html#mamba",
    "href": "development-environments/jupyterhub-installing-packages.html#mamba",
    "title": "Installing Packages in UBC LT’s Jupyter Open or UBC Syzygy",
    "section": "Mamba",
    "text": "Mamba\nIf you are simply running into issues with installing packages via Conda, try using Mamba instead. Mamba is a fast and lightweight alternative to Conda that is already included in JupyterHub’s default environment. It runs more efficiently in computing environments with fewer resources, such as UBC LT’s Jupyter Open or UBC Syzygy."
  },
  {
    "objectID": "development-environments/jupyterhub-installing-packages.html#virtual-environments",
    "href": "development-environments/jupyterhub-installing-packages.html#virtual-environments",
    "title": "Installing Packages in UBC LT’s Jupyter Open or UBC Syzygy",
    "section": "Virtual Environments",
    "text": "Virtual Environments\nWhile installing new packages within the default environment can initially seem quite convenient. You can often run into issues when attempting to install a package that shares a dependency with one of the packages already installed in the default environment. You will also find that packages that are installed within the default environment do not persist between sessions, requiring you to re-install them whenever you start a new Jupyter session. It is important to note that the default environment does not only contain a set of packages that JupyterHub administrators thought would be frequently useful to users, it also contains packages that run the Jupyter user interface (JupyterLab) and the pre-installed Jupyter kernels. To avoid some of those issues, such as conflicts between package dependencies, you can use virtual environments, which help to isolate your computing environment from the default environment thus enabling you to more freely install, manage, and persist your packages.\nThe following instructions use Mamba as a drop in replacement for Conda and can be used for setting up either Python or R-based environments.\n\nNavigate to your preferred JupyterHub service, start a new Jupyter server, and launch a terminal.\nFrom the terminal, create a new virtual environment and install the appropriate kernel package. Replace &lt;environment_name&gt; with a unique name that can be used to easily identify your environment, like a project name. The name will be used to create a new directory in your Home directory, so avoid including any spaces.\nPython:\n$ mamba create -p &lt;environment_name&gt; ipykernel -y\nR:\n$ mamba create -p &lt;environment_name&gt; r-irkernel -y\nBefore you can activate your virtual environment and start installing packages within it, you will need to initialize Mamba and restart your Bash shell.\n$ mamba init -q && source ~/.bashrc\nNext switch from the default environment to your newly created virtual environment by activating it. Replace &lt;username&gt; with the username displayed in the terminal’s shell prompt. On Open Jupyter, this would be jovyan, while on UBC Syzygy, it would be your CWL username. And again replace &lt;environment_name&gt; with the name you used earlier in Step 2.\n$ mamba activate /home/&lt;username&gt;/&lt;environment_name&gt;\nNow you can begin installing your packages into the virtual environment.\n$ mamba install &lt;package_names&gt; -y\nNote: Some packages that you need may not be available via Mamba’s default package repository, conda-forge, and are only available from the Python Package Index (PyPI) or the Comprehensive R Archive Network (CRAN). To install those packages, you can use Python’s pip or R’s install.packages() with one of the following commands:\n$ python -m pip install &lt;python_package_names&gt;\n$ Rscript -e 'install.packages(c(\"&lt;r_package_names&gt;\"),repo=\"https://mirror.rcg.sfu.ca/mirror/CRAN/\",quiet=TRUE)'`\nWith your virtual environment setup and still active, you will need to register the environment as a Jupyter kernel using the kernel packages installed during the creation of your environment to make it available to your notebooks.\nPython:\n$ python -m ipykernel install --user --name &lt;environment_name&gt; --display-name \"Python (&lt;environment_name&gt;)\"\nR:\n$ Rscript -e 'IRkernel::installspec(name=\"&lt;environment_name&gt;\", displayname=\"R (&lt;environment_name&gt;)\")'`\n\nAfter a few seconds, the custom kernel built from you virtual environment will be listed in the JupyterLab launcher and you will be able to select it as the preferred kernel to run within your notebooks.\nTo install more packages into your environment, repeat Steps 3-5 from a JupyterLab terminal.\nIt is important to note that the directories created for each virtual environment can be quite large as more packages are installed within them. Consider purging the environments when they are no longer needed and remember to include the directory in your .gitignore file if you are using Git/GitHub.\nYou can purge an environment from JupyterHub by completing the following steps:\n\nOpen a JupyterLab terminal.\nRemove the kernel from Jupyter.\n$ jupyter kernelspec remove &lt;environment_name&gt; -y\nRemove the virtual environment.\n$ mamba remove -p /home/&lt;username&gt;/&lt;environment_name&gt; --all"
  },
  {
    "objectID": "development-environments/conda-getting-started.html",
    "href": "development-environments/conda-getting-started.html",
    "title": "Getting Started with Conda, Virtual Environments, and Python",
    "section": "",
    "text": "There are many different ways to install and start using Python on your personal device, and no one way is the correct way. How one installs Python often simply comes down to personal preference. In general, we recommend using a tool called Conda. When performing geospatial computing, you’ll likely be working with multiple projects that depend on not only Python but other languages too, like R, Julia, or JavaScript. You may also find that you projects can’t always run on the same exact version of Python. Conda enables you to easily install and run multiple versions of Python along with a range of other languages and software packages."
  },
  {
    "objectID": "development-environments/conda-getting-started.html#conda",
    "href": "development-environments/conda-getting-started.html#conda",
    "title": "Getting Started with Conda, Virtual Environments, and Python",
    "section": "Conda",
    "text": "Conda\nJust as there is with Python, there are multiple ways to install Conda. We recommend using the latest Miniconda installer.\nConda’s documentation includes installation instructions for Windows, MacOS, and Linux here.\nOnce you have Conda installed on your machine, follow along with ‘Getting Started With Conda’ to familiarize yourself with some of Conda’s basic functionalities."
  },
  {
    "objectID": "development-environments/conda-getting-started.html#virtual-environments",
    "href": "development-environments/conda-getting-started.html#virtual-environments",
    "title": "Getting Started with Conda, Virtual Environments, and Python",
    "section": "Virtual Environments",
    "text": "Virtual Environments\nAlong with being a package manager, which makes installing and managing software, significantly more convenient, Conda also acts as an environment manager. Before getting started with using Conda and Python, it’s important to have an understanding of virtual environments and how Conda works with them to install software and packages onto your machine. If you followed along with the Managing Environments section of ‘Getting Started With Conda’, then you got a very basic introduction to creating and using a virtual environment.\nLet’s walkthrough the process again by creating a virtual environment that has JupyterLab installed. We could then install some extensions and other packages into this environment and clone it in the future as we start new projects in order to save ourselves a little bit of time.\nTo start, you will need to open a terminal on your machine. If you are using Windows, the Conda installer would have added a new application to your machine named Anaconda Prompt. Running this application is the recommended approach to starting a terminal and running Conda.\nBefore we can start installing Conda packages, we will need to create our new virtual environment. We will name it jupyter.\n$ conda create --name jupyter\nNext we’ll want to activate the jupyter virtual environment so we can start interacting with it and installing our packages into it.\n$ conda activate jupyter\nWhen you activate a virtual environment, the name of the environment will be prepended to your shell prompt, like so:\n(jupyter) $\nNow that we have our virtual environment activated, we can install JupyterLab along with all of its dependencies.\n(jupyter) $ conda install --channel conda-forge jupyterlab\nJupyterLab has a decent number of dependencies, one of which is Python. You will notice that Conda lists those dependencies and asks if you want to proceed with installing them along with Python. This is a good time to ensure you did not enter the install command with a typo and are about to accidentally install the wrong package. If everything looks correct, respond to the prompt to proceed.\nEvery package that you install with Conda includes information within it that lists all of its dependent packages and the range of versions that it will be compatible with. We can use Conda to review what dependencies were included with JupyterLab and which version of Python has been installed within the environment.\n(jupyter) $ conda list\n# packages in environment at ...\\miniconda3\\envs\\jupyter:\n#\n# Name      Version       Build                 Channel\nanyio       3.7.1         pyhd8ed1ab_0          conda-forge\n...\npython      3.10.12       h4de0772_0_cpython    conda-forge\n...\nzipp        3.16.0        pyhd8ed1ab_1          conda-forge\nYou will notice that I have abbreviated the list quite a bit with ellipses, but I can see that JupyterLab included Python 3.10.12 as a dependency for me. You may see a newer version listed when you run this command. In a later step, we will look at how we can upgrade or downgrade Python to another version that remains compatible with JupyterLab.\nOkay, let us get JupyterLab up and running, with the following command:\n(jupyter) $ jupyter lab\nIf everything worked correctly, you will notice a new browser window has opened with JupyterLab up and ready to go and the shell in your terminal has started logging information from JupyterLab. Let us minimize our terminal for now to let it continue logging info, and we will jump into a new terminal session within JupyterLab. Because we are running JupyterLab within our virtual environment, we will not actually see the environment name prepended to the terminal prompt as we did before, but we can be confident that the virtual environment is activated within our terminal. If we wanted to double-check, we could always run the following command to list our virtual environments:\n$ conda info --envs\n# conda environments:\n#\nbase                     C:\\Users\\user\\miniconda3\njupyter               *  C:\\Users\\user\\miniconda3\\envs\\jupyter  # The * notifies us that 'jupyter' is the currently active environment\nMacOS and Linux users will see different paths listed for the locations of their virtual environments.\nFrom here, you can continue using Conda to install any other packages that you want within this environment. You could add R, Julia and their accompanying kernel packages along with some helpful JupyterLab extensions, like jupyterlab-git or jupyterlab-github. You can also use Python’s built-in package manager to install packages that are available from the Python Package Index (PyPI). Consider installing packages that you think would be useful in all of your projects.\nNow that we have setup our basic JupyterLab environment, we can clone the environment to start working on a new project, where we will install packages that are specific to that project. First, we will shutdown our JupyterLab and return to our original terminal. Then we will run the following commands to deactivate our jupyter environment and clone it to create a new environment named geog\n(jupyter) $ conda deactivate\n$ conda create -n geog --clone jupyter\nWhile cloning can be helpful for quickly copying a virtual environment on your local machine, it is not going to be much help if you want to share a virtual environment with someone else or transfer it to a different machine. To make the process easy, Conda includes the ability to take a snapshot of all the packages installed within an environment and store the name and versions of those packages into a text-based YAML (.yml) file using the following command.\n$ conda activate jupyter\n(jupyter) $ conda env export &gt; jupyter_environment.yml\nLet us make one more environment using the environment file that we just created to see how it works, and we will just list our environments again to verify that it was created.\n(jupyter) $ conda deactivate\n$ conda env create -n testenv -f jupyter_environment.yml\n$ conda info --envs\n# conda environments:\n#\nbase                   * C:\\Users\\user\\miniconda3\ngeog                     C:\\Users\\user\\miniconda3\\envs\\geog\njupyter                  C:\\Users\\user\\miniconda3\\envs\\jupyter\ntestenv                  C:\\Users\\user\\miniconda3\\envs\\testenv\nNext let us just do a little cleanup by removing the testenv environment that we just created and verify that it is no longer on our machine.\n$ conda remove -n testenv --all\n$ conda info --envs\n# conda environments:\n#\nbase                   * C:\\Users\\user\\miniconda3\ngeog                     C:\\Users\\user\\miniconda3\\envs\\geog\njupyter                  C:\\Users\\user\\miniconda3\\envs\\jupyter"
  },
  {
    "objectID": "development-environments/conda-getting-started.html#python",
    "href": "development-environments/conda-getting-started.html#python",
    "title": "Getting Started with Conda, Virtual Environments, and Python",
    "section": "Python",
    "text": "Python\nNow I want to return to my geog environment, and rather than running Python 3.10.12, which was installed as a dependency with JupyterLab, I want to run the latest Python release available. We can use the following commands to activate the geog environment and then use Conda’s search tool to see what Python versions are available.\n$ conda activate geog\n(geog) $ conda search --channel conda-forge python\nLoading channels: done\n#Name               Version         Build           Channel\npython              2.7.12          0               conda-forge\n...\npython              3.11.4          he1021f5_0      pkgs/main\nYou will notice that I have abbreviated the list quite a bit with an ellipsis, but I currently have access to a range of Python versions from 2.7.12 to 3.11.4.\nNow which Python version you install will heavily depend on the other packages and software that you intend to use within your environment. You will need to take a close look at what Python versions the packages that you want to use are compatible with.\nAt the time of writing this, the default Python version installed with JupyterLab, 3.10.12, would actually be the best option. It is usually a good choice to select one minor version older than the newest version of Python (currently 3.11.4). This ensures a broader level of compatibility with a range of packages. For many package maintainers, it can be quite a bit of work to update their code to make it compatible with the latest minor release of Python, so we can account for this by using an earlier minor release.\nToday, I am feeling a bit adventurous though, and I want to to see if I can install a newer version of Python in my geog environment. I will run the following command to update Python to the latest version compatible with JupyterLab and all of its other dependencies.\n(geog) $ conda update --channel conda-forge python\nAnd voilà, I have Python 3.11.4 installed and ready to go. A newer version of Python 3.11 may have been released since writing this, so you might actually see a newer version installed in your environment."
  },
  {
    "objectID": "cloud-computing/object-storage.html",
    "href": "cloud-computing/object-storage.html",
    "title": "Object Storage",
    "section": "",
    "text": "Object storage, also known as blob storage or even S3 storage in the context of cloud computing as it has become synonymous with Amazon’s Simple Storage Service (S3), can provide a cheap and easy-to-implement solution for storing and sharing files of widely varying sizes.\nCloud-based object storage can fluently handle file uploads when working with extraordinarily large files while reducing opportunities for data loss by providing an extremely high level of redundancy and durability. Once files are stored in an object storage bucket/container, they can be made publicly available without needing to setup and configure a web server.\nCommon use cases include:"
  },
  {
    "objectID": "cloud-computing/object-storage.html#s3-compatible-clients",
    "href": "cloud-computing/object-storage.html#s3-compatible-clients",
    "title": "Object Storage",
    "section": "S3-Compatible Clients",
    "text": "S3-Compatible Clients\nMost cloud-based object storage providers support Amazon’s S3 API meaning that they can be accessed and managed via an S3-compatible client. If you are using multiple cloud storage providers, these are helpful tool to get familiar with.\n\nCyberduck\nRclone"
  },
  {
    "objectID": "cloud-computing/object-storage.html#ubc-arc-chinook",
    "href": "cloud-computing/object-storage.html#ubc-arc-chinook",
    "title": "Object Storage",
    "section": "UBC ARC Chinook",
    "text": "UBC ARC Chinook\nUBC faculty members can access object storage via UBC ARC’s Chinook platform. Overall storage capacity is determined by allocation awards while individual files stored on the platform can be up to 5 TB. While Globus remains the recommended tool for managing object storage on Chinook, an S3 API can be enabled upon request thus giving you the ability to use the noted S3 clients above.\nFor more details about Chinook, see: Research Data Storage"
  },
  {
    "objectID": "cloud-computing/object-storage.html#digital-research-alliance-dra",
    "href": "cloud-computing/object-storage.html#digital-research-alliance-dra",
    "title": "Object Storage",
    "section": "Digital Research Alliance (DRA)",
    "text": "Digital Research Alliance (DRA)\nFaculty across Canada can also access up to 10 TB of object storage via DRA Cloud’s Arbutus data centre (University of Victoria) by submitting a Rapid Access Service (RAS) request. If those limits are not sufficient, additional resources can be requested through the annual Resource Allocation Competition (RAC) with applications due between late September and early November. Upon approval, RAC resources are granted the following April.\n\nDRA - Arbutus Object Storage\n\nProjects allocated on the Arbutus cloud are administered through an OpenStack dashboard, which provides limited functionality for managing object storage. If you need more flexibility, you’ll want to install one of the S3-compatible clients listed above or alternatively an OpenStack Swift connector or client.\n\nOpenStack Swift Client\nOpenStack provides an official client for Swift services that can be installed and ran via Python. This tool is not recommended for Windows environments as it should be ran from a Bash shell.\n# Install the openstack-swift CLI tool via Python\npython -m pip install openstack-swift\n# Download OpenStack RC File from the DRA OpenStack Dashboard via Project -&gt; API Access -&gt; Download OpenStack RC File\n# Store your DRA Project credentials as environment variables with the downloaded shell script\nsource &lt;project name&gt;-openrc.sh\n# Start uploading files/directories to your object storage container/bucket\nswift upload &lt;your_container_name&gt; &lt;path_to_directory&gt;"
  },
  {
    "objectID": "cloud-computing/object-storage.html#qgis---accessing-objects-files-in-a-private-containerbucket",
    "href": "cloud-computing/object-storage.html#qgis---accessing-objects-files-in-a-private-containerbucket",
    "title": "Object Storage",
    "section": "QGIS - Accessing Objects (Files) in a Private Container/Bucket",
    "text": "QGIS - Accessing Objects (Files) in a Private Container/Bucket\nWhile any data stored in a public container/bucket can easily be accessed and imported into QGIS using an HTTPS URL, private data can also be accessed given QGIS has access to your object storage credentials.\n\nOpenStack Swift\nSimilarly to using the OpenStack Swift client, you’ll need to download your OpenStack RC file from the dashboard by navigating to Project -&gt; API Access -&gt; Download OpenStack RC File. The RC file will include important and sensitive authentication information so be sure to properly secure it. You will need to extract the environment variables held in this file and enter them into QGIS by opening Settings -&gt; Options… -&gt; System -&gt; Environment. Be sure to check the box to use custom variables and start copying in the variables from the RC file.\n\n\n\nQGIS Settings with custom environment variables\n\n\nOnce you have copied your credentials, you will need to restart QGIS. You can then load a file from your OpenStack container by adding the layer using the OpenStack Swift Object Storage protocol type along with providing the name of the container in which the file is stored and the name of the file as the object key (note: if the file is stored in a subdirectory, the name of the directory will need to be included)\n\n\nS3-Compatible\nThe process for using an S3-compatible object storage provider, is similar to that listed above, but instead of using an OpenStack RC file and environment variables, you will need to set the following custom environment variables in QGIS, which can be attained from any S3-compatible object storage provider.\n\nAWS_SECRET_ACCESS_KEY\nAWS_ACCESS_KEY_ID\n\nFinally, while importing your file as a layer, you will need to use the AWS S3 protocol type."
  },
  {
    "objectID": "cloud-computing/object-storage.html#commercial-cloud-object-storage-providers",
    "href": "cloud-computing/object-storage.html#commercial-cloud-object-storage-providers",
    "title": "Object Storage",
    "section": "Commercial Cloud Object Storage Providers",
    "text": "Commercial Cloud Object Storage Providers\n\nAWS S3\nGoogle Cloud Storage\nMicrosoft Azure Blob Storage\nCloudflare R2\nBackblaze B2"
  },
  {
    "objectID": "development-environments/containerized-environments.html",
    "href": "development-environments/containerized-environments.html",
    "title": "Using Containers for Development Environments",
    "section": "",
    "text": "While Conda can provide extremely convenient development environments, you may run into some instances where Conda does not afford you the level of isolation required to install or run a piece of software that you need for your research. This is an area where containers can be particularly useful as they provide a valuable middle ground between a Conda virtual environment and a full virtual machine.\nFor more information about containers and a few of the container engines available, see Containers. To build and run containers on your local machine, you’ll need to have one of these engines installed."
  },
  {
    "objectID": "development-environments/containerized-environments.html#jupyterlab",
    "href": "development-environments/containerized-environments.html#jupyterlab",
    "title": "Using Containers for Development Environments",
    "section": "JupyterLab",
    "text": "JupyterLab\nThe Jupyter team maintains a core set of container images that can be pulled and used for your own development environment. These images can also be built upon to create new container images that include and software you may need.\n\nJupyter Docker Stacks Documentation\n\nDockerfiles provide instructions for creating your customized container image. These files can be easily shared and reused to build your container. The following example walks through a basic Dockerfile that builds a new image based on the jupyter/minimal-notebook image, which includes everything you need to get Jupyter up and running with all the necessities. Review the ‘Selecting an image’ when selecting a Jupyter image to build on top of.\nStart by creating a new Dockerfile with your preferred text editor.\nnano Dockerfile\nNext, within the Dockerfile, write out the list of commands you would want to run to create your development environment. Review the Dockerfile Reference for more details.\n# Always start with a FROM instruction that points to an existing image\nFROM docker.io/jupyter/minimal-notebook\n\n# Switch to the root user to enable installing packages via APT\nUSER root\n\n# Replace &lt;apt_packages&gt; with your APT dependencies\nRUN apt-get update -y && \\\n    apt-get install -y --no-install-recommends \\\n    &lt;apt_packages&gt; && \\\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n\n# Switch back to the default Jupyter user\nUSER {NB_UID}\n\n# Replace &lt;conda_forge_packages&gt; with your Conda dependencies\nRUN mamba install -y \\\n    &lt;conda_forge_packages&gt; &&\n    mamba clean --all -f -y && \\\n    fix-permissions \"${CONDA_DIR}\" && \\\n    fix-permissions \"/home/${NB_USER}\"\n\n# If any R dependencies aren't included on conda-forge, install them from CRAN.\nRUN R -q -e 'install.packages(c(&lt;CRAN_packages&gt;),\n                                repo=\"https://mirror.rcg.sfu.ca/mirror/CRAN/\")'\nBuild your customized container image using the Dockerfile. Replace  with a memorable name for your image.\ndocker build -t &lt;image_name&gt; .\nTest your image!\ndocker run -it --rm -p 8888:8888 &lt;image_name&gt;\nFollow the link provided by Jupyter in your terminal. You may need to substitute 127.0.0.1 with localhost in the URL.\nIf you prefer using Podman over Docker to run containers, you can find some additional details here for running a Jupyter container in rootless mode."
  },
  {
    "objectID": "development-environments/containerized-environments.html#rstudio",
    "href": "development-environments/containerized-environments.html#rstudio",
    "title": "Using Containers for Development Environments",
    "section": "RStudio",
    "text": "RStudio\nThe Rocker Project provides a very useful set of container images that are R/RStudio-based equivalents to the Jupyter Docker Stack.\n\nThe Rocker Images\n\nBoth the rocker/geospatial and rocker/ml-verse (geospatial with CUDA support) images are particularly worth noting as they include a wide array of geospatial packages. You can find a list of geospatial packages installed in the images here.\nTo build a new image on top of a Rocker image that includes an additional selection of packages, create a new Dockerfile with your preferred text editor.\nnano Dockerfile\nNext write out the list of commands you would want to run to create your development environment. Review the Dockerfile Reference for more details.\n# Always start with a FROM instruction that points to an existing image\nFROM docker.io/rocker/geospatial\n\n# Replace &lt;apt_packages&gt; with any system packages that you need to install using APT\nRUN apt-get update -y && \\\n    apt-get install -y --no-install-recommends \\\n    &lt;apt_packages&gt; && \\\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n\n# Note: The Rocker images do not include Conda or Mamba, so you will need to check the system requirements for each of your R packages and ensure that any required system software is installed using APT.\n\n# Rocker images come with a handy helper command, install2.r, to make installing R packages a bit simpler. The following command will only build the image if no errors are encountered, while also skipping any packages that may have already been installed and attempting to run the installation as quickly by using the maximum available CPU cores.\nRUN install2.r --error --skipinstalled --ncpus -1 \\\n    &lt;R_packages&gt; \\\n    && rm -rf /tmp/downloaded_packages\nBuild your customized container image using the Dockerfile. Replace  with a memorable name for your image.\ndocker build -t &lt;image_name&gt; .\nTest your image!\ndocker run -it --rm -e PASSWORD=&lt;your_password&gt; -p 8787:8787 &lt;image_name&gt;\nNavigate to http://localhost:8787in your browser and login to RStudio with username:rstudio` and . With RStudio running in your browser, you should find a large array of packages already installed and ready to be loaded.\nIf you prefer using Podman over Docker to run containers, follow the same instructions as above, but sign into RStudio with root as the username."
  },
  {
    "objectID": "development-environments/containerized-environments.html#vs-code",
    "href": "development-environments/containerized-environments.html#vs-code",
    "title": "Using Containers for Development Environments",
    "section": "VS Code",
    "text": "VS Code\nUsing a development container with VS Code can be easily managed using the Dev Containers extension. You can also find more information on setting up and using development containers here.\nPrior to using development containers, again consider whether a Conda/Mamba-based virtual environment would just as adequately meet your needs as this can save you from adding further complexities to your project with containers.\nWithin your project folder or local repository, create a directory named .devcontainer and within the directory create a new file named devcontainer.json. If a container image already exists that includes all the packages you need for development, you can name the image you want to use within the devcontainer.json file, like so:\n{\n  \"image\": \"docker.io/condaforge/mambaforge\"\n}\nThis would pull the condaforge/mambaforge container image that is hosted on Docker Hub and create a new container from that image as my development environment. From a terminal, you could then install packages with Mamba, but any installed packages would not be preserved once the container is stops.\nTo ensure your development environment always starts with the specific set of packages needed to run your code, you can create a Dockerfile, which would specify exactly how to create your customized development environment.\nYou will need to start by editing the devcontainer.json to match the following:\n{\n  \"build\": {\n    \"dockerfile\": \"Dockerfile\"\n  }\n}\nThen you will need to create your Dockerfile within the .devcontainer directory. Review the Dockerfile Reference for more details on creating a Dockerfile.\nThe following example would start with an Ubuntu base image and install packages using APT, Mamba/Conda, and/or R’s CRAN.\nFROM docker.io/docker/ubuntu\n\n# Replace &lt;apt_packages&gt; with your APT dependencies\nRUN apt-get update -y && \\\n    apt-get install -y --no-install-recommends \\\n    &lt;apt_packages&gt; && \\\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n\n# Replace &lt;conda_forge_packages&gt; with your Conda dependencies\nRUN mamba install -y \\\n    &lt;conda_forge_packages&gt; && \\\n    mamba clean --all -f -y\n\n# If any R dependencies aren't included on conda-forge, install them from CRAN.\nRUN R -q -e 'install.packages(c(&lt;CRAN_packages&gt;),\n                              repo=\"https://mirror.rcg.sfu.ca/mirror/CRAN/\")'\nHold down ctrl+shift+p to open VS Code’s command pallette and run the command ‘Dev Containers: Reopen in Container’ if you are creating the container for the first time or ‘Dev Containers: Rebuild and Reopen Container’ if not. You can then start editing and running your code within the development container."
  },
  {
    "objectID": "research-data-management/research-data-storage.html",
    "href": "research-data-management/research-data-storage.html",
    "title": "Research Data Storage",
    "section": "",
    "text": "This document lists current data storage options available to UBC Geography researchers and could be consulted when developing Data Management Plans for new research projects. For information and/or consultation about Research Data Management and developing a Data Management Plan, check out UBC Library RDM or UBC Advanced Research Computing. For questions about these data storage options and how best to integrate them with your data collection processes, contact UBC Geography IT.\nUBC Library’s Research Data Management team, recommends the following:\nThe storage location for each copy of your data will depend on the nature of your project, with specifics determined by how and where data will be collected and/or analyzed.\nThe ‘here copy’ can be anywhere that your data is initially collected. Common examples include:\nA ‘near copy’ should provide an easy, reliable, and preferably automated process for backing up the data in your ‘here copy.’ Common examples include:\nUBC Geography IT can provide support in selecting and acquiring an external hard drive or installing a secondary disk on your desktop machine.\n‘Far copies’ are often stored in ‘cloud’ servers, where the servers are off-site from the ‘here’ and ‘near’ copies. If you are already using a ‘cloud’ provider, like OneDrive, to store your ‘here’ and ‘near’ copies, backup the data to a local storage device or another external ‘cloud’ provider. While ‘cloud’ storage includes a high-level of redundancy with multiple off-site backups already in place, backing up your data from ‘cloud’ storage guarantees additional protections from catastrophic failure.\nIf you do not plan on using High Performance Computing to run resource intensive computations on your data, need less than 1 TB of storage, and do not anticipate storing any files above 15 GB, your UBC OneDrive account should be able to cover your needs. If you are using Windows, OneDrive is likely already installed on your device. Mac is also supported with instructions here. Use this link to learn more about using OneDrive and other Microsoft cloud services for data storage. You can also view this document to see a feature comparison of UBC’s online storage solutions.\nIf you do plan to run resource intensive computations on your data or plan on storing more than 1 TB of data, consider applying for resource allocations from either UBC Advanced Research Computing or the Digital Research Alliance. Your data can then be stored and easily analyzed using High Performance Computing resources."
  },
  {
    "objectID": "research-data-management/research-data-storage.html#hpc-options-for-ubc-faculty",
    "href": "research-data-management/research-data-storage.html#hpc-options-for-ubc-faculty",
    "title": "Research Data Storage",
    "section": "HPC Options for UBC Faculty",
    "text": "HPC Options for UBC Faculty\n\nUBC Chinook\nDocumentation: https://confluence.it.ubc.ca/display/UARC/Using+Chinook\nEligibility:\n\nMinimum: UBC Faculty or UBC Principal Investigator\nPriority:\n\nNew to UBC\nIn first 5 years of career\nData must be stored on-site and can’t be accommodated by other resources (e.g. Digital Research Alliance)\n\n\nApplication: https://arc.ubc.ca/apply-chinook\nAllocation Renewal: Annual\nLocations:\n\nUBC Vancouver\nUBC Okanagan\n\nStorage Quota:\n\nMinimum Request: 1 TB\nMaximum Request: None\n\nFile Size Limit: 5 TB\nData Transfer and Sharing Services: Globus\nTransfer Limits: Set by Globus\nSupported Computing Software for HPC Data Analysis via UBC Sockeye: https://confluence.it.ubc.ca/display/UARC/Software\n\n\nDigital Research Alliance\nDocumentation: https://docs.alliancecan.ca/wiki/Storage_and_file_management\nEligibility: Academic faculty in Canada\nApplication:\n\nAccount: https://alliancecan.ca/en/services/advanced-research-computing/account-management/apply-account\nRAS (Rapid Access Service – for fast allocations up to 10 TB): https://alliancecan.ca/en/services/advanced-research-computing/accessing-resources/rapid-access-service\nResource Allocation Competition: https://alliancecan.ca/en/services/advanced-research-computing/accessing-resources/resource-allocation-competition\n\nSubmission Start: Late September\nSubmission End: Early November\nAnnouncements: Late March\nImplementation: Early April\n\n\nAllocation Renewal: Annual\nLocations:\n\nSFU Burnaby (Cedar)\nUW Waterloo (Graham)\nMcGill Montreal (Béluga)\nUT Toronto (Niagra)\n\nStorage Quota:\n\nMinimum:\n\nHome: 50 GB per user\nProject: 1 TB per group\nNearline (Cold storage): 2 TB per group\n\nMaximum Request:\n\nVia Rapid Access Service: 10 TB\nVia Resource Allocation Competition: None\n\n\nData Transfer and Sharing Services: Globus\nAlternative Transfer Options: https://docs.alliancecan.ca/wiki/Transferring_data\nSupported Computing Software via ARC Clusters: https://docs.alliancecan.ca/wiki/Available_software"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This repository includes a listing of computing resources and accompanying documentation that can be helpful in familiarizing oneself with various tools and software that are used in Geography research, instruction, and learning.\n\nDigital Communications\n\nWordPress\nStatic Site Generators / Web Publishing\nDigital Exhibit Tools\n\nResearch Data Management\nGIS Software\nUNIX Shells & Command Line Interfaces (CLI)\nDevelopment Environments\n\nVirtual Environments\nJupyter\nRStudio\nVisual Studio Code\n\nPython\nR\nJavaScript\nDatabases\nVersion Control Systems\n\nGit\nGitHub\n\nHigh Performance Computing (HPC)\nCloud Computing\nContainers\nXR - Virtual Reality / Augmented Reality\n\nFor in-depth workshop opportunities on applying some of these resources and more relevant materials, check out the following resources:\n\nUBC Library Research Commons Workshops\nUBC Library Geographic Information Systems (GIS) Guide\nUBC Geospatial Resources\nUBC Open Geography\nEarth Lab\nThe Turing Way - handbook to reproducible, ethical and collaborative data science\n\nIf you have recommendations for additions or edits, feel free to create a pull request or reach out to kellen.malek@ubc.ca.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\n\n Back to top"
  },
  {
    "objectID": "research-data-management/index.html",
    "href": "research-data-management/index.html",
    "title": "Research Data Management",
    "section": "",
    "text": "UBC Library Research Commons - Research Data Management\nResearch Data Management Video – Part 1\nResearch Data Management Video – Part 2\nResearch Data Management in the Canadian Context: A Guide for Practitioners and Learners"
  },
  {
    "objectID": "research-data-management/index.html#recommended-gis-and-geospatial-file-formats",
    "href": "research-data-management/index.html#recommended-gis-and-geospatial-file-formats",
    "title": "Research Data Management",
    "section": "Recommended GIS and Geospatial File Formats",
    "text": "Recommended GIS and Geospatial File Formats\nGIS software and other geospatial computing software and packages can support a wide array of file formats for vector and raster data thanks in large part to the Geospatial Data Abstraction Library (GDAL). When creating, enhancing, and/or storing geospatial data, it is important to carefully select a format that best meets the needs of your project. The formats listed below focus on cases that may require broad usability among various GIS platforms and software, preservation, and performance.\nRelevant resources:\n\nOpen Geospatial Consortium Standards\nLibrary of Congress - Preservation - Recommended Formats Statement - GIS, Geospatial and Non-GIS Cartographic\nGDAL Vector Drivers\nGDAL Raster Drivers\nArcGIS - Best Practices - Imagery Formats and Performance\n\n\nVector\n\nOGC GeoPackage\nDeveloped and maintained as an OGC standard, GeoPackage has become a broadly supported format for storing and transferring GIS data. In addition to vector data, it can also store raster data. This is the recommended and default format for vector data in QGIS.\n\n\nESRI File Geodatabase (FileGDB)\nCreated by ESRI, the File Geodatabase format has been developed as an alternative to Shapefile with the intention to overcome some of its shortcomings and act as a possible successor.\n\n\nESRI Shapefile\nBy far the most popular vector format, Shapefile was developed by ESRI in the 90’s and has continued to be maintained by them. While the format is not fully open, it’s nevertheless found an extraordinary level of support among GIS and other geospatial software.\n\n\nGeoJSON\nGeoJSON provides a lightweight format that can be easily read and written via JavaScript. This format is particularly well-suited for web mapping and easily integrates with Leaflet.\n\n\nFlatGeobuf\nA relatively new format that has shown significant performance improvements compared to the formats listed above. FlatGeobuf currently lacks the backing of standardization, but it has found broad support in geospatial packages and software. It is also in early stages for proposal as an OGC Community standard.\n\n\nGeoParquet\nSimilar to FlatGeobuf, GeoParquet has recently seen a stable release with significant performance improvements compared to GeoPackage and Shapefile, and it is quickly finding support among GIS and other geospatial computing software. This format is open-source, but has yet to reach standardization. Its developers intend to propose it for adoption as an OGC standard.\n\n\n\nRaster\n\nGeoTIFF and Cloud Optimized GeoTIFF (COG)\nGeoTIFF has become the dominant format for raster data used in GIS and other geospatial computing due in large part to its development on open-source standards. It’s also often the default and recommended format in libraries like GDAL and GIS software like QGIS. The format has been further improved thanks to the development of Cloud Optimized GeoTIFFs (COG), which enhances the capacity of GeoTIFF for cloud computing and access via the web. COG files can be stored and easily accessed via S3 object stores, like that supported by UBC ARC Chinook.\nBuilt atop the TIFF file format, GeoTIFFs also support multiple compression algorithms that can significantly reduce their overall file size. Understanding and using these algorithms effectively can maximize computing resources while also mitigating any data loss.\n\nUncompressed vs Lossless vs Lossy\nGeoTIFFs can often be distributed without any compression at all. While the clear drawback to this approach is extraordinarily large file sizes, there can be benefits in ensuring that files do not need to be either encoded or decoded, which requires varying levels of computing time and in some cases libraries that are not supported by commonly used operating systems or software.\nOn the other hand, GeoTIFFs can be compressed using lossy algorithms, with the most common being JPEG. Applying a lossy algorithm can significantly reduce overall file sizes, but it comes with a large drawback, varying levels of data loss. A GeoTIFF that is compressed using JPEG will always lose some data, even if the compression quality is set to 100.\nLossless compression provides a valuable middle ground between lossy compression and using no compression at all. A GeoTIFF that uses lossless compression can see varying levels of file size reduction based on the raster data and compression algorithm used. Again a single drawback is that encoding and decoding a large lossless file can require varying amounts of computing time. Encoding files on powerful machines can reduce encoding times while serving files as COGs can help reduce some of the decoding time as only the needed portions of the file will be downloaded and decoded by end users.\nThe Translate tool found in the GDAL library is the most commonly used tool for creating GeoTIFFs and COGs, and it supports a broad set of compression algorithms. The following algorithms are important to note:\n\nLZW - This is a lossless algorithm with broad support, and it’s the current default used by GDAL. While it can be relatively fast, this algorithm is not optimized for raster data, so reductions in file size may be minimal. Nevertheless, when in doubt, this is the algorithm to choose.\nLERC - Developed and maintained by Esri, this algorithm has been optimized for raster data and can support either lossy or lossless compression. While lacking the level of support provided by LZW or JPEG, this algorithm is relatively fast and provides a valuable middle ground between the two.\nJPEG - Well supported and backed by solid, open-source standards, this has been the defacto lossy algorithm for raster data over the past 30 years. This a solid choice if you are looking to display a raster on the web and require broad support across web browsers and other software.\nWEBP - While it is well supported by modern web browsers, this algorithm is not developed on open-source standards. It supports both lossy and lossless compression. It also provides improved compression ratios over JPEG and currently provides the best option for displaying a raster on the web, but it lacks broader support in other software and will likely be replaced by JXL in the future.\nJXL - This is still a relatively new standard that has been developed as a successor to JPEG and JPEG2000. Similar to LERC and WEBP, it supports both lossy and lossless compression, but it is capable of reaching significantly better compression ratios at the expense of slower encoding and decoding times. It also has yet to reach similar levels of support as JPEG or LZW and not all distributions of GDAL include the necessary library to use this algorithm, but it is well worth monitoring in the future as its support grows."
  },
  {
    "objectID": "unix-shells-and-clis/index.html",
    "href": "unix-shells-and-clis/index.html",
    "title": "Unix Shells & Command Line Interfaces (CLI)",
    "section": "",
    "text": "Introduction to the UNIX Shell\nIntermediate UNIX Shell\nBash Course and Webinar\nIntro to Linux and the Bash Shell in HPC Environments\nIntro to Linux Shell – Video Lecture\nPro Bash\n\n\n\n\n Back to top"
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Python",
    "section": "",
    "text": "Python is one of the most popular programming languages in the world. Its low learning curve paired with a massive ecosystem has made it a powerful tool for geospatial computing. GIS software like QGIS and ArcGIS, provide the ability to extend their functionality through Python, while many programming libraries that have been created to efficiently perform geospatial computing tasks have been either written in Python, released as Python packages, and/or include application programming interfaces (APIs) that coherently communicate with Python code.\nIf you need recommendations for installing and getting started with Python on your local machine, see Getting Started with Conda, Virtual Environments, and Python."
  },
  {
    "objectID": "python/index.html#geospatial-books",
    "href": "python/index.html#geospatial-books",
    "title": "Python",
    "section": "Geospatial Books",
    "text": "Geospatial Books\n\nGeographic Data Science with Python\nLearning Geospatial Analysis with Python\nGIS Algorithms\nPython for Geospatial Data Analysis\nGeocomputation with Python\nApplied Geospatial Data Science with Python\nMastering Geospatial Analysis with Python: Explore GIS Processing and Learn to Work with GeoDjango, CARTOframes and MapboxGL-Jupyter\nMachine Learning on Geographical Data Using Python Introduction into Geodata with Applications and Use Cases\n\n\nExtending GIS Software\n\nA Geographer’s Guide to Computing Fundamentals: Python in ArcGIS Pro\nPython for ArcGIS Pro\nAdvanced Python Scripting for ArcGIS Pro\nMastering Geospatial Development with QGIS 3\nQGIS Python Programming Cookbook"
  },
  {
    "objectID": "python/index.html#geospatial-librariespackages",
    "href": "python/index.html#geospatial-librariespackages",
    "title": "Python",
    "section": "Geospatial Libraries/Packages",
    "text": "Geospatial Libraries/Packages\n\nAwesome Geospatial - Python\nGDAL Python Documentation"
  },
  {
    "objectID": "python/index.html#formatting",
    "href": "python/index.html#formatting",
    "title": "Python",
    "section": "Formatting",
    "text": "Formatting\nSometimes writing code can get a bit messy. Formatters, like black, can automatically reformat your code to make it cleaner and easier to read while following a set of standards and best practices.\n\nblack Documentation"
  },
  {
    "objectID": "python/index.html#linting",
    "href": "python/index.html#linting",
    "title": "Python",
    "section": "Linting",
    "text": "Linting\nUsing a static analysis tool, or linter, is a common best practice among programmers that helps in identifying and fixings mistakes when writing code by ensuring that you follow the correct syntax and a guiding set of best practices.\n\nflake8 Documentation"
  },
  {
    "objectID": "python/index.html#type-checking",
    "href": "python/index.html#type-checking",
    "title": "Python",
    "section": "Type Checking",
    "text": "Type Checking\nWhile Python is inherently a dynamically typed language, meaning you the programmer don’t have to worry about the data types of each variable, static types can be helpful for reducing bugs when writing complex scripts or programs. Mypy enables programmers to annotate types into their code and returns a helpful error when those types aren’t explicitly followed.\n\nmypy Documentation"
  },
  {
    "objectID": "python/index.html#testing-framework",
    "href": "python/index.html#testing-framework",
    "title": "Python",
    "section": "Testing Framework",
    "text": "Testing Framework\nSimilar to type checking, unit testing can be a helpful tool when writing large and complex scripts or programs. Testing frameworks, like pytest, which is included in Python’s standard library, enable you to define tests that can run over your functions and ensure they are following expected behavior in a range of practical scenarios.\n\npytest Documentation"
  },
  {
    "objectID": "python/index.html#web-frameworks",
    "href": "python/index.html#web-frameworks",
    "title": "Python",
    "section": "Web Frameworks",
    "text": "Web Frameworks\nWhile Python supports a large number of popular all-purpose web frameworks, including Django, Flask, and FastAPI, it also supports specialized frameworks that excel at supporting and sharing geospatial research.\n\nInteractive Visualizations and Dashboards\n\nVoila - Developed under the Jupyter Project, Voilà is a tool for converting and rendering Jupyter notebooks as interactive web applications. These applications can be deployed on a private server or via hosting providers like Binder, Railway, or Google App Engine.\nVoici - Built by the same team behind Voilà, Voici is an experimental tool that can similarly convert notebooks to interactive applications, but rather than running the Python kernel on a server, Voici shifts computation off of the server and into a user’s web browser using WebAssembly (WASM) and a static webpage. This makes hosting the application virtually free via services like GitHub Pages while also making it easier to manage and preserve. The crucial downside to note is that user’s with poor internet connections and/or limited hardware will struggle to run the applications, especially those that include large datasets and long, resource-intensive computations.\nMercury\nShiny for Python\n\nShiny for Python Ultimate - Course - Access via https://go.oreilly.com/univ-british-columbia\n\nStreamlit\nDash\nPanel\n\n\n\nGeographic Web Frameworks\n\nGeoDjango"
  },
  {
    "objectID": "python/index.html#other-related-librariespackages",
    "href": "python/index.html#other-related-librariespackages",
    "title": "Python",
    "section": "Other Related Libraries/Packages",
    "text": "Other Related Libraries/Packages\n\nSciPy\nNumPy\npandas\nscikit-learn\nmatplotlib\nseaborn\nbokeh\nplotly\nFolium\nDask\nrpy2"
  },
  {
    "objectID": "javascript/index.html",
    "href": "javascript/index.html",
    "title": "JavaScript",
    "section": "",
    "text": "While not nearly as useful as Python or R in geospatial computing, JavaScript can be a useful language for a geographer to have on their radar, especially if they are interested in creating web maps and/or sharing their research online via interactive visualizations."
  },
  {
    "objectID": "javascript/index.html#geospatial-librariespackages",
    "href": "javascript/index.html#geospatial-librariespackages",
    "title": "JavaScript",
    "section": "Geospatial Libraries/Packages",
    "text": "Geospatial Libraries/Packages\n\nAwesome Frontend GIS\n\n\nWeb Mapping\n\nAwesome Geospatial – Web Mapping\n\n\nLeaflet\nThe smallest and most popular of the three major JavaScript web mapping libraries, Leaflet lacks some of the functionalities of comparable libraries, but provides the best options for quickly rendering small to medium datasets. For additional functionality, Leaflet can be extended using a wide array of plugins.\n\nUBC Library Research Commons - Web Mapping with LeafletJS\nLeaflet Tutorials\n\n\n\nOpenLayers\nWhile nearly twice as large as Leaflet, OpenLayers includes additional functionalities and often performs better than Leaflet when working with larger datasets1.\n\nOpenLayers Documentation\n\n\n\nMapbox GL\nAt a size significantly larger than both OpenLayers and Leaflet, Mapbox GL includes a wide range of features when rendering maps and is particularly powerful when rending 3D features. Unlike Leaflet and OpenLayers, Mapbox GL is not free and open-source meaning that it must be paired with Mapbox’s APIs.\n\nMapbox GL JS Guide\n\n\n\n\nVisualizations\n\nD3 Documentation\nObservable Plot Documentation\nDeck.gl Documentation\n\n\n\nObservable JS\n\nObservable Documentation"
  },
  {
    "objectID": "javascript/index.html#formatting",
    "href": "javascript/index.html#formatting",
    "title": "JavaScript",
    "section": "Formatting",
    "text": "Formatting\nSometimes writing code can get a bit messy. Formatters, like black, can automatically reformat your code to make it cleaner and easier to read while following a set of standards and best practices.\n\nPrettier Documentation"
  },
  {
    "objectID": "javascript/index.html#linting",
    "href": "javascript/index.html#linting",
    "title": "JavaScript",
    "section": "Linting",
    "text": "Linting\nUsing a static analysis tool, or linter, is a common best practice among programmers that helps in identifying and fixings mistakes when writing code by ensuring that you follow the correct syntax and a guiding set of best practices.\n\nESLint Documentation"
  },
  {
    "objectID": "javascript/index.html#type-checking",
    "href": "javascript/index.html#type-checking",
    "title": "JavaScript",
    "section": "Type Checking",
    "text": "Type Checking\nLike Python, JavaScript is inherently a dynamically typed language, meaning you the programmer don’t have to worry about the data types of each variable. Static types can be helpful for reducing bugs when writing complex scripts or programs. The primary way to add type checking to JavaScript is to actually write in another programming language, TypeScript, which is a superset of JavaScript. Another method is to use JSDoc, which enables you to annotate your JavaScript code with types. The TypeScript transpiler then uses those comments to check your types and return errors as it finds them.\n\nJSDoc Documentation\nTypeScript Documentation - JSDoc Reference"
  },
  {
    "objectID": "javascript/index.html#testing-framework",
    "href": "javascript/index.html#testing-framework",
    "title": "JavaScript",
    "section": "Testing Framework",
    "text": "Testing Framework\nSimilar to type checking, unit testing can be a helpful tool when writing large and complex scripts or programs. Jest enables you to define tests that can run over your functions and ensure they are following expected behavior in a range of practical scenarios.\n\nJest Documentation"
  },
  {
    "objectID": "javascript/index.html#footnotes",
    "href": "javascript/index.html#footnotes",
    "title": "JavaScript",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nZunino, A., Velázquez, G., Celemín, J., Mateos, C., Hirsch, M., & Rodriguez, J. (2020). Evaluating the performance of three popular web mapping libraries: A case study using Argentina’s life quality index. ISPRS International Journal of Geo-Information, 9(10), 563. https://doi.org/10.3390/ijgi9100563↩︎"
  },
  {
    "objectID": "version-control-systems/index.html",
    "href": "version-control-systems/index.html",
    "title": "Version Control Systems",
    "section": "",
    "text": "Implementing version control is a commonly used best practice among programmers, and its usage is strongly encouraged for researchers whether they are working with large codebases or small Jupyter Notebooks. Version control is especially helpful when collaborating with other researchers or developers as it enables changes to be tracked coherently and new code to be explored separately from a stable, main codebase in easily managed branches."
  },
  {
    "objectID": "version-control-systems/index.html#code",
    "href": "version-control-systems/index.html#code",
    "title": "Version Control Systems",
    "section": "Code",
    "text": "Code\n\nGit\nGit is by far the most popular version control system and comes with a powerful command line interface while integrating with code sharing and development platforms, like GitHub and GitLab.\n\nUBC Library Research Commons - Introduction to Git and GitHub\nDigital Research Alliance - Git Tutorial and Workshops\nPro Git\n\n\n\nGitHub\nGitHub is a popular platform for sharing source code and Jupyter Notebooks as Git repositories. These can either be public and shared broadly with other users to copy (clone) or private for restricted sharing among invited users. UBC LT provides access to GitHub Enterprise which can securely share repositories among students within a single course or with other UBC collaborators.\n\nGitHub Documentation\nUBC LT - GitHub Instructor Guide\n\nGitHub Desktop provides a helpful graphical user interface that can help in managing local Git repositories and pushing changes to remote repositories on GitHub.\n\nGitHub Desktop Documentation\n\nAn additional benefit to storing your code in a public repository on GitHub is that your code can easily be archived into Zenodo, where it will receive a digital object identifier (DOI). This then makes it extraordinarily easy for your code to be cited and accessed by other researchers. If later you make revisions or improvements to your code, you can cut a new release and Zenodo will automatically update your archive and assign a new DOI.\n\nGitHub Documentation - Referencing and Citing Content\n\nWhile GitHub excels at displaying code on the web, it also supports some other interesting functionalities relevant to geospatial computing. For example, if you don’t mind storing your tabular data in CSV or GeoCSV, GitHub will render the data to an interactive table, which can be easily searched and edited.\n\nGitHub Documentation - Rendering CSV and TSV Data\n\nYou can also quickly generate basic web maps directly within your repository by storing your vector data in the GeoJSON or TopoJSON format. GitHub will render the vector data with Azure Maps and Leaflet.js. The generated web map can then be embedded on different sites with a simple snippet of JavaScript.\n\nGitHub Documentation - Mapping GeoJSON/TopoJSON files on GitHub\nGitHub GeoJSON Web Map Example\n\n\n\nIntegrations\n\nJupyterLab\n\nHow to Use the JupyterLab Git Extension\nJupyterLab-Git README\nWorking with Private GitHub Repositories from JupyterHub\n\n\n\nRStudio\n\nGitHub and RStudio\nHappy Git and GitHub for the UseR\n\n\n\nVS Code\n\nVS Code Extensions - Remote Repositories"
  },
  {
    "objectID": "version-control-systems/index.html#data",
    "href": "version-control-systems/index.html#data",
    "title": "Version Control Systems",
    "section": "Data",
    "text": "Data\nWhile version control systems are most commonly applied to code, to further ensure the reproducibility of their work, researchers have been developing and improving systems that can work fluently with their data as well. These systems are often built to work atop or adjacent to Git while adding functionality to manage and store large datasets via Git LFS or object storage.\n\nThe Turing Way - Version Control for Data\n\n\nKart\nKart is a tool that has been built on top of Git and extends its functionality to work with vector datasets while also integrating with Git LFS (Large File Storage) to handle large raster datasets. The Kart developers also provide a QGIS plugin to ensure easy integration.\n\nKart Documentation\nKart QGIS Plugin\n\n\n\nDataLad\nDatalad provides a general-purpose data version control system that supports a broad set of storage options including Microsoft OneDrive, DRA’s Arbutus object storage (OpenStack Swift), and a range of other S3-compatible object storage providers.\n\nDatalad: The Handbook\n\n\n\nDVC\nWhile applicable to a range of other data-intensive tasks, DVC is a data version control system that specializes in machine learning. It supports fewer storage options compared to Datalad, but can provide a smoother integration and setup experience in certain applications.\n\nDVC Documentation\nWestDRI - Version Control for Data Science and Machine Learning with DVC"
  },
  {
    "objectID": "cloud-computing/index.html",
    "href": "cloud-computing/index.html",
    "title": "Cloud Computing",
    "section": "",
    "text": "Similar to high performance computing (HPC), cloud computing provides hardware for running resource-intensive computations, but while HPC focuses on completing a complex set of tasks as quickly as possible with an extraordinarily powerful set of resources, cloud computing has been traditionally dedicated to providing hardware through virtual machines (VM) of varying specifications over a long period of time in order to support continuously run services such as web applications and databases."
  },
  {
    "objectID": "cloud-computing/index.html#digital-research-alliance-dra",
    "href": "cloud-computing/index.html#digital-research-alliance-dra",
    "title": "Cloud Computing",
    "section": "Digital Research Alliance (DRA)",
    "text": "Digital Research Alliance (DRA)\nCanada-based faculty and librarians with DRA accounts can create a VMs at any time of the year by submitting a Rapid Access Service (RAS) request. If the RAS resource limits noted below and in more detail here are not sufficient, additional resources can be requested through the annual Resource Allocation Competition (RAC) with applications due between late September and early November. Upon approval, RAC resources are granted the following April.\n\nDRA - Cloud\n\nThe DRA Cloud provides access two different types of VMs with different use cases in mind, which are listed below.\n\nCompute Instances\nCompute VMs are ephemeral and provide a middle-ground between running batch jobs on the DRA HPC clusters and Persistent VMs. They can provide a powerful solution when using software that cannot be ran on the HPC clusters either natively or via Apptainer or when running computations that can run for days and/or weeks.\nFor Geography-related research and instruction, these instances can be particularly useful for the following use cases:\n\nSpinning up a customized JupyterHub instance and/or RStudio server for a workshop or class\nWorking with massive datasets in a desktop application, such as QGIS, AliceVision, or Blender, where access to a GPU can be significantly beneficial\n\n\nRAS Resource Limits\n\n# of VMs: 20\nvCPUs: 80\nRAM: 300 GB\nStorage: 10 TB\nvGPU: 16 GB of memory on an Nvidia V100\n\n\n\n\nPersistent Instances\nThese VMs are ideal for running continuous computations, like web crawlers, and ongoing services, like web servers. It’s important to remember that services cannot be ran on the DRA Cloud indefinitely, so ensure you have a plan for your VM’s end-of-life (EOL) along with a set of clear security procedures before creating it. The DRA requires that researchers reassess their cloud services annually in April by submitting a form that restate the goals of the project and necessary resources for continued use.\nFor Geography-related research and instruction, these instances can be particularly useful for the following use cases:\n\nHosting a PostgresQL database with PostGIS\nRunning a web service with software like GeoServer, GeoNode, Ushahidi, or QGIS server.\nDeveloping dynamic digital exhibits or publications with Omeka and/or Scalar.\n\nAgain, when working with persistent VMs, consider any implications that may come from preserving your work once the VM is no longer available by asking yourself whether your work be easily exported, web archived, and/or hosted externally with minimal resources and no long-term maintenance?\n\nRAS Resource Limits\n\n# of VMs: 10\nvCPUs: 25\nRAM: 50 GB\nStorage: 10 TB"
  },
  {
    "objectID": "cloud-computing/index.html#ubc-it",
    "href": "cloud-computing/index.html#ubc-it",
    "title": "Cloud Computing",
    "section": "UBC IT",
    "text": "UBC IT\nIn some cases, the DRA Cloud may not be the best option for a research project, in those cases, UBC IT can provide some budget-friendly alternatives. For minimal LAMP-stack web applications, like WordPress, Drupal, or Omeka, UBC IT provides simple shared hosting solutions, while more resource-intensive applications and computation tasks can run on an EduCloud VM. Additionally, UBC IT has begun a broker system that can enable resources to be requested from large commercial cloud providers like Amazon, Microsoft, and Google with additional support.\n\nUBC IT - EduCloud Server Service\nUBC IT - Hybrid Cloud Service"
  },
  {
    "objectID": "cloud-computing/index.html#commercial-cloud-computing-providers",
    "href": "cloud-computing/index.html#commercial-cloud-computing-providers",
    "title": "Cloud Computing",
    "section": "Commercial Cloud Computing Providers",
    "text": "Commercial Cloud Computing Providers\nAlong with UBC IT’s Hybrid Cloud Services, resources from commercial cloud computing providers can be acquired with funding and credit opportunities via UBC Advanced Research Computing (ARC). Prior to using a commercial cloud computing provider, contact UBC ARC for consultation as they can provide guidance in selecting the best provider for your project and identifying any security concerns that may be applicable. This consultation can be crucial when navigating the massive and overwhelming number of services/options presented by commercial providers.\n\nAmazon Web Services (AWS)\nTo further support researchers in using allocated credits on AWS, UBC ARC provides access to RONIN, a web application that can simplify and increase the accessibility of clouding computing services.\n\nUBC ARC RONIN\nGeospatial Data Analytics on AWS\nAWS - Geospatial ML with Amazon SageMaker\n\n\n\nMicrosoft Azure\nAzure integrates smoothly with Esri software, making it a strong option for research that is heavily dependent on ArcGIS.\n\nAzure Maps\nAzure - Geospatial Data Processing and Analytics\nAzure - End-To-End Geospatial Storage, Analysis, and Visualization\n\n\n\nGoogle Cloud Platform (GCP)\n\nGoogle Cloud - Geospatial Analytics"
  },
  {
    "objectID": "xr/index.html",
    "href": "xr/index.html",
    "title": "XR - Virtual Reality / Augmented Reality",
    "section": "",
    "text": "Extended reality (XR), which encompasses virtual reality (VR), augmented reality (AR), and mixed reality (MR), is a collection of rapidly growing technologies with an increasing number of applications being researched and deployed in the field of geography."
  },
  {
    "objectID": "xr/index.html#content",
    "href": "xr/index.html#content",
    "title": "XR - Virtual Reality / Augmented Reality",
    "section": "Content",
    "text": "Content\n\nGeography-related\n\nUBC Geography Tours - AR\nUBC Geography - Stanley Park Virtual Tour\nCamosun Bog 360\nBiogeography Teaching and Research Lab - Virtual Field Trips\nThe Stanford Ocean Acidification Experience\nGoogle Earth VR\nGoogle Arts & Culture - Exhibitions\nUsability of WebXR Visualizations in Urban Planning - VR Examples\n\n\n\nOthers\n\nWebXR Samples\nA-Frame Examples\nBabylon.js Community Examples\nPlayCanvas Games Demos\nSteam VR Games"
  },
  {
    "objectID": "xr/index.html#development-tools-and-frameworks",
    "href": "xr/index.html#development-tools-and-frameworks",
    "title": "XR - Virtual Reality / Augmented Reality",
    "section": "Development Tools and Frameworks",
    "text": "Development Tools and Frameworks\nXR development has and continues to be a complex and specialized field often requiring in-depth knowledge of game development, 3D modelling, and programming languages like C# or JavaScript. When developing content for XR, identify the goals of your project and consult UBC’s Emerging Media Lab to get assistance with setting a clear scope and connecting with partners.\n360 images and video are often a good place to get started with VR content as they can be easier to create and accessible to wider audiences while still providing an immersive experience.\nFor AR, review platforms, like Echoes, which provide interfaces and support for developing and distributing virtual tours.\n\nWebXR\nWebXR is a new set of standards that enable VR and AR content to be rendered via web browsers. In some browsers, like Firefox and Safari, WebXR is still flagged as experimental, but it can be enabled through adjustments in settings.\n\nAR and VR Using the WebXR API: Learn to Create Immersive Content with WebGL, Three.js, and A-Frame\n\nDifferent frameworks have been in development over the past few years to help simplify developing VR content with WebXR and its now obsolete predecessor, WebVR.\n\nA-Frame - A popular, HTML and JavaScript-based framework for developing web-based VR content.\nBabylon.js A JavaScript-based framework for developing web-based VR Content\n\nGoing the Distance with Babylon.js\n\n\n\n\nUnity\nUnity provides provides a powerful editor game engine for developing AR and VR applications. Multiple software development kits have been developed to extend its functionality and cohesively work with GIS software.\n\nA Guide to Capturing and Preparing Photogrammetry for Unity\nUnity Development Cookbook\nUnity Virtual Reality Development with VRTK4\n\n\nRelevant Plugins and SDK\n\nArcGIS Maps SDK for Unity\nMapBox Maps SDK for Unity\nCesium for Unity Quickstart\n\n\n\n\nGodot\nWhile lacking the maturity, ease-of-use, and breadth of GIS integrations compared to Unity, the Godot game engine is an increasingly popular choice for AR and VR development in both commercial and academic settings due to it being a powerful, free and open-source option. LandscapeLab provides an excellent example of how the engine can be applied within a geography-centered setting.\n\nGodot Engine Documentation\nGodot 4 Game Development Projects\nGodot 4 Game Development Cookbook\nGame Development with Blender and Godot\n\n\nRelevant Plugins and SDK\n\nGeodot-Plugin README\nHTerrain Plugin Documentation\n\n\n\n\nOther Tools\n\nDEM.Net Elevation API - Quickly generate 3D Models from Open Street Map data\nQgis2threejs Plugin - Export DEM data in QGIS to 3D models"
  }
]