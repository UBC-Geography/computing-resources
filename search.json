[
  {
    "objectID": "containers/index.html",
    "href": "containers/index.html",
    "title": "Containers",
    "section": "",
    "text": "Similar to virtual machines, containers are a form of virtualization that package software and the multiple dependencies required to run that software into a single, distributable environment. That container can then be run on any system that is running the appropriate engine (Docker, Podman, etc.). They are often lighter and more agile than virtual machines but can include increased security risks if not deployed correctly. In general, virtual environments managed using tools like Conda as discussed in ‘Getting started with Conda, Virtual Environments, and Python’ should provide sufficient isolation for most geospatial computing cases without the need to familiarize oneself with container technologies, but containers can be a helpful tool in cases when neither virtual environments or virtual machines are feasible.\nFor examples on leveraging containers to run highly-customized development environments with Jupyter, RStudio, or VS Code, see Using Containers for Development Environments.\nIt is important to note that most containers can only run in Linux environments. Each of the container systems listed below include tools and steps for setting up simple, Linux environments from which containers can be ran on either Windows or Mac machines.\nAdditionally, a large majority of container images are built with a simplified Linux distribution (often Ubuntu) as their base image. While some images have been built with Windows Server as their base image, it is important to note that these images cannot and likely never will run a desktop environment and/or graphical user interface (GUI).",
    "crumbs": [
      "Containers"
    ]
  },
  {
    "objectID": "containers/index.html#container-images",
    "href": "containers/index.html#container-images",
    "title": "Containers",
    "section": "Container Images",
    "text": "Container Images\nThere are many open container registries, where you can find publicly shared container images. Docker Hub is one of the most popular options and can be a useful tool when searching for images. In general, you should be careful when selecting images and be sure to select images that are well-maintained. If you need a very minimal environment with Conda/Mamba, Conda-forge’s mambaforge image can be a good place to start as you’ll be able to install packages with APT or Mamba.\n\nPython\n\nJupyter - Minimal Notebook: A Jupyter server running JupyterLab and a Python kernel. No geospatial packages are included in this environment, but it is an excellent base image for creating customized Jupyter environments.\nGDS_ENV - GDS_PY: Built on top of Jupyter - Minimal Notebook, this image includes a large selection of Python geospatial packages.\n\n\n\nR\n\nJupyter - R Notebook: Similarly built from Jupyter - Minimal Notebook, this image includes an R kernel and a few R packages. Custom R environments with a preference towards JupyterLab should use this as their base image.\nGDS_ENV - GDS: Built on top of GDS_ENV - GDS_PY, this image includes an R kernel and a wide selection of R geospatial packages.\nRocker - Geospatial: Swapping JupyterLab for RStudio, this container image includes a range of R packages with a heavy focus on geospatial research.\n\n\n\nOthers\n\nPostGIS\nGeoServer\nQGIS Server",
    "crumbs": [
      "Containers"
    ]
  },
  {
    "objectID": "containers/index.html#engines",
    "href": "containers/index.html#engines",
    "title": "Containers",
    "section": "Engines",
    "text": "Engines\n\nApptainer (formerly Singularity)\nDeveloped specifically for academic and high performance computing, Apptainer avoids some of the security pitfalls of other engines, like Docker, at the expense of certain functionalities. Apptainer can only run a specific container format, but it provides all the necessary tools to complete conversions.\n\nApptainer Documentation\nRunning Apptainer on an Alliance cluster\nRunning Apptainer on UBC ARCSockeye\nUBC ARC - Apptainer Workshop 2023 - Videos: Part 1 & Part 2\n\nFor a few simple examples on using and running Apptainer, see High Performance Computing (HPC).\n\n\nPodman\nSimilar to Apptainer, Podman avoids some of the security pitfalls found in Docker, but unlike Apptainer, Podman provides a more general-purpose container engine. When developing containers, Podman can provide a more flexible and feature-rich set of tooling compared to Apptainer. A common workflow may include building containers with Podman and then converting them to Apptainer containers prior to running in an HPC environment.\n\nPodman Documentation\nPodman in Action : UBC Library | WorldCat\n\n\n\nDocker\nDocker is by far the most popular container system, so similar to Podman, it can provide a helpful tool for developing and running containers on your local machine. If you are still new to managing and running containers, Docker can be a good system to get started with because of the large collection of resources and tools that have been built around it.\n\nUBC Library Research Commons - Introduction to Docker\nDocker Documentation\nDocker: Up and Running : UBC Library | WorldCat\nPractical Docker with Python : UBC Library | WorldCat\nDocker Deep Dive : UBC Library | WorldCat",
    "crumbs": [
      "Containers"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html",
    "href": "high-performance-computing/index.html",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "",
    "text": "High-performance computing (HPC) and high-throughput computing (HTC) can provide a powerful solution when working with incredibly large datasets as they allow you and your collaborators to run scripts and programs over those datasets without facing limitations in your hardware and network speeds. UBC’s Advanced Research Computing provides access to HPC/HTC resources via their Sockeye compute cluster along with secure and redundant digital object storage via Chinook. Additionally, the Canada-based organization, Digital Research Alliance, provides compute clusters across Canada, including one located at Simon Fraser University.",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html#ubc-advanced-research-computing-sockeye",
    "href": "high-performance-computing/index.html#ubc-advanced-research-computing-sockeye",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "UBC Advanced Research Computing Sockeye",
    "text": "UBC Advanced Research Computing Sockeye\nDocumentation: https://confluence.it.ubc.ca/display/UARC/Using+Sockeye\nResource Limits:\n\nHTC:\n\n40 cores (Gold 6230), 754 GB of RAM\n\nHPC (non-blocking):\n\nCPU-Intensive:\n\n1,000 cores on 25 nodes, 186 GB of RAM per node\n\nMemory-Intensive:\n\n320 cores on 8 nodes, 754 GB of RAM per node\n640 cores on 16 nodes, 376 GB of RAM per node\n\n\n\nSupported Software: https://confluence.it.ubc.ca/display/UARC/Software\n\n\n\n\n\n\nNote\n\n\n\nLinux (Rocky Linux) software can be installed along with conda packages or any software available within an Apptainer container.",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html#digital-research-alliance-clusters",
    "href": "high-performance-computing/index.html#digital-research-alliance-clusters",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "Digital Research Alliance Clusters",
    "text": "Digital Research Alliance Clusters\nResources held on the Alliance HPC clusters are available for any Canada-based faculty or academic librarian to use. To start submitting jobs to one of the clusters listed below, register for an account with the Alliance using this link. Graduate students and university staff can also register an account and submit jobs through an active sponsorship from a registered faculty member or principal investigator.\nThe HPC clusters uses a job scheduler to manage and share resources as efficiently as possible. This means that jobs are not completed on a first-come, first-served basis, rather multiple factors can impact when a job is finally run, which can make it difficult to estimate how long a job will wait in the queue. In general running shorter jobs on fewer resources can significantly reduce wait times, so ensure that you are optimizing your code as much as possible. If you plan on using a cluster extensively (200 core-years or more) and/or need higher prioritization from the scheduler, you should review and submit an application (due at the end of October) to the Alliance’s annual Resource Allocation Competition (RAC). This additionally enables you to request access to the Niagara cluster and receive larger allocations of storage.\nDocumentation: https://docs.alliancecan.ca/wiki/Getting_started\nClusters:\n\nCedar (SFU)\n\nResource Limits:\n\nHTC:\n\nCPU-Intensive:\n\n48 cores (Platinum 8160F), 187 GB of RAM\n\nMemory-Intensive:\n\n40 cores (Gold 5215), 6,000 GB of RAM\n\n\nHPC (non-blocking):\n\nCPU-Intensive:\n\n1,536 cores on 32 nodes, 187 GB of RAM per node\n\nMemory-Intensive:\n\n96 cores on 3 nodes, 4,000 GB of RAM per node\n256 cores on 8 nodes, 1,510 GB of RAM per node\n640 cores on 20 nodes, 502 GB of RAM per node\n\n\n\n\nGraham (UW)\n\nResource Limits:\n\nHTC:\n\n64 cores (E7-4850 v4), 3,022 GB of RAM\n\nHPC (non-blocking):\n\nCPU-Intensive:\n\n1,024 cores on 32 nodes, 125 GB of RAM per node\n\nMemory-Intensive:\n\n192 cores on 3 nodes, 3,022 GB of RAM per node\n768 cores on 24 nodes, 502 GB of RAM per node\n\n\n\n\nNiagara (U of T) (RAC-only)\n\nResource Limits:\n\nHTC:\n\n40 cores (Skylake CPU), 202 GB of RAM\n\nHPC (non-blocking):\n\n17,280 cores on 432 nodes, 202 GB of RAM per node\n\n\n\nBéluga (McGill)\n\nResource Limits:\n\nHTC:\n\n40 cores (Gold 6148), 752 GB of RAM\n\nHPC (non-blocking):\n\n640 cores on 16 nodes, 752 GB of RAM per node\n\n\n\nNarval (UQ)\n\nResource Limits:\n\nHTC:\n\n64 cores (Epyc 7502), 4,000 GB of RAM\n\nHPC (non-blocking):\n\n3,584 cores on 56 nodes, 249 GB of RAM per node\n\n\n\n\nSupported Software: https://docs.alliancecan.ca/wiki/Available_software\n\n\n\n\n\n\nNote\n\n\n\nAdditionally, any software that can be installed within an Apptainer container.",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html#qgis",
    "href": "high-performance-computing/index.html#qgis",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "QGIS",
    "text": "QGIS\nQGIS can run on either UBC Sockeye or Alliance clusters. To interact with QGIS through a graphical user interface, you’ll need to run it within an interactive job with X11 forwarding enabled. You can find more information on creating graphical interactive jobs on UBC Sockeye here, while QGIS-specific documentation for Alliance clusters is listed below.\n\nAlliance Documentation - QGIS\n\nAlternatively, you can run QGIS from an Alliance cluster and access it directly through your browser by using one of the JupyterHub instances noted in the next section. Generally, we recommend requesting a Jupyter server on the Graham cluster because it provides access to more resources and is largely focused on supporting visualization workflows. You should also request at least 2 CPU cores and 4 GB of RAM to effectively run QGIS.\nOnce your Jupyter server has been allocated and started, launch the Desktop application from the JupyterLab launcher. This will open a Linux-based desktop environment, which is running on your Jupyter server resources, in separate browser tab. Next open a terminal in the desktop environment and enter the following commands to load and then start QGIS.\n$ module load gcc\n$ module load qgis\n$ qgis",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html#jupyter",
    "href": "high-performance-computing/index.html#jupyter",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "Jupyter",
    "text": "Jupyter\nA handful of Alliance clusters provide access to hardware via a JupyterHub instance. While hardware resources are significantly restricted, using Jupyter can provide an easy method for familiarizing oneself with the software and capabilities of an HPC environment. JupyterHub provides an easy-to-follow form for setting up an interactive job on a given cluster, where your Jupyter server will be running from.\n\nAlliance Documentation - JupyterHub\nWorking with Jupyter on Clusters (Alliance) Video\nClusters Running JupyterHub (requires an Alliance account):\n\nCedar (SFU)\n\nResource Limits:\n\nTime (Session Length): 30 minutes - 5 hours\nCPU Cores: 8\nMemory: 46 GB\nGPUs: 4 x V100L\n\n\nGraham (UW)\n\nResource Limits:\n\nTime (Session Length): 15 minutes - 24 hours\nCPU-Intensive:\n\nCores: 128\nMemory: 2,000 GB\nGPUs: 8 x A100\n\nMemory-Intensive:\n\nCores: 64\nMemory: 3,022 GB\nGPUs: none\n\n\n\nBéluga (McGill)\n\nResource Limits:\n\nTime (Session Length): 1 - 12 hours\nCPU Cores: 10\nMemory: 46 GB\nGPUs: 1 x V100\n\n\nNarval (UQ)\n\nResource Limits:\n\nTime (Session Length): 1 - 8 hours\nCPU Cores: 16\nMemory: 78 GB\nGPUs: 1 x A100\n\n\n\nSupported Kernels (Programming Languages):\n\nPython 2.7, 3.6, 3.7, 3.8, 3.9 (default), 3.10, and 3.11\nR 4.2\nJulia 1.5 & 1.8\n\n\n\n\n\n\n\n\nNote\n\n\n\nOther kernels can be supported using Apptainer containers.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can only install packages from Alliance’s maintained set of Python wheels. To run JupyterLab with conda or external libraries held on PYPI or CRAN, you’ll need to run it using a pre-built Apptainer container as documented in the alternative approaches below.\n\n\n\nAlternatives Approaches for Running JupyterLab:\n\nAlliance - Instructions for running JupyterLab via a virtual environment\nUBC ARC Sockeye - Instructions for running JupyterLab via an Apptainer container\n\n\n\nRunning a Jupyter Kernel from a Container\nContainers enable you to pre-build customized environments from which you can run software that is not supported on the Alliance clusters, and they can be leveraged to extend the Alliance’s JupyterHub clusters by running them as a custom Jupyter kernels. This can be extraordinarily useful considering Conda is not supported on the cluster.\nBefore using this approach, double-check that the software you need is not already available on the cluster either as modules or Python wheels. Software installed within a container is going to perform less efficiently than equivalent modules on Alliance, which have been optimized for HPC.\n\nCreating the container environment with Micromamba (Conda)\nWhile this can be done within the Alliance login node, it is highly recommended that the containers are built from another Linux environment and copied onto the cluster. This will ensure that the build runs a bit faster and you aren’t competing for resources with other researchers on the login node.\nApptainer will only run on Linux, so Windows and MacOS users will need to install a Linux virtual machine either via Windows Subsystem for Linux (WSL) or Lima to create and run Apptainer containers locally. Apptainer includes instructions in their documentation at this link.\n\n\n\n\n\n\nNote\n\n\n\nWhenever you start a new Jupyter server on the cluster, you’ll need to load the apptainer module again in order for the kernel to run.\n\n\n\nEnsure Apptainer is installed in the Linux environment\n$ apptainer --version\n\nIf Apptainer is not installed, you can use the following steps on Debian or Ubuntu\n$ sudo apt update\n$ sudo apt install -y software-properties-common\n$ sudo add-apt-repository -y ppa:apptainer/ppa\n$ sudo apt update\n$ sudo apt install -y apptainer\n\nPull the mambaorg/micromamba Docker container from Docker Hub and build a sandbox container from it. This will enable you to use Micromamba, an alternative to Conda and Mamba that runs more efficiently in environments where resources are limited.\n$ apptainer build --sandbox &lt;container_name&gt;/ docker://mambaorg/micromamba:bookworm-slim\nStart a Bash shell within the sandbox container with write privileges.\n$ apptainer shell -C --shell /bin/bash --writable &lt;container_name&gt;\nUpdate Micromamba.\nApptainer&gt; micromamba self-update\nUse Micromamba to install the kernel package needed for your preferred programming language (Python: ipykernel; R: r-irkernel) and any other packages you may need.\nApptainer&gt; micromamba install -y -q -n base -c conda-forge &lt;kernel_package&gt; &lt;other_packages&gt;\nClean up any extra files that are no longer needed\nApptainer&gt; micromamba clean --all -y\nExit the sandbox container shell\nApptainer&gt; exit\nBuild your Apptainer container from the sandbox container. Including a time stamp with &lt;container_name&gt; is recommended when building a container as you may want to rebuild the container with updated packages in the future.\n$ apptainer build &lt;container_name&gt;_&lt;timestamp&gt;.sif &lt;container_name&gt;\n\nIf you need flexibility in your container to add and remove packages, you can create a sandbox environment from your uploaded container and modify the sandbox container while accessing the cluster on a login node.\n\n\nInstalling a Micromamba Python-Based Container as a Kernel\n\nStart a server on the JupyterHub cluster.\nStart a new terminal.\nLoad the apptainer module.\n$ module load apptainer\nClose the terminal and start a new one.\nInstall the container as a Python kernel\n$ python -m ipykernel install --user --name &lt;container_name&gt; --display-name=\"Python (&lt;container_name&gt;)\"\nThe previous step provided an initial configuration for the kernel, but we will need to slightly modify it to ensure the kernel executes from the container.\n$ nano /home/&lt;username&gt;/.local/share/jupyter/kernels/&lt;container_name&gt;/kernel.json\nModify kernel.json file to match the following:\n\n\nkernel.json\n\n{\n  \"argv\": [\n    \"apptainer\",\n    \"exec\",\n    \"/home/&lt;username&gt;/&lt;container_name&gt;.sif\",\n    \"micromamba\",\n    \"run\",\n    \"python\",\n    \"-m\",\n    \"ipykernel_launcher\",\n    \"-f\",\n    \"{connection_file}\"\n  ],\n  \"display_name\": \"Python (&lt;container_name&gt;)\",\n  \"language\": \"python\",\n  \"metadata\": {\n    \"debugger\": true\n  }\n}\n\nClose the terminal and wait a few seconds for the kernel to register in the JupyterLab launcher.\n\n\n\nInstalling an R-Based Container as a Kernel\n\nStart a server on the JupyterHub cluster.\nStart a new terminal.\nLoad the apptainer module.\n$ module load apptainer\nClose the terminal and start a new one.\nUsing the nano text editor, create a new directory and file to store the custom Jupyter kernel configuration\n$ nano &lt;container_name&gt;/kernel.json\nWithin the kernel.json file, enter the following:\n\n\nkernel.json\n\n{\n  \"argv\": [\n    \"apptainer\",\n    \"exec\",\n    \"/home/&lt;username&gt;/&lt;container_name&gt;.sif\",\n    \"micromamba\",\n    \"run\",\n    \"R\",\n    \"--slave\",\n    \"-e\",\n    \"IRkernel::main()\",\n    \"--args\",\n    \"{connection_file}\"\n  ],\n  \"display_name\": \"R (&lt;container_name&gt;)\",\n  \"language\": \"R\",\n  \"metadata\": {\n    \"debugger\": true\n  }\n}\n\nInstall the container as a custom kernel using the kernel.json file.\n$ jupyter kernelspec install &lt;container_name&gt; --user\nClose the terminal and wait a few seconds for the kernel to register in the JupyterLab launcher.",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html#rstudio-server",
    "href": "high-performance-computing/index.html#rstudio-server",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "RStudio Server",
    "text": "RStudio Server\n\nvia JupyterHub\nThe easiest way to start an RStudio session in a HPC environment is to launch it within a JupyterLab session. See the entry above for session limits and documentation on starting a JupyterLab session from a JupyterHub cluster. Once your JupyterLab session is running, select the Software tab in the sidebar and find/load the rstudio-server module. You can then click the RStudio launcher, to open a new RStudio session. While you can install any R library from CRAN, you can’t use package managers like conda and can only use software loaded as modules from the Alliance. If you need more flexibility for your RStudio session, setup and run your environment within an Apptainer container as documented below.\n\n\nvia Container\n\n\n\n\n\n\nNote\n\n\n\nthe linked instructions above cover running RStudio Server on the UBC ARC Sockeye cluster using Apptainer, but the instructions should be extremely similar for running on an Alliance cluster.\n\n\n\nInstructions for creating an Apptainer container with pre-installed dependencies:\n\nYou can either build your container on a local Linux system and copy the container to the cluster or build it directly on a login node.\nExample of building an RStudio container on a local Linux (Ubuntu) system\n# Install Apptainer if not already done so\n$ sudo apt update\n$ sudo apt install -y software-properties-common\n$ sudo add-apt-repository -y ppa:apptainer/ppa\n$ sudo apt update\n$ sudo apt install -y apptainer\n# Build a new sandbox container from one of the Rocker Project images\n# The geospatial images includes a range of geospatial packages\n$ apptainer build --sandbox rstudio/ docker://rocker/geospatial\n# Run a shell within the sandbox container with write and sudo privileges\n$ apptainer shell --writable --fakeroot rstudio/\n# Start R and install any additional packages that may be needed\nApptainer&gt; R\nR version 4.4.1 (2024-06-14) -- \"Race for Your Life\"\n...\n&gt; install.packages('climatol')\n...\n&gt; q()\nApptainer&gt; exit\n# Convert the sandbox container into a SIF container to be transferred to the HPC cluster\n$ apptainer build rstudio.sif rstudio",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html#python",
    "href": "high-performance-computing/index.html#python",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "Python",
    "text": "Python\nPrior to running Python in an HTC/HPC environment, ensure you have optimized your code as much as possible. You can find resources relevant to parallelizing Python code via system and external libraries, like multiprocessing, Dask, and mpi4py, under Python - Increasing Performance.\n\nAlliance Documentation - Python\nSupported Versions: 2.7, 3.6, 3.7, 3.8, 3.9, 3.10, 3.11 and 3.12\nSupported Virtual Environment Managers:\n\nUBC ARC Sockeye:\n\nconda, virtualenv, and venv\nUsing Virtual Environments on Sockeye\n\nAlliance: virtualenv or venv. conda is not supported, but it can be run using an Apptainer container. In general, Alliance recommends avoiding conda-forge and Anaconda packages if possible. Go here for more details. See instructions below for creating and running micromamba, a lightweight, drop-in replacement for conda, from an Apptainer container:\n\nCreating an Apptainer container with conda and Anaconda packages:\n# Pull the micromamba Docker container and create a new sandbox container from it\n$ apptainer build --sandbox container/ docker://mambaorg/micromamba:bookworm-slim\n# Run a shell with write access in the sandbox container\n$ apptainer shell --writable -C --shell /bin/bash\n# Update micromamba\nApptainer&gt; micromamba self-update\n# Install some packages from conda-forge\nApptainer&gt; micromamba install -n base python=3.11 &lt;package_names&gt;\nApptainer&gt; exit\n# Convert the sandbox container into a SIF container\n$ apptainer build &lt;container_name&gt;_&lt;timestamp&gt;.sif container/\nSLURM job script for running a Python script within the container created above:\n#!/bin/bash\n# Include the shebang at the top of the file, to clearly identify that the following script is meant for the Bash Shell\n# Provide flags as comments in the script. These will be read by SLURM to set option flags\n# Identify your user account\n#SBATCH --account=def-someuser\n# Identify the amount of memory to use per CPU\n#SBATCH --mem-per-cpu=1.5G  # In this case the job will only use one CPU with 1.5 GB of memory\n#SBATCH --time=1:00:00 # And I expect that the job will take a little less than an hour\n# At the beginning of your job load the Apptainer module\n$ module load apptainer\n# Run a Python script using an environment within your conda container\n$ apptainer run -C -B &lt;directory holding Python script&gt; -W $SLURM_TMPDIR &lt;container_name&gt;_&lt;timestamp&gt;.sif micromamba run python &lt;script&gt;\n\n\nSupported Packages (Alliance): https://docs.alliancecan.ca/wiki/Available_Python_wheels\n\nNote: include --no-index flag with pip to only install Alliance wheels. The Alliance Python wheels have been specifically compiled and optimized to run as effectively on HPC clusters as possible.\n\nExample Job Script:\n#!/bin/bash\n# Include the shebang at the top of the file, to clearly identify that the following script is meant for the Bash Shell\n# Provide flags as comments in the script. These will be read by SLURM to set option flags\n# Identify your user account\n#SBATCH --account=def-someuser\n# Identify the amount of memory to use per CPU\n#SBATCH --mem-per-cpu=1.5G  # In this case the job will only use one CPU with 1.5 GB of memory\n#SBATCH --time=1:00:00 # And I expect that the job will take a little less than an hour\n# At the beginning of your job load any software dependencies needed for your job\nmodule load python/3.10\n# If running Python, create and activate a virual environment\nvirtualenv --no-download $SLURM_TMPDIR/venv\nsource $SLURM_TMPDIR/venv/bin/activate\n# Upgrade pip and install Python dependencies using Alliance wheels listed as dependencies in requirements.txt\npython -m pip install --no-index --upgrade pip\npython -m pip install --no-index -r requirements.txt\n# Run the script\npython -m script.py\n# Deactivate the virtual environment\ndeactivate\nInteractive Sessions:\n\nSockeye\nAlliance\n\nUse the Slurm allocation (salloc) command\nExample command for starting a single core IPython session on Alliance:\n$ salloc --time=00:15:00\nsalloc: Pending job allocation 1234567\n...\nsalloc: Nodes cdr&lt;###&gt; are ready for your job\n$ module load gcc/9.3.0 python/3.10\n$ virtualenv --no-download venv\n$ source venv/bin/activate\n(venv) $ python -m pip install --no-index ipython\n(venv) $ ipython\nPython 3.10.2 ...\n...\nIn [1]: &lt;python code&gt;\n...\nIn [2]: exit\n(venv) $ deactivate\n$ exit\nsalloc: Relinquishing job allocation 1234567\n\n\n\n\nOther Resources\n\nWorking with the Python Dask (Parallelization) Library Video",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html#r",
    "href": "high-performance-computing/index.html#r",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "R",
    "text": "R\nPrior to running R in an HTC/HPC environment, ensure you have optimized your code as much as possible. You can find resources relevant to parallelizing R code via system and external libraries, like parallel, data.table and snowfall, under R - Increasing Performance.\n\nAlliance Documentation - R\nSupported Versions: 4.0, 4.1, 4.2, 4.3, and 4.4\nExample Job:\n#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --mem-per-cpu=1.5G\n#SBATCH --time=1:00:00\n####################################################\nmodule load gcc/12.3 r/4.4.0\nRscript script.r\nInteractive Sessions:\n\nSockeye\nAlliance\n\nUse the Slurm allocation (salloc) command\nExample command for starting session:\n$ salloc --time=00:15:00\nsalloc: Pending job allocation 1234567\n...\nsalloc: Nodes cdr&lt;###&gt; are ready for your job\n$ module load gcc/12.3 r/4.4.0\n$ R\nR version 4.4.0 (2024-04-24) -- \"Puppy Cup\"\n...\n&gt; &lt;r code here&gt;\n&gt; q()\nSave workspace image? [y/n/c]:\n$ exit\nsalloc: Relinquishing job allocation 1234567\n\n\n\n\nOther Resources:\n\nIntroduction to HPC in R Webinar\nHigh-performance R Tutorial",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html#other-supported-languages",
    "href": "high-performance-computing/index.html#other-supported-languages",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "Other Supported Languages",
    "text": "Other Supported Languages\n\nC, C++, Objective-C, and other GCC supported languages\nElixir (Alliance)\nGo (Alliance)\nJava\nJavaScript/TypeScript via node.js\nJulia\nMATLAB\nOctave\nPerl\nRuby (Alliance)\nRust (Alliance)",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html#containers-on-hpc-htc",
    "href": "high-performance-computing/index.html#containers-on-hpc-htc",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "Containers on HPC / HTC",
    "text": "Containers on HPC / HTC\n\nUse Cases:\n\nRunning software that is not already included as a module on a HPC cluster or HTC node\nBuilding runtime environments from existing projects and easily reproducing research\n\nDocumentation\n\nAlliance\nSockeye\n\nDocumentation for HPC Container Platform - Apptainer",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html#photogrammetry",
    "href": "high-performance-computing/index.html#photogrammetry",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "Photogrammetry",
    "text": "Photogrammetry\n\nPhotogrammetry on HPC Clusters\n\nPhotogrammetry software can be incredibly resource intensive, so running it on an HPC cluster can save you a lot of time and resources when working with extraordinarily large datasets.\nFor an assortment of reasons, most HPC clusters only install free and open-source software. This includes UBC ARC’s Sockeye cluster and the Digital Research Alliance’s clusters. This means that software like Agisoft Metashape, ArcGIS Drone2Map, and Pix4D are unfortunately not available for use on HPC clusters.\nTo leverage HPC clusters for processing drone images or video into orthophotos, 3D models, point clouds, or elevation models, Open Drone Map (ODM) will likely be your best option.\nODM is most often distributed within a Docker container, and it can just as easily run within alternative container engines, like Apptainer or Podman. Prior to running ODM on an HPC cluster, it is highly recommended that you take a few test runs on a local machine with a subset of your data. This will give you an opportunity to explore and optimize any option flags to produce the best results for your dataset. You can find information on installing Apptainer here.\nTo test ODM on a local machine that has Apptainer installed, make a directory that will be bound to the container for ODM’s input and output. Then nest another directory named ‘images’ with a subset of your data stored within it. The example below uses a directory named ‘odm_test’.\n$ apptainer run -B odm_test:/project/code docker://opendronemap/odm:latest --project-path /project\nAfter processing your dataset, ODM creates a report at ‘odm_test/odm_report/report.pdf’, which can be used to quickly analyze your results and start calculating a total job time estimate for your HPC job script.\nOnce you’ve identified any flags that you want to add to the command, you will need to convert the ODM Docker container into an Apptainer SIF container with the following command on your local machine:\n$ apptainer build odm_latest.sif docker://opendronemap/odm:latest\n# For a container with GPU support, run:\n# apptainer build odm_gpu.sif docker://opendronemap/odm:gpu\nThen copy the Apptainer SIF file from your local machine to your HPC home directory and ensure your dataset is stored in a project directory.\nyou can use the following in a SLURM job script on the HPC cluster:\n#!/bin/bash\n# Include the shebang at the top of the file, to clearly identify that the following script is meant for the Bash Shell\n# Provide flags as comments in the script. These will be read by SLURM to set option flags\n# Identify your user account\n#SBATCH --account=def-someuser\n# Identify the amount of CPU cores and memory to allocate the job\n#SBATCH --cpus-per-task=20\n#SBATCH --mem=64G\n#SBATCH --time=5:00:00 # You can use the test runs on your local machine to help you estimate this\n# At the beginning of your job load the Apptainer module\nmodule load apptainer\n# CPU Only\napptainer run -C -W $SLURM_TMPDIR -B /project/images:/project/code/images,/scratch:/project odm_latest.sif --project-path /project\n# For a container with GPU support add the following to the top of your job script along with the following command\n#SBATCH --gpu-per-node=1\n# apptainer run -C -W $SLURM_TMPDIR -B /project/images:/project/code/images,/scratch:/project --nv odm_gpu.sif --project-path /project\nThe command in the above job script may need to be slightly adjusted based on the structure of the HPC cluster that you are using with the job script parameters also being modified to account for the overall size of your dataset. In general, the above parameters should work well for a dataset of approximately 1000 images1. You can also review the following table to help estimate the amount of memory to allocate your job based on the size of your dataset.\n\n\n\nNumber of Images\nRAM or RAM + Swap\n\n\n\n\n40\n4\n\n\n250\n16\n\n\n500\n32\n\n\n1,500\n64\n\n\n2,500\n128\n\n\n3,500\n192\n\n\n5,000\n256\n\n\n\nSource: https://docs.opendronemap.org/installation/#hardware-recommendations\nBased on best practices, you will want to store your dataset within your project folder and bind the directory holding your dataset to a directory within the container with the path ‘/project/code/images’. You will also want to set ODM to output to your scratch directory by binding it to the ODM project directory. Finally, flag ODM to use the bound scratch directory as the project output directory.\nOnce the job has been completed, you will need to review ODMs output within the scratch directory and transfer any relevant files back into your project directory before cleaning up the scratch directory.",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "high-performance-computing/index.html#footnotes",
    "href": "high-performance-computing/index.html#footnotes",
    "title": "High-Performance Computing / High-Throughput Computing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGbagir, A. G., Ek, K., & Colpaert, A. (2023). OpenDroneMap: Multi-platform performance analysis. Geographies, 3(3), 446-458. https://doi.org/10.3390/geographies3030023↩︎",
    "crumbs": [
      "High-Performance Computing (HPC) / High-Throughput Computing (HTC)"
    ]
  },
  {
    "objectID": "databases/index.html",
    "href": "databases/index.html",
    "title": "Databases",
    "section": "",
    "text": "Databases, and particularly spatial databases as a piece of GIS software, play a crucial role in storing and accessing geospatial data. Multiple relational database management systems can be extended to support spatial databases, while common geospatial data formats like GeoPackage and Esri’s Geodatabase are self-contained databases.\nUnderstanding how databases and how to query them efficiently with SQL, is an extraordinarily useful skill when working with geospatial data.",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "databases/index.html#duckdb",
    "href": "databases/index.html#duckdb",
    "title": "Databases",
    "section": "DuckDB",
    "text": "DuckDB\nWhen running memory-intensive workloads, DuckDB is a powerful tool to have on hand. Unlike many other database management systems, which focus on data storage, DuckDB focuses on data analysis, and it includes easy installation methods for Python, R, and JavaScript environments.\n\nGetting Started with DuckDB : UBC Library | WorldCat\nDuckDB: Up and Running : UBC Library | WorldCat\nDuckDB Documentation\nDuckDB - Spatial Extension",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "databases/index.html#postgresql",
    "href": "databases/index.html#postgresql",
    "title": "Databases",
    "section": "PostgreSQL",
    "text": "PostgreSQL\nDue to the PostGIS extension, PostgreSQL is one of the most commonly used database management systems for storing and accessing geospatial data. GIS software, like GeoServer and QGIS, provide tight integrations around PostGIS-enabled databases.\n\nQGIS Manual - Database Concepts with PostgreSQL\nLearn PostgreSQL : UBC Library | WorldCat\nPostgreSQL Query Optimization : UBC Library | WorldCat\n\n\nHosting\nWhile it’s possible to host a PostgreSQL database on your local machine, to take full advantage of a relational database management system, you’ll want to host the database on its own server. The Digital Research Alliance provides free database hosting for researchers based in Canada through their cloud computing services, and alternatively, many commercial cloud computing and software-as-a-service providers include specialized services for hosting databases.\n\nAlliance\n\nDatabase Servers – Cedar PostgreSQL\nWestDRI - How to create and access MySQL and PostgreSQL databases on DRI systems\n\n\n\nCloud / SaaS Providers\n\nMicrosoft Azure\nAWS\nGoogle Cloud\nSupabase\nRailway\n\n\n\n\nGIS Extensions / Libraries\n\nPostGIS\n\nIntroduction to PostGIS\nPostGIS in Action : UBC Library | WorldCat\nPostGIS Cookbook : UBC Library | WorldCat\nMastering PostGIS : UBC Library | WorldCat\n\npgRouting",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "databases/index.html#mysql-and-mariadb",
    "href": "databases/index.html#mysql-and-mariadb",
    "title": "Databases",
    "section": "MySQL and MariaDB",
    "text": "MySQL and MariaDB\nWhile not nearly as a popular in geospatial computing as PostgreSQL, MySQL and its closely-related fork, MariaDB, are extraordinarily popular database management systems, which remains a fundamental building block for many open-source web applications, like WordPress, Drupal, Omeka-S, and Scalar.\n\nHosting\nSimilar to PostgreSQL, both the Alliance cloud and numerous commercial cloud providers include support for hosting and managing instances of a MySQL server.\n\nAlliance\n\nDatabase Servers – Cedar MySQL\n\n\n\nCloud / SaaS Providers\n\nMicrosoft Azure\nAWS\nGoogle Cloud\nPlanetScale\nRailway\n\n\n\n\nGIS Extensions / Libraries\n\nMySQL Spatial",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "javascript/index.html",
    "href": "javascript/index.html",
    "title": "JavaScript",
    "section": "",
    "text": "While JavaScript has fewer applications in geography compared to other languages like Python and R, it remains a useful skill to develop, particularly for interacting with Google Earth Engine or creating interactive web maps and web-based visualizations.\nIt’s important to note that JavaScript code can be executed in various and often quite different contexts with the most common being web browsers (front-end or client-side) and runtime engines (back-end or server-side), like Node.js and Deno. Most applications of JavaScript relevant to geographers occurs in the former with the use of libraries, like Leaflet, D3.js, and Observable.\nThe following resources will provide general introductions to JavaScript and its applications within the context of a web browser, so any of them can be useful for learning the basics of JavaScript.",
    "crumbs": [
      "JavaScript"
    ]
  },
  {
    "objectID": "javascript/index.html#geospatial-librariespackages",
    "href": "javascript/index.html#geospatial-librariespackages",
    "title": "JavaScript",
    "section": "Geospatial Libraries/Packages",
    "text": "Geospatial Libraries/Packages\nA relatively comprehensive list of libraries that focus on using JavaScript in the web browser can be found below:\n\nAwesome Frontend GIS\n\nAdditionally, you may find a few more libraries along with packages that run in Node.js through the following list:\n\nAwesome Geospatial - JavaScript\n\n\nGoogle Earth Engine (GEE)\nThe code editor that is built into Google Earth Engine acts as a JavaScript runtime or playground. Through the code editor, you can write JavaScript scripts that interact with the Google Earth Engine API to access and analyze data. Computational resources to the GEE Code Editor may be too limited for some computational work. In those cases, you can transfer your code to your local machine and run it from an environment where Node.js and the GEE API library are installed. If you have Miniforge installed on your machine, you can use the following commands to quickly set up an environment using mamba.\n$ mamba create -n nodejs nodejs\n$ mamba activate nodejs\n$ npm install --save @google/earthengine\nThen you can following these instructions to setup a service account for your Google Cloud Project and authenticate to the API in your JavaScript script.\n\nGoogle Earth Engine Guides\nCloud-Based Remote Sensing with Google Earth Engine\n\n\n\nWeb Mapping\nDeveloping interactive maps that can be shared via the web is one of the most commonly used applications of JavaScript in geography. A few resources and libraries used for developing web maps are listed in the link below, followed by resources associated with the four major web mapping libraries.\n\nAwesome Geospatial – Web Mapping\n\nUBC Library Research Commons frequently provides workshops on web mapping using some of the tools listed below. You can find upcoming workshops here.\nAll web maps include at least one basemap, which helps contextualize the geospatial data layered on top of it. While other web mapping tools often provide a set of basemaps for the user to from, when setting up a web map using JavaScript, you’ll need to identify and load a basemap from a basemap provider’s map tile server. One of the most popular providers is OpenStreetMap, which provides a freely accessible map tile server with a very lenient Tile Usage Policy. Alternatively basemaps can be self-hosted using software, like OpenFreeMap on an Alliance Cloud VM, or through file formats like PMTiles on the Alliance’s Arbutus Object Storage.\nWhen selecting a basemap, it’s important to compare whether your tiles should be built using raster or vector data. Maptiler provides a great overview of the differences between raster and vector map tiles within the context of web mapping.\n\nLeaflet\nThe smallest and most popular of the four major JavaScript web mapping libraries, Leaflet lacks some of the functionalities of comparable libraries, but provides the best options for quickly rendering small to medium datasets. For additional functionality, Leaflet can be extended using a wide array of plugins.\n\n\n\n\n\n\nNote\n\n\n\nWhile either raster tiles or PMTiles can work extraordinarily well as Leaflet basemaps, vector tiles that rely on the Mapbox Vector Tile specification are currently only supported in Leaflet by loading MapLibre GL, an alternative web map library as listed below, and another package that binds them. If you plan on using Mapbox Vector Tiles for your basemap, it may be more efficient to rely on either OpenLayers with the ol-mapbox-style plugin or MapLibre GL.\n\n\n\nUBC Library Research Commons - Web Mapping with LeafletJS\nLeaflet Tutorials\n\n\n\nOpenLayers\nWhile nearly twice as large as Leaflet, OpenLayers includes additional functionalities and often performs better than Leaflet when working with larger datasets1.\n\nOpenLayers Documentation\n\n\n\nMapbox GL\nAt a size significantly larger than both OpenLayers and Leaflet, Mapbox GL includes a wide range of features when rendering maps and is particularly powerful when rending 3D features. Unlike Leaflet and OpenLayers, Mapbox GL is not free and open-source meaning that it must be paired with Mapbox’s APIs.\n\nMapbox GL JS Guide\n\n\n\nMapLibre GL JS\nMapLibre is a free and open-source fork of Mapbox GL that retains some of the unique functionalities of Mapbox GL while enabling the usage of alternative datasets and APIs.\n\nMapLibre GL JS Documentation",
    "crumbs": [
      "JavaScript"
    ]
  },
  {
    "objectID": "javascript/index.html#interactive-visualizations-and-dashboards",
    "href": "javascript/index.html#interactive-visualizations-and-dashboards",
    "title": "JavaScript",
    "section": "Interactive Visualizations and Dashboards",
    "text": "Interactive Visualizations and Dashboards\nJavaScript excels at creating interactive visualizations, and similar to web mapping, there is a massive number of libraries that can make development easier. Resources for a few of the most popular libraries are listed below.\n\nD3 Documentation\nD3.js in Action : UBC Library | WorldCat\nObservable Plot Documentation\nDeck.gl Documentation\nPlotly - JavaScript\n\n\nObservable JS\nObservable provides an interactive alternative to Jupyter with computational work completed within a user’s browser rather than on a server. Tools like Quarto integrate smoothly with Observable.\n\nObservable Documentation\nQuarto - Observable JS",
    "crumbs": [
      "JavaScript"
    ]
  },
  {
    "objectID": "javascript/index.html#code-quality-tools",
    "href": "javascript/index.html#code-quality-tools",
    "title": "JavaScript",
    "section": "Code Quality Tools",
    "text": "Code Quality Tools\n\nFormatting\nSometimes writing code can get a bit messy. Formatters can automatically reformat your code to make it cleaner and easier to read while following a set of standards and best practices.\n\nPrettier Documentation\n\n\n\nLinting\nUsing a static analysis tool, or linter, is a common best practice among programmers that helps in identifying and fixings mistakes when writing code by ensuring that you follow the correct syntax and a guiding set of best practices.\n\nESLint Documentation\n\n\n\nType Checking\nLike Python, JavaScript is inherently a dynamically typed language, meaning you the programmer don’t have to worry about the data types of each variable. Static types can be helpful for reducing bugs when writing complex scripts or programs. The primary way to add type checking to JavaScript is to actually write in another programming language, TypeScript, which is a superset of JavaScript. Another method is to use JSDoc, which enables you to annotate your JavaScript code with types. The TypeScript transpiler then uses those comments to check your types and return errors as it finds them.\n\nJSDoc Documentation\nTypeScript Documentation - JSDoc Reference\n\n\n\nTesting Framework\nSimilar to type checking, unit testing can be a helpful tool when writing large and complex scripts or programs. Jest enables you to define tests that can run over your functions and ensure they are following expected behavior in a range of practical scenarios.\n\nJest Documentation",
    "crumbs": [
      "JavaScript"
    ]
  },
  {
    "objectID": "javascript/index.html#footnotes",
    "href": "javascript/index.html#footnotes",
    "title": "JavaScript",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nZunino, A., Velázquez, G., Celemín, J., Mateos, C., Hirsch, M., & Rodriguez, J. (2020). Evaluating the performance of three popular web mapping libraries: A case study using Argentina’s life quality index. ISPRS International Journal of Geo-Information, 9(10), 563. https://doi.org/10.3390/ijgi9100563↩︎",
    "crumbs": [
      "JavaScript"
    ]
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Python",
    "section": "",
    "text": "Python is one of the most popular programming languages in the world. Its low learning curve paired with a massive ecosystem has made it a powerful tool for geospatial computing. GIS software like QGIS and ArcGIS, provide the ability to extend their functionality through Python, while many programming libraries that have been created to efficiently perform geospatial computing tasks have been either written in Python, released as Python packages, and/or include application programming interfaces (APIs) that coherently communicate with Python code.\nIf you need recommendations for installing and getting started with Python on your local machine, see Getting Started with Conda, Virtual Environments, and Python.\nThe following resources will provide general introductions to Python and its applications within data science, so any of them can be useful for learning the basics of Python.\nIf you are looking for a more in-depth introduction to Python, UBC Extended Learning provides multiple courses that can develop confidence in the language while learning best practices for development and maintenance.",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/index.html#geospatial-books",
    "href": "python/index.html#geospatial-books",
    "title": "Python",
    "section": "Geospatial Books",
    "text": "Geospatial Books\n\nCarpentries Incubator - Introduction to Geospatial Raster and Vector Data with Python\nGeographic Data Science with Python\nPython for Geographic Data Analysis\nProject Pythia\nLearning Geospatial Analysis with Python : UBC Library | WorldCat\nEarth Observation Using Python : UBC Library | WorldCat\nGIS Algorithms : UBC Library | WorldCat\nPython for Geospatial Data Analysis : UBC Library | WorldCat\nGeocomputation with Python\nApplied Geospatial Data Science with Python : UBC Library | WorldCat\nMastering Geospatial Analysis with Python : UBC Library | WorldCat\nMachine Learning on Geographical Data Using Python : UBC Library | WorldCat\n\n\nExtending GIS Software\n\nA Geographer’s Guide to Computing Fundamentals: Python in ArcGIS Pro : UBC Library | WorldCat\nPython for ArcGIS Pro : UBC Library | WorldCat\nAdvanced Python Scripting for ArcGIS Pro : UBC Library | WorldCat\nMastering Geospatial Development with QGIS 3 : UBC Library | WorldCat\nQGIS Python Programming Cookbook : UBC Library | WorldCat",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/index.html#geospatial-librariespackages",
    "href": "python/index.html#geospatial-librariespackages",
    "title": "Python",
    "section": "Geospatial Libraries/Packages",
    "text": "Geospatial Libraries/Packages\nA relatively comprehensive list of packages can be found in the Python section of Awesome Geospatial.\nMany of the packages included in that list provide powerful, easy-to-use functionalities around GDAL. If you need to write code that directly interacts with GDAL, Python bindings are provided, which can be explored through the GDAL documentation.\nYou may also find some other helpful packages listed in Python for Data Science - Geodata.",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/index.html#interactive-visualizations-and-dashboards",
    "href": "python/index.html#interactive-visualizations-and-dashboards",
    "title": "Python",
    "section": "Interactive Visualizations and Dashboards",
    "text": "Interactive Visualizations and Dashboards\nDashboard libraries enable you to create interactive visualizations from either Python code or Jupyter Notebooks, which can be hosted on sparsely-resourced servers and shared as web applications. You can find information about some of the most popular libraries at PyViz - Dashboarding tools.\n\nOpensource Computing for Earth Science Education (OCESE) Project - Developing Dashboards\n\nResources for developing interactive maps via a dashboarding library are listed below:\n\nVoilà & Voici - ipyleaflet\nShiny for Python - ipyleaflet\nDisplaying Interactive Maps in Streamlit\nDash - Plotly Maps\nPanel - Windturbine example & hvPlot - Geographic data\n\nAlternatively, libraries like Voici can similarly convert notebooks to interactive applications, but rather than running the Python code on a server, Voici shifts computation off of the server and into a user’s web browser using WebAssembly (WASM) and a static webpage. This makes hosting the application virtually free via services like GitHub Pages while also making it easier to manage and preserve. The crucial downside to note is that user’s with poor internet connections and/or limited hardware will struggle to run the applications, especially those that include large datasets and long, resource-intensive computations.",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/index.html#increasing-performance",
    "href": "python/index.html#increasing-performance",
    "title": "Python",
    "section": "Increasing Performance",
    "text": "Increasing Performance\nCompared to other languages, Python isn’t considered particularly fast due to its focus on ease and speed of development over performance. For many geography-based researchers, development workflows first focus on developing code that performs accurate analysis and then later improving the code as needed to ensure the analysis runs as efficiently as possible in both local and HTC environments before finally adapting code to work in an HPC environment if needed.\n\nCarpentries Incubator - Parallel Programming in Python\nWorking with the Python Dask library\nGeoPandas-Dask Documentation\nHigh Performance Python : UBC Library | WorldCat\nFast Python : UBC Library | WorldCat\nAdvanced Python Programming : UBC Library | WorldCat\nMPI for Python Documentation\nAlliance Documentation - MPI for Python",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/index.html#code-quality-tools",
    "href": "python/index.html#code-quality-tools",
    "title": "Python",
    "section": "Code Quality Tools",
    "text": "Code Quality Tools\nEach of the tools listed below can be easily integrated with most development environments and set up to run automatically. When working with Jupyter Notebooks, we strongly encourage you to install nbqa alongside these tools in order to ensure code quality within your notebooks.\n\nFormatting\nSometimes writing code can get a bit messy. Formatters, like black, can automatically reformat your code to make it cleaner and easier to read while following a set of standards and best practices.\n\nblack Documentation\n\n\n\nLinting\nUsing a static analysis tool, or linter, is a common best practice among programmers that helps in identifying and fixings mistakes when writing code by ensuring that you follow the correct syntax and a guiding set of best practices.\n\nflake8 Documentation\nruff Documentation\n\n\n\nType Checking\nWhile Python is inherently a dynamically typed language, meaning you the programmer don’t have to worry about the data types of each variable, static types can be helpful for reducing bugs when writing complex scripts or programs. Mypy enables programmers to annotate types into their code and returns a helpful error when those types aren’t explicitly followed.\n\nmypy Documentation\n\n\n\nTesting Framework\nSimilar to type checking, unit testing can be a helpful tool when writing large and complex scripts or programs. Testing frameworks, like pytest, which is included in Python’s standard library, enable you to define tests that can run over your functions and ensure they are following expected behavior in a range of practical scenarios.\n\npytest Documentation",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "unix-shells-and-clis/index.html",
    "href": "unix-shells-and-clis/index.html",
    "title": "Unix Shells & Command Line Interfaces (CLI)",
    "section": "",
    "text": "Having confidence when using a Unix shell, particularly Bash, is crucial when working within an HPC environment. It can also be extraordinarily helpful when setting up development environments, running scripts in Python or R, or using containers and command line tools.\nVia their Opensource Computing for Earth Science Education (OCESE) Project, UBC’s Department of Earth, Ocean and Atmospheric Science has compiled an excellent and concise tutorial for both Bash and its Windows equivalent, Powershell, on the Command Line & Shells page.\nIf you are a Windows user looking to install Bash, you can find a helpful walkthrough here.\nUBC Library Research Commons frequently runs introductory workshops on Bash, which are listed here. And the materials for these workshops are available below:\nAdditionally, the SFU’s Research Computing Group provides a full-day workshop on Bash during their annual Summer School in early June with some of the materials for that course available below:\nAdditional resources:",
    "crumbs": [
      "UNIX Shells and CLIs"
    ]
  },
  {
    "objectID": "unix-shells-and-clis/index.html#running-mamba-on-git-bash-for-windows",
    "href": "unix-shells-and-clis/index.html#running-mamba-on-git-bash-for-windows",
    "title": "Unix Shells & Command Line Interfaces (CLI)",
    "section": "Running mamba on Git-Bash for Windows",
    "text": "Running mamba on Git-Bash for Windows\nWhen installing mamba via Miniforge with the recommended options, it is not included in your PATH by default, so you’ll only be able use it by running Miniforge Prompt from your Start Menu. For most folks that might be just fine, but we recommend adding both the mamba and conda commands to your Git-Bash environment, so you can more easily take advantage of Bash and Git.\nStart by locating the ‘profile.d’ folder that was installed with Miniforge via the File Explorer. Most likely this would located at: C:\\Users\\&lt;your_username&gt;\\miniforge3\\etc. Right click the ‘profile.d’ folder and select ‘Open Git Bash here’. In the Git Bash terminal, run the following commands:\n$ echo \"source '${PWD}/conda.sh'\" &gt;&gt; ~/.bashrc\n$ echo \"source '${PWD}/mamba.sh'\" &gt;&gt; ~/.bashrc\nThen restart your Bash session and check that mamba works:\n$ source ~/.bashrc\n$ mamba --version\nYou should see output telling you the versions of mamba and conda that are installed.",
    "crumbs": [
      "UNIX Shells and CLIs"
    ]
  },
  {
    "objectID": "unix-shells-and-clis/index.html#command-line-tools",
    "href": "unix-shells-and-clis/index.html#command-line-tools",
    "title": "Unix Shells & Command Line Interfaces (CLI)",
    "section": "Command Line Tools",
    "text": "Command Line Tools\n\nGDAL\nGDAL is used by a variety of GIS software to read and write geospatial data, and it is one of the most commonly used geospatial libraries. A range of functionalities can be accessed through the various CLI tools that accompany it, including conversions between a large collection of raster and vector data formats.\n\nGDAL Documentation\n\nYou can install GDAL from the conda-forge repository using conda or mamba with one of the following commands:\n\nMambaConda\n\n\n$ mamba install libgdal -y\n\n\n$ conda install -c conda-forge libgdal -y\n\n\n\n\n\nImageMagick\nWhen processing large collections of images, ImageMagick is a very popular tool with an extraordinary amount of functionality. It’s commonly used to convert, resize, and optimize images files in various formats.\nIf you are using Windows, you can find an installer here, which will enable you to use ImageMagick from your preferred shell. Mac OS and Linux users can install ImageMagick from the conda-forge repository using conda or mamba with one of the following commands:\n\nMambaConda\n\n\n$ mamba install imagemagick -y\n\n\n$ conda install -c conda-forge imagemagick -y\n\n\n\n\nImageMagick Documentation\nCommand-line image processing with ImageMagick\n\n\n\nFFmpeg\nFFmpeg is an extremely powerful tool for batch processing both video and audio files. It can be installed using one of the following commands:\n\nMambaConda\n\n\n$ mamba install ffmpeg -y\n\n\n$ conda install -c conda-forge ffmpeg -y\n\n\n\n\nFFmpeg Documentation\nQuick Start Guide to FFmpeg UBC Library | WorldCat",
    "crumbs": [
      "UNIX Shells and CLIs"
    ]
  },
  {
    "objectID": "research-data-management/index.html",
    "href": "research-data-management/index.html",
    "title": "Research Data Management",
    "section": "",
    "text": "UBC Library Research Commons - Research Data Management\nResearch Data Management Video – Part 1\nResearch Data Management Video – Part 2\nResearch Data Management in the Canadian Context: A Guide for Practitioners and Learners",
    "crumbs": [
      "Research Data Management",
      "Overview"
    ]
  },
  {
    "objectID": "research-data-management/index.html#recommended-gis-and-geospatial-file-formats",
    "href": "research-data-management/index.html#recommended-gis-and-geospatial-file-formats",
    "title": "Research Data Management",
    "section": "Recommended GIS and Geospatial File Formats",
    "text": "Recommended GIS and Geospatial File Formats\nGIS software and other geospatial computing software and packages can support a wide array of file formats for vector and raster data thanks in large part to the Geospatial Data Abstraction Library (GDAL). When creating, enhancing, and/or storing geospatial data, it is important to carefully select a format that best meets the needs of your project. The formats listed below focus on cases that may require broad usability among various GIS platforms and software, preservation, and performance.\nRelevant resources:\n\nOpen Geospatial Consortium Standards\nLibrary of Congress - Preservation - Recommended Formats Statement - GIS, Geospatial and Non-GIS Cartographic\nGDAL Vector Drivers\nGDAL Raster Drivers\nArcGIS - Best Practices - Imagery Formats and Performance\n\n\nVector\n\nOGC GeoPackage\nDeveloped and maintained as an OGC standard, GeoPackage has become a broadly supported format for storing and transferring GIS data. In addition to vector data, it can also store raster data. This is the recommended and default format for vector data in QGIS.\n\n\nESRI File Geodatabase (FileGDB)\nCreated by ESRI, the File Geodatabase format has been developed as an alternative to Shapefile with the intention to overcome some of its shortcomings and act as a possible successor.\n\n\nESRI Shapefile\nBy far the most popular vector format, Shapefile was developed by ESRI in the 90’s and has continued to be maintained by them. While the format is not fully open, it has nevertheless found an extraordinary level of support among GIS and other geospatial software.\n\n\nGeoJSON\nGeoJSON provides a lightweight format that can be easily read and written via JavaScript. This format is particularly well-suited for web mapping and easily integrates with web mapping libraries, like Leaflet and OpenLayers.\n\n\nFlatGeobuf\nA relatively new format that has shown significant performance improvements compared to the formats listed above. FlatGeobuf currently lacks the backing of standardization, but it has found broad support in geospatial packages and software. It is also currently under review as a proposed OGC Community Standard.\n\n\nGeoParquet\nSimilar to FlatGeobuf, GeoParquet has recently seen a stable release with significant performance improvements compared to GeoPackage and Shapefile, and it is quickly finding support among GIS and other geospatial computing software. This format is open-source, but has yet to reach standardization. Its developers intend to propose it for adoption as an OGC standard.\n\n\n\nRaster\n\nGeoTIFF and Cloud Optimized GeoTIFF (COG)\nGeoTIFF has become the dominant format for raster data used in GIS and other geospatial computing due in large part to its development on open-source standards. It is also often the default and recommended format in libraries like GDAL and GIS software like QGIS. The format has been further improved thanks to the development of Cloud Optimized GeoTIFFs (COG), which enhances the capacity of GeoTIFF for cloud computing and access via the web. COG files can be stored and easily accessed via S3 object stores, like that supported by UBC ARC Chinook and the DRA’s Arbutus Object Storage.\nBuilt atop the TIFF file format, GeoTIFFs also support multiple compression algorithms that can significantly reduce their overall file size. Understanding and using these algorithms effectively can maximize computing resources while also mitigating any unnecessary data loss.\n\nUncompressed vs Lossless vs Lossy\nGeoTIFFs can often be distributed without any compression at all. While the clear drawback to this approach is extraordinarily large file sizes, there can be benefits in ensuring that files do not need to be either encoded or decoded, which requires varying levels of computing time and in some cases libraries that are not supported by commonly used operating systems or software.\nOn the other hand, GeoTIFFs can be compressed using lossy algorithms, with the most common being JPEG. Applying a lossy algorithm can significantly reduce overall file sizes, but it comes with a large drawback, varying levels of data loss. A GeoTIFF that is compressed using JPEG will always lose some data, even if the compression quality is set to 100.\nLossless compression provides a valuable middle ground between lossy compression and using no compression at all. A GeoTIFF that uses lossless compression can see varying levels of reduction in file size based on the raster data and compression algorithm used. Again a single drawback is that encoding and decoding a large lossless file can require varying amounts of computing time. Encoding files on powerful machines can reduce encoding times while serving files as COGs can help reduce some of the decoding time as only the needed portions of the file will be downloaded and decoded by end users.\nThe Translate tool found in the GDAL library is the most commonly used tool for creating GeoTIFFs and COGs, and it supports a broad set of compression algorithms. The following algorithms are important to note:\n\nLZW - This is a lossless algorithm with broad support, and it’s the current default used by GDAL. While it can be relatively fast, this algorithm is not optimized for raster data, so reductions in file size may be minimal. Nevertheless, when in doubt, this is the algorithm to choose.\nLERC - Developed and maintained by Esri, this algorithm has been optimized for raster data and can support either lossy or lossless compression. While lacking the level of support provided by LZW or JPEG, this algorithm is relatively fast and provides a valuable middle ground between the two.\nJPEG - Well supported and backed by solid, open-source standards, this has been the defacto lossy algorithm for raster data over the past 30 years. This a solid choice if you are looking to display a raster on the web and require broad support across web browsers and other software.\nWEBP - While it is well supported by modern web browsers, this algorithm is not developed on open-source standards. It supports both lossy and lossless compression. It also provides improved compression ratios over JPEG and currently provides the best option for displaying a raster on the web, but it lacks broader support in other software and will likely be replaced by JXL in the future.\nJXL - This is still a relatively new standard that has been developed as a successor to JPEG and JPEG2000. Similar to LERC and WEBP, it supports both lossy and lossless compression, but it is capable of reaching significantly better compression ratios at the expense of slower encoding and decoding times. It also has yet to reach similar levels of support as JPEG or LZW and not all distributions of GDAL include the necessary library to use this algorithm, but it is well worth monitoring in the future as its support grows.",
    "crumbs": [
      "Research Data Management",
      "Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This repository includes a listing of computing resources and accompanying documentation that can be helpful in familiarizing oneself with various tools and software that are used in Geography research, instruction, and learning.\nFor in-depth workshop opportunities relevant to these resources and other useful materials, check out the following resources:\n\nUBC Library Research Commons Workshops\nUBC Library Geographic Information Systems (GIS) Guide\nUBC Geospatial Resources\nEarth Lab\nThe Turing Way - handbook to reproducible, ethical and collaborative data science\n\nIf you are a faculty member within our department, and you are seeking information on acquiring local computing hardware, you can can contact the Geography IT team for advice and support in procuring, setting up, and maintaining laptops and workstations.\nFor research computing, review Cloud Computing and High-Performance Computing (HPC) / High-Throughput Computing (HTC) to identify available resources through UBC Advanced Research Computing and the Digital Research Alliance.\nAnd for instructional use, our department hosts and manages two computer labs that are specialized to fit the instructional needs of our faculty. Additionally, UBC Learning Technology (LT) Hub maintains a JupyterHub instance that can be remotely accessed via a web browser and is open to all UBC students. Each student is allocated a single CPU core, 3 GB of RAM, and 10 GB of storage in an environment where programming languages, like Python, R, and Julia are pre-installed to run scripts and computational notebooks.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "research-data-management/research-data-storage.html",
    "href": "research-data-management/research-data-storage.html",
    "title": "Research Data Storage",
    "section": "",
    "text": "UBC researchers have access to multiple options for their research data. Selecting a storage option can be dependent on the nature of the data itself (privacy and size) along with the location of research collaborators, personal preferences, and processes used for data collection. When developing a Data Management Plan (DMP), consider all of these factors when selecting the best options for your research.\nFor information and/or consultation about Research Data Management (RDM) and developing a DMP, check out UBC Library RDM or UBC Advanced Research Computing. For initial questions about data storage options and how best to integrate them with your data collection processes, contact UBC Geography IT.\nUBC Library’s Research Data Management team, recommends the following:\n\nYou should have at least 3 copies of your data\n\nThe here copy, which can be your working copy.\nThe near copy, which is a local backup. Perhaps an external hard drive.\nThe far copy that is stored off-site and ideally accessible from a computer other than yours.\n\nThe copies should be stored in 2 different locations, like your backup disk and the cloud. Please don’t put two extra copies on the same backup disk!\n\nThe storage location for each copy of your data will depend on the nature of your project, with specifics determined by how and where data will be collected and/or analyzed.\nThe ‘here copy’ can be anywhere that your data is initially collected. Common examples include:\n\nthe hard drive on your laptop or workstation,\na volume connected to a virtual machine,\nor an object storage bucket on cloud provider’s infrastructure.\n\nA ‘near copy’ should provide an easy, reliable, and preferably automated process for backing up the data in your ‘here copy.’ Common examples include:\n\nan external hard drive connected to your laptop,\na secondary drive/disk within your workstation or desktop,\na local Network Attached Storage drive located in your office or lab,\na UBC provided network attached drive, like UBC Home Drive or UBC TeamShare,\n\nUBC Geography IT can provide support in selecting an external hard drive for your laptop or installing a secondary disk on your desktop.\n‘Far copies’ are often stored in ‘cloud’ servers, where the servers are off-site from the ‘here’ and ‘near’ copies. If you are already using a ‘cloud’ provider, like OneDrive, to store your ‘here’ and ‘near’ copies, backup the data to a local storage device. While ‘cloud’ storage includes a high-level of redundancy with multiple off-site backups already in place, backing up your data from ‘cloud’ storage guarantees additional protections from various types of catastrophic failure.\n\nNetwork, Cloud, and HPC Storage\n\n\nService\nStorage per User\nStorage per Allocation\nCosts\n\n\n\n\nUBC Home Drive\n20-100 GB\n\n20 GB free then $0.35/GB/year\n\n\nUBC OneDrive\n1 TB\n\nNone\n\n\nUBC TeamShare\n\n20 GB or more\n$0.15/GB/year\n\n\nARC Sockeye\n50 GB\n5 TB+\nNone\n\n\nARC Chinook\n\nDetermined by allocation\nNone\n\n\nAlliance NextCloud\n50 GB\n\nNone\n\n\nAlliance HPC\n50 GB\n1 TB+\nNone\n\n\nAlliance Cloud Arbutus Object\n\n1 GB+\nNone\n\n\nAlliance Cloud Virtual Machine\n\n20 GB+\nNone\n\n\n\n\nA common and cost-effective setup for researchers that are collecting data on their local machine, includes downloading the OneDrive desktop client, setting up automatic synchronization on any folders that contains research data, connecting a second drive to the machine, and running either daily or weekly backups onto the secondary drive.\nFor use cases, where the overall amount of data being collected and analyzed nears or exceeds 1TB, contact Geography IT about setting up a RAID array with your local machine and submitting a request to either UBC ARC or the Digital Research Alliance for an allocation of HPC/object storage.",
    "crumbs": [
      "Research Data Management",
      "Research Data Storage"
    ]
  },
  {
    "objectID": "development-environments/containerized-environments.html",
    "href": "development-environments/containerized-environments.html",
    "title": "Using Containers for Development Environments",
    "section": "",
    "text": "While Conda can provide extremely convenient development environments, you may run into some instances where Conda does not afford you the level of isolation required to install or run a piece of software that you need for your research. This is an area where containers can be particularly useful as they provide a valuable middle ground between a Conda virtual environment and a full virtual machine.\nFor more information about containers and a few of the container engines available, see Containers. To build and run containers on your local machine, you’ll need to have one of these engines installed.",
    "crumbs": [
      "Development Environments",
      "Containerized Environments"
    ]
  },
  {
    "objectID": "development-environments/containerized-environments.html#jupyterlab",
    "href": "development-environments/containerized-environments.html#jupyterlab",
    "title": "Using Containers for Development Environments",
    "section": "JupyterLab",
    "text": "JupyterLab\nThe Jupyter team maintains a core set of container images that can be pulled and used for your own development environment. These images can also be built upon to create new container images that include and software you may need.\n\nJupyter Docker Stacks Documentation\n\nDockerfiles provide instructions for creating your customized container image. These files can be easily shared and reused to build your container. While initially developed around Docker, Dockerfiles are also compatible with other container engines like Podman and Apptainer. The following example walks through a basic Dockerfile that builds a new image based on the jupyter/minimal-notebook image, which includes everything you need to get Jupyter up and running with all the necessities. Review the ‘Selecting an image’ when selecting a Jupyter image to build on top of.\nWithin a Dockerfile, you’ll write out the list of commands you would want to run to create your development environment. Review the Dockerfile Reference for more details.\n\n\nDockerfile\n\n# Always start with a FROM instruction that points to an existing image\nFROM docker.io/jupyter/minimal-notebook\n\n# Switch to the root user to enable installing packages via APT\nUSER root\n\n# Replace &lt;apt_packages&gt; with your APT dependencies\nRUN apt-get update -y && \\\n    apt-get install -y --no-install-recommends \\\n    &lt;apt_packages&gt; && \\\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n\n# Switch back to the default Jupyter user\nUSER {NB_UID}\n\n# Replace &lt;conda_forge_packages&gt; with your Conda dependencies\nRUN mamba install -y \\\n    &lt;conda_forge_packages&gt; &&\n    mamba clean --all -f -y && \\\n    fix-permissions \"${CONDA_DIR}\" && \\\n    fix-permissions \"/home/${NB_USER}\"\n\n# If any R dependencies aren't included on conda-forge, install them from CRAN.\nRUN R -q -e 'install.packages(c(&lt;CRAN_packages&gt;),repo=\"https://mirror.rcg.sfu.ca/mirror/CRAN/\")'\n\nBuild your customized container image using the Dockerfile. Replace  with a memorable name for your image.\ndocker build -t &lt;image_name&gt; .\nTest your image!\ndocker run -it --rm -p 8888:8888 &lt;image_name&gt;\nFollow the link provided by Jupyter in your terminal. You may need to substitute 127.0.0.1 with localhost in the URL.\nIf you prefer using Podman over Docker to run containers, you can find some additional details here for running a Jupyter container in rootless mode.",
    "crumbs": [
      "Development Environments",
      "Containerized Environments"
    ]
  },
  {
    "objectID": "development-environments/containerized-environments.html#rstudio",
    "href": "development-environments/containerized-environments.html#rstudio",
    "title": "Using Containers for Development Environments",
    "section": "RStudio",
    "text": "RStudio\nThe Rocker Project provides a very useful set of container images that are R/RStudio-based equivalents to the Jupyter Docker Stack.\n\nThe Rocker Images\n\nBoth the rocker/geospatial and rocker/ml-verse (geospatial with CUDA support) images are particularly worth noting as they include a wide array of geospatial packages. You can find a list of geospatial packages installed in the images here.\nTo build a new image on top of a Rocker image that includes an additional selection of packages, you’ll write out the list of commands you would want to run to create your development environment. Review the Dockerfile Reference for more details.\n\n\nDockerfile\n\n# Always start with a FROM instruction that points to an existing image\nFROM docker.io/rocker/geospatial\n\n# Replace &lt;apt_packages&gt; with any system packages that you need to install using\n# APT\nRUN apt-get update -y && \\\n    apt-get install -y --no-install-recommends \\\n    &lt;apt_packages&gt; && \\\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n\n# Note: The Rocker images do not include Conda or Mamba, so you will need to\n# check the system requirements for each of your R packages and ensure that any\n# required system software is installed using APT.\n\n# Rocker images come with a handy helper command, install2.r, to make installing\n# R packages a bit simpler. The following command will only build the image if\n# no errors are encountered, while also skipping any packages that may have\n# already been installed and attempting to run the installation as quickly by\n# using the maximum available CPU cores.\nRUN install2.r --error --skipinstalled --ncpus -1 \\\n    &lt;r_packages&gt; \\\n    && rm -rf /tmp/downloaded_packages\n\nBuild your customized container image using the Dockerfile. Replace  with a memorable name for your image.\ndocker build -t &lt;image_name&gt; .\nTest your image!\ndocker run -it --rm -e PASSWORD=&lt;your_password&gt; -p 8787:8787 &lt;image_name&gt;\nNavigate to http://localhost:8787in your browser and login to RStudio with username:rstudio` and . With RStudio running in your browser, you should find a large array of packages already installed and ready to be loaded.\nIf you prefer using Podman over Docker to run containers, follow the same instructions as above, but sign into RStudio with root as the username.",
    "crumbs": [
      "Development Environments",
      "Containerized Environments"
    ]
  },
  {
    "objectID": "development-environments/containerized-environments.html#vs-code",
    "href": "development-environments/containerized-environments.html#vs-code",
    "title": "Using Containers for Development Environments",
    "section": "VS Code",
    "text": "VS Code\nUsing a development container with VS Code can be easily managed using the Dev Containers extension. You can also find more information on setting up and using development containers here.\nPrior to using development containers, again consider whether a Conda/Mamba-based virtual environment would just as adequately meet your needs as this can save you from adding further complexities to your project with containers.\nWithin your project folder or local repository, create a directory named .devcontainer and within the directory create a new file named devcontainer.json. If a container image already exists that includes all the packages you need for development, you can name the image you want to use within the devcontainer.json file, like so:\n\n\n.devcontainer/devcontainer.json\n\n{\n  \"image\": \"docker.io/condaforge/mambaforge\"\n}\n\nThis would pull the condaforge/mambaforge container image that is hosted on Docker Hub and create a new container from that image as my development environment. From a terminal, you could then install packages with Mamba, but any installed packages would not be preserved once the container is stops.\nTo ensure your development environment always starts with the specific set of packages needed to run your code, you can create a Dockerfile, which would specify exactly how to create your customized development environment.\nYou will need to start by editing the devcontainer.json to match the following:\n\n\n.devcontainer/devcontainer.json\n\n{\n  \"build\": {\n    \"dockerfile\": \"Dockerfile\"\n  }\n}\n\nThen you will need to create your Dockerfile within the .devcontainer directory. Review the Dockerfile Reference for more details on creating a Dockerfile.\nThe following example would start with an Ubuntu base image and install packages using APT, Mamba/Conda, and/or R’s CRAN.\n\n\nDockerfile\n\nFROM docker.io/docker/ubuntu\n\n# Replace &lt;apt_packages&gt; with your APT dependencies\nRUN apt-get update -y && \\\n    apt-get install -y --no-install-recommends \\\n    &lt;apt_packages&gt; && \\\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n\n# Replace &lt;conda_forge_packages&gt; with your Conda dependencies\nRUN mamba install -y \\\n    &lt;conda_forge_packages&gt; && \\\n    mamba clean --all -f -y\n\n# If any R dependencies aren't included on conda-forge, install them from CRAN.\nRUN R -q -e 'install.packages(c(&lt;CRAN_packages&gt;),\n                              repo=\"https://mirror.rcg.sfu.ca/mirror/CRAN/\")'\n\nHold down ctrl+shift+p to open VS Code’s command pallette and run the command ‘Dev Containers: Reopen in Container’ if you are creating the container for the first time or ‘Dev Containers: Rebuild and Reopen Container’ if not. You can then start editing and running your code within the development container.",
    "crumbs": [
      "Development Environments",
      "Containerized Environments"
    ]
  },
  {
    "objectID": "cloud-computing/object-storage.html",
    "href": "cloud-computing/object-storage.html",
    "title": "Object Storage",
    "section": "",
    "text": "Object storage, also known as blob storage or even S3 storage in the context of cloud computing as it has become synonymous with Amazon’s Simple Storage Service (S3), can provide a cheap and easy-to-implement solution for storing and sharing files of widely varying sizes.\nCloud-based object storage can fluently handle file uploads when working with extraordinarily large files while reducing opportunities for data loss by providing an extremely high level of redundancy and durability. Once files are stored in an object storage bucket/container, they can be made publicly available without needing to setup and configure a web server.\nCommon use cases include:",
    "crumbs": [
      "Cloud Computing",
      "Object Storage"
    ]
  },
  {
    "objectID": "cloud-computing/object-storage.html#s3-compatible-clients",
    "href": "cloud-computing/object-storage.html#s3-compatible-clients",
    "title": "Object Storage",
    "section": "S3-Compatible Clients",
    "text": "S3-Compatible Clients\nMost cloud-based object storage providers support Amazon’s S3 API meaning that they can be accessed and managed via an S3-compatible client. If you are using multiple cloud storage providers, these are helpful tool to get familiar with.\n\nCyberduck\nRclone",
    "crumbs": [
      "Cloud Computing",
      "Object Storage"
    ]
  },
  {
    "objectID": "cloud-computing/object-storage.html#ubc-arc-chinook",
    "href": "cloud-computing/object-storage.html#ubc-arc-chinook",
    "title": "Object Storage",
    "section": "UBC ARC Chinook",
    "text": "UBC ARC Chinook\nUBC faculty members can access object storage via UBC ARC’s Chinook platform. Overall storage capacity is determined by allocation awards while individual files stored on the platform can be up to 5 TB. While Globus remains the recommended tool for managing object storage on Chinook, an S3 API can be enabled upon request thus giving you the ability to use the noted S3 clients above.\nFor more details about Chinook, see: Research Data Storage",
    "crumbs": [
      "Cloud Computing",
      "Object Storage"
    ]
  },
  {
    "objectID": "cloud-computing/object-storage.html#digital-research-alliance-dra",
    "href": "cloud-computing/object-storage.html#digital-research-alliance-dra",
    "title": "Object Storage",
    "section": "Digital Research Alliance (DRA)",
    "text": "Digital Research Alliance (DRA)\nFaculty across Canada can also access up to 10 TB of object storage via DRA Cloud’s Arbutus data centre (University of Victoria) by submitting a Rapid Access Service (RAS) request. If those limits are not sufficient, additional resources can be requested through the annual Resource Allocation Competition (RAC) with applications due between late September and early November. Upon approval, RAC resources are granted the following April.\nYou can additionally use the RAS request form to share access to your Arbutus storage with other Alliance users, including sponsored staff and graduate students.\n\n\n\n\n\n\nNote\n\n\n\nThe Alliance doesn’t provide backups or redundancy for object storage, so if plan on storing any data on Arbutus be sure to retain at least one copy on your local machine and preferably another copy on a local backup drive.\n\n\n\nDRA - Arbutus Object Storage\n\nOnce the Alliance has fulfilled your RAS request and allocated you with a new project. You’ll be able start managing your project by signing in to the Arbutus Dashboard using your Alliance username and password.\nBefore you can start uploading files, you’ll need to first create a new container. You can think of a container as a folder or directory in which you’ll be storing your uploaded files. Within the Arbutus Dashboard, the button for creating a new container can be found under Project &gt; Object Store &gt; Containers. When you create a new container, you’ll be prompted to provide a container name, select a storage policy, and identify whether the container should be public.\n\n\n\nDRA - Arbutus Dashboard - Create container\n\n\nArbutus uses container names when generating URLs for all files stored within it. This means your container name will need to be unique and should be URL-friendly. A good example of a container name avoids symbols and spaces to ensure generated URLs avoid unnecessary encoding and are easier to read while also noting if the contents of the container are public and open for anyone to view via their URL.\nOnce a container has been created, you can select it and start uploading files using the upload button and begin creating sub-folders to help organize your files.\n\n\n\nDRA - Arbutus Dashboard - Upload files to container\n\n\nThe Arbutus Dashboard can work well for completing basic tasks like creating new containers and uploading small individual files, but for more advanced use cases, like needing to upload files that exceed 500 MB or wanting to upload multiple files at once, you’ll want to install one of the S3-compatible clients listed above or alternatively an OpenStack Swift connector or client.\n\nCyberduck OpenStack Swift Connector\nCyberduck provides one of the easiest-to-use clients for connecting to Arbutus Object Storage on Windows or MacOS.\nBy default, Cyberduck does not include the OpenStack Swift connector, but it can be easily added within Cyberduck, by navigating to Edit &gt; Preferences.. &gt; Profiles and then checking ‘OpenStack Swift (Keystone 3)’.\n\n\n\nCyberduck - Add OpenStack Swift Connector\n\n\nYou’ll then be able open a new connection to Arbutus using that connector and the Identity API with the following values:\n\nConnector: OpenStack Swift (Keystone 3)\nServer: arbutus.cloud.computecanada.ca\nPort: 5000\nProject:Domain:Username: &lt;your_project_name&gt;:CCDB:&lt;your_Alliance_username&gt;\nPassword: &lt;your_Alliance_password&gt;\n\n\n\n\nCyberduck - Open Connection to Arbutus Object Storage\n\n\n\n\nOpenStack Swift Client\nOpenStack provides an official client for Swift services that can be installed via Python. This tool is not recommended for Windows environments, but it can provide a powerful CLI tool for MacOS and Linux users who are familiar with the Bash shell.\n# Install the openstack-swift CLI tool via Python\npython -m pip install openstack-swift\n# Download OpenStack RC File from the DRA OpenStack Dashboard via Project -&gt; API Access -&gt; Download OpenStack RC File\n# Store your DRA Project credentials as environment variables with the downloaded shell script\nsource &lt;project name&gt;-openrc.sh\n# Start uploading files/directories to your object storage container/bucket\nswift upload &lt;your_container_name&gt; &lt;path_to_directory&gt;",
    "crumbs": [
      "Cloud Computing",
      "Object Storage"
    ]
  },
  {
    "objectID": "cloud-computing/object-storage.html#qgis---accessing-objects-files-in-a-private-containerbucket",
    "href": "cloud-computing/object-storage.html#qgis---accessing-objects-files-in-a-private-containerbucket",
    "title": "Object Storage",
    "section": "QGIS - Accessing Objects (Files) in a Private Container/Bucket",
    "text": "QGIS - Accessing Objects (Files) in a Private Container/Bucket\nWhile any data stored in a public container/bucket can easily be accessed and imported into QGIS using an HTTPS URL, private data can also be accessed given QGIS has access to your object storage credentials.\n\nOpenStack Swift\nSimilarly to using the OpenStack Swift client, you’ll need to download your OpenStack RC file from the dashboard by navigating to Project -&gt; API Access -&gt; Download OpenStack RC File. The RC file will include important and sensitive authentication information so be sure to properly secure it. You will need to extract the environment variables held in this file and enter them into QGIS by opening Settings -&gt; Options… -&gt; System -&gt; Environment. Be sure to check the box to use custom variables and start copying in the variables from the RC file.\n\n\n\nQGIS Settings with custom environment variables\n\n\nOnce you have copied your credentials, you will need to restart QGIS. You can then load a file from your OpenStack container by adding the layer using the OpenStack Swift Object Storage protocol type along with providing the name of the container in which the file is stored and the name of the file as the object key (note: if the file is stored in a subdirectory, the name of the directory will need to be included)\n\n\nS3-Compatible\nThe process for using an S3-compatible object storage provider, is similar to that listed above, but instead of using an OpenStack RC file and environment variables, you will need to set the following custom environment variables in QGIS, which can be attained from any S3-compatible object storage provider.\n\nAWS_SECRET_ACCESS_KEY\nAWS_ACCESS_KEY_ID\n\nFinally, while importing your file as a layer, you will need to use the AWS S3 protocol type.",
    "crumbs": [
      "Cloud Computing",
      "Object Storage"
    ]
  },
  {
    "objectID": "cloud-computing/object-storage.html#commercial-cloud-object-storage-providers",
    "href": "cloud-computing/object-storage.html#commercial-cloud-object-storage-providers",
    "title": "Object Storage",
    "section": "Commercial Cloud Object Storage Providers",
    "text": "Commercial Cloud Object Storage Providers\nIf neither the Alliance Cloud or Chinook provide sufficient features or you plan on using a particular cloud computing service that is only available through a commercial cloud provider, you can access object storage directly through commercial cloud providers.\nContact UBC ARC to get details about accessing credits that can mitigate costs associated with using the following cloud providers.\n\nAWS S3\nGoogle Cloud Storage\nMicrosoft Azure Blob Storage\n\nIf you are working with an organization outside of UBC and/or are unable to access credits to help cover the costs of object storage, the following providers have generous free tiers and generally lower pricing options.\n\nCloudflare R2\nBackblaze B2",
    "crumbs": [
      "Cloud Computing",
      "Object Storage"
    ]
  },
  {
    "objectID": "cloud-computing/deploying-a-dashboard.html",
    "href": "cloud-computing/deploying-a-dashboard.html",
    "title": "Deploying a Dashboard",
    "section": "",
    "text": "This page provides a tutorial on how to deploy an interactive dashboard to an Alliance Cloud virtual machine (VM) using a Jupyter notebook and the Jupyter project, Voilà.\nAdapted from the Voilà Deployment Guide with some opinions.\nRequirements:\n\nAn interactive Jupyter notebook\nStatus as faculty or a librarian with an institution that holds eligibility for CFI grants\nWorking knowledge of UNIX/Linux\nDomain name\n\n\nCanada-based faculty can access cloud computing resources through the Alliance Cloud. The best-suited resource for interactive dashboards is a persistent virtual machine, which acts as a virtual private server. Create an account with Alliance (if you haven’t already done so) and submit a Rapid Access Service request (RAS) with the number of resources that you anticipate using. The following resource recommendations should accommodate a very simple dashboard, but you should request more if your dashboard will be running resource-intensive computations on large datasets.\n\nvCPU: 1\nInstances: 1\nVolumes: 1\nVolume snapshots: 0\nRAM: 1.5\nFloating IPs: 1\nPersistent storage: 20\nObject storage: 0\nShared filesystem storage: 0\n\nOnce your request has been approved, the Alliance Cloud team will provide you with access to an OpenStack dashboard, where you’ll be able to manage your allocated cloud resources. Follow the Alliance’s Cloud Quick Start guide to set up an SSH key pair (if you haven’t already generated one) and launch a persistent virtual machine. We recommend using the Debian image as your boot source. You should also review security considerations with running a VM.\nYou should now have SSH access to your virtual machine. If the Jupyter notebook that you are using for your dashboard is stored in a GitHub repository, you can clone it to your VM. Otherwise you can use scp to copy the notebook along with a requirements.txt or environment.yml file from your local machine to the VM.\n$ sudo apt install git\n$ git clone &lt;your repo URL&gt;\nWhile Voila can be installed via Python and pip, we recommend installing it using mamba and running it from a conda environment. This will improve the reproducibility of your dashboard while also making it easier to maintain by providing more flexibility around Python and other software dependencies.\n$ curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n$ bash Miniforge3-$(uname)-$(uname -m).sh\n$ mamba activate\nIf you’ve included an environment.yml or requirements.txt file alongside your notebook, use it to install any dependencies. Also install voila and ipykernel if needed.\n$ mamba env create -n &lt;project_name&gt; -f environment.yml\n$ mamba install -n &lt;project_name&gt; voila ipykernel\nTo ensure that voila starts running whenever your VM reboots, you’ll need to create and enable a new systemd service. Use the following command to create the service config file.\n$ sudo nano /usr/lib/systemd/system/voila.service\nThen use the following text as a template for your service:\n[Unit]\nDescription=Voila\n\n[Service]\nType=simple\nPIDFile=/run/voila.pid\nExecStart=mamba run -n &lt;project_name&gt; voila --no-browser voila/notebooks/&lt;notebook_file&gt;\nUser=debian\nWorkingDirectory=/home/debian/\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\nNext enable and start the service with the following commands:\n$ sudo systemctl enable voila.service\n$ sudo systemctl start voila.service\nBy default, Voilà starts running from port 8866. Rather than opening this port from your virtual machine’s network firewall through OpenStack Security Groups, we strongly recommend taking the additional step to setup a reverse proxy with a domain name, which will provide your dashboard with better performance and security.\nApache, nginx, and Caddy are all popular options for reverse proxies. We’ll use Caddy in this guide because it’s the easiest one to setup.\nThrough your domain name registrar, create a DNS A record that points your domain name or subdomain to your VM’s floating IP address. Then you’ll need to open up the standard HTTP/HTTPS ports (80 and 443) on your VM’s network firewall through the OpenStack Security Groups. To do this, create a new security group and add both the HTTP and HTTPS rules to it. Navigate to the virtual machine’s interfaces tab and add the new HTTP/HTTPS security group to it alongside the default security group.\nFinally, run the following commands in your virtual machine to install Caddy and start running it as a reverse proxy to Voilà.\n$ sudo apt install caddy\n$ sudo -E caddy reverse-proxy --from &lt;your_domain_name&gt; --to :8866\nNavigate to your dashboard to see it in action!",
    "crumbs": [
      "Cloud Computing",
      "Deploying a Dashboard"
    ]
  },
  {
    "objectID": "cloud-computing/deploying-omeka-classic.html",
    "href": "cloud-computing/deploying-omeka-classic.html",
    "title": "Deploying Omeka Classic",
    "section": "",
    "text": "Omeka Classic is a free and open-source content management system that focuses on supporting the development, curation, and sharing of digital exhibits. Researchers with access to an Omeka Classic instance can upload digital media files alongside a rich set of metadata that can help contextualize and highlight relationships among different collection items.\nThis page provides a tutorial on how to deploy an Omeka Classic instance to an Alliance Cloud virtual machine (VM) with the option to store media files in an Arbutus Object Storage container (bucket). The deployment process is relatively technical and has the following requirements:\nFor alternative options that require less technical knowledge, both Omeka.net and Reclaim Hosting provide affordable shared hosting with Omeka either already installed or installable through an easy one-click process.\nBefore setting up an Omeka Classic instance, it can be helpful to review the Omeka Manual’s Site Planning Tips. This will help you ensure your instance meets the needs of your project while also identifying which features to pull into your instance through added plugins."
  },
  {
    "objectID": "cloud-computing/deploying-omeka-classic.html#installation",
    "href": "cloud-computing/deploying-omeka-classic.html#installation",
    "title": "Deploying Omeka Classic",
    "section": "Installation",
    "text": "Installation\n\nCanada-based faculty can access cloud computing resources through the Alliance Cloud. The best-suited resource for running an ongoing service is a persistent virtual machine, which acts as a virtual private server. Create an account with Alliance (if you haven’t already done so) and submit a Rapid Access Service request (RAS) with the number of resources that you anticipate using. The following resource recommendations should accommodate the needs of most Omeka Classic instances, but you should request more if you anticipate heavy traffic or higher storage needs.\n\nvCPU: 1\nInstances: 1\nVolumes: 1\nVolume snapshots: 0\nRAM: 1.5\nFloating IPs: 1\nPersistent storage: 20\nObject storage: 0\nShared filesystem storage: 0\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe recommendations above provides 20 GB of storage in total to your VM. You can anticipate that about 2.5 GB will be reserved for the Linux OS, leaving you with 17.5 GB of storage for media assets. Omeka instances often require a significant amount of storage, particularly when hosting large, high-quality video files. Adjust your request for persistent storage as needed. If you plan on using more than 50 GB of storage, consider including object storage in your request and using that for as your media storage location. Details about integrating object storage with an Omeka instance are noted below.\n\n\n\nOnce your request has been approved, the Alliance Cloud team will provide you with access to an OpenStack dashboard, where you’ll be able to manage your allocated cloud resources. Follow the Alliance’s Cloud Quick Start guide to set up an SSH key pair (if you haven’t already generated one) and launch a persistent virtual machine. We recommend using the Debian image as your boot source. You should also review security considerations with running a VM.\nYou should now have SSH access to your virtual machine, and you can start installing Omeka’s system dependencies, which includes MariaDB, Apache2, PHP, and ImageMagick, with the following command. We’ll also include Unzip, which will be used to extract the Omeka source code after downloading it from the developer’s releases page.\nsudo apt install mariadb-server libapache2-mod-php php-mysql php-xml imagemagick unzip\nNext we’ll want to lock down MariaDB to improve its security by running the following command and running through the prompts that appear.\nsudo mysql_secure_installation\nNow that the MariaDB installation has been secured, you can start setting up Omeka’s database. Remember to replace &lt;password&gt; with a secure password.\nsudo mysql\n&gt; CREATE USER 'omeka'@'localhost' IDENTIFIED BY '&lt;password&gt;';\n&gt; CREATE DATABASE omeka CHARACTER SET utf8 COLLATE utf8_unicode_ci;\n&gt; GRANT ALL PRIVILEGES ON omeka.* TO 'omeka'@'localhost';\n&gt; exit;\nWith the system dependices installed and a secure database created, we can now download the Omeka source code from GitHub and extract it from the zip archive.\nwget https://github.com/omeka/Omeka/releases/download/v3.1.2/omeka-3.1.2.zip\nunzip omeka-3.1.2.zip\nEnsure Omeka can access its database by modifying the db.ini config file to include the database name and user info. Again, replace &lt;password&gt; with the same password you provided in step 5.\nnano omeka-3.1.2/db.ini\n\n\n~/omeka-3.1.2/db.ini\n\n...\n[database]\nhost        = \"localhost\"\nusername    = \"omeka\"\npassword    = \"&lt;password&gt;\"\ndbname      = \"omeka\"\n...\n\nOmeka relies on an Apache web server to render and serve pages, so we’ll move Omeka’s source code into Apache’s default root directory and update the permissions on the source code files. This will enable Apache to read and execute Omeka’s PHP code.\nsudo mv omeka-3.1.2 /var/www/omeka\nsudo chown -R www-data:www-data /var/www/omeka/files\nsudo chmod -R 755 /var/www/omeka/files\nWe’ll then need to configure Apache, so it starts correctly serving files from the Omeka directory.\nsudo nano /etc/apache2/sites-available/000-default.conf\n\n\n/etc/apache2/sites-available/000-default.conf\n\n...\nDocumentRoot /var/www/omeka\n&lt;Directory \"/var/www/omeka\" &gt;\n  AllowOverride All\n&lt;/Directory&gt;\n...\n\nsudo a2enmod rewrite\nsudo systemctl restart apache2\nThrough your domain name registrar, create a DNS A record that points your domain name or subdomain to your VM’s floating IP address. Then you’ll need to open up the standard HTTP/HTTPS ports (80 and 443) on your VM’s network firewall through the OpenStack Security Groups. To do this, create a new security group and add both the HTTP and HTTPS rules to it. Navigate to the virtual machine’s interfaces tab and add the new HTTP/HTTPS security group to it alongside the default security group.\nOmeka should now be available through HTTP, but to ensure visitors can access it securely, a certificate will need to be acquired to enable HTTPS. Certbot is a great tool that can automate much of the process for you, and step-by-step instructions can be found here.\nFinally, access Omeka through your provided domain name and finalize the installation process by creating your user account. You now have a running Omeka instance can start uploading and describing your materials."
  },
  {
    "objectID": "cloud-computing/deploying-omeka-classic.html#plugins-optional",
    "href": "cloud-computing/deploying-omeka-classic.html#plugins-optional",
    "title": "Deploying Omeka Classic",
    "section": "Plugins (Optional)",
    "text": "Plugins (Optional)\nOmeka provides the following plugins, which can be easily installed from the Admin Plugins pages.\n\nCOinS\nExhibit Builder\nSimple Pages\n\nOmeka also provides a comprehensive listing of additional plugins along with instructions on adding new plugins to your Omeka instance in the Omeka Classic Manual.\nFor many geographers, the Geolocation plugin will be a very helpful one to add, so we’ve included instruction for installing it below:\n\nConnect to your Omeka VM using SSH.\nDownload the Geolocation plugin source code.\nwget https://github.com/omeka/plugin-Geolocation/releases/download/v3.2.3/Geolocation-3.2.3.zip\nunzip Geolocation-3.2.3.zip\nMove the Geolocation source code to Omeka’s plugin directory\nsudo mv Geolocation /var/www/omeka/plugins\nOpen the Omeka Admin Plugins page in your web browser and install the Geolocation plugin. Once the installation is finished you should see a new map tab listed when uploading and describing new items."
  },
  {
    "objectID": "cloud-computing/deploying-omeka-classic.html#object-storage-optional",
    "href": "cloud-computing/deploying-omeka-classic.html#object-storage-optional",
    "title": "Deploying Omeka Classic",
    "section": "Object Storage (Optional)",
    "text": "Object Storage (Optional)\nObject storage, like AWS S3, is often cheaper and easier to request in larger quantities than the traditional persistent storage assigned to a shared hosting account or cloud VM. Reclaim Hosting provides a very helpful piece of documentation on integrating AWS S3 with an Omeka Classic instance running on their shared hosting services. This section adapts that documentation with a focus on using the Alliance Cloud’s Arbutus Object Storage and includes the following requirements:\n\nA Unix or Unix-like shell\nPython and pip\n\nAs note earlier, you’ll need to modify your RAS request to include an allocation for object storage. Once that request has been approved, you’ll be granted access to the Arbutus OpenStack dashboard, where you’ll be able to start creating object storage containers, also known as buckets. We have brief instruction on how to do so here. Note that you’ll need ensure your object storage container is public to ensure uploaded files are available to anonymous users.\nTo connect your Omeka instance to an Arbutus Object Storage container, you’ll need to generate an access key and a secret key using an OpenStack command line tool. These credentials will then be used in your Omeka config file to authenticate your instance to Arbutus Object Storage.\n\nBefore you can start using the OpenStack command line tool, you’ll need to have access to a Unix or Unix-like shell, like BASH. MacOS and Linux users will already have this through their terminal applications while Windows users will need to install it, with the most recommended approach being Git BASH, which is included with Git for Windows.\nNext you’ll download a shell script from the Arbutus OpenStack dashboard, called an OpenStack RC File, which can be found by opening the dropdown under your username in the upper right-hand corner of the OpenStack dashboard. And use the following command to run the OpenStack RC File, which will be used to authenticate the command line tool to manage your allocated resources.\nsource &lt;project-name&gt;-openrc.sh\nThe OpenStack command line tool is distributed as a Python package, so you can install it using Python’s package manager, pip. The following commands use mamba to install the tool into a virtual environment. You can learn more about mamba and virtual environments here.\nmamba create -n openstack\nmamba activate openstack\n(openstack) python -m pip install python-openstackclient\nWith the OpenStack command line tool installed, you can now generate a new set of credentials that can then be used to authenticate your Omeka instance with your Arbutus project.\n(openstack) openstack ec2 credentials create\nWhen the credentials are generated, values will be provided for both an access key and a secret key. These are the equivalent of a username and password for your Alliance project, so record them and keep them secure.\nFinally, you’ll need to modify one of Omeka’s config files to direct your instance to start using object storage by providing your access key, secret key, and the name of your container. Additionally, you’ll need to update the adapter and endpoint using the values listed below.\nnano /var/www/omeka/application/config/config.ini\n\n\n/var/www/omeka/application/config/config.ini\n\n...\n;\nstorage.adapter = \"Omeka_Storage_Adapter_ZendS3\"\nstorage.adapterOptions.accessKeyId = &lt;access key&gt;\nstorage.adapterOptions.secretAccessKey = &lt;secret key&gt;\nstorage.adapterOptions.bucket = &lt;container name&gt;\n; storage.adapterOptions.expiration = 10 ; URL expiration time (in minutes)\nstorage.adapterOptions.endpoint = https://object-arbutus.cloud.computecanada.ca\nstorage.adapterOptions.forceSSL = 1 ; Boolean value (optional)\n\n..."
  },
  {
    "objectID": "development-environments/conda-getting-started.html",
    "href": "development-environments/conda-getting-started.html",
    "title": "Getting Started with Conda, Virtual Environments, and Python",
    "section": "",
    "text": "There are many different ways to install and start using Python on your personal device, and no one way is the correct way. How one installs Python often simply comes down to personal preference. In general, we recommend using a tool called mamba, which has been developed as a faster alternative to the popular package manager, conda, and a lighter alternative to the larger Python distribution that it’s associated with, Anaconda. When performing geospatial computing, you’ll likely be working with multiple projects that depend on not only Python but other languages too, like R, Julia, or JavaScript. You may also find that you projects can’t always run on the same exact version of Python. conda and mamba enable you to easily install and run multiple versions of Python along with a range of other languages and software packages.",
    "crumbs": [
      "Development Environments",
      "Conda - Getting Started"
    ]
  },
  {
    "objectID": "development-environments/conda-getting-started.html#conda-and-mamba",
    "href": "development-environments/conda-getting-started.html#conda-and-mamba",
    "title": "Getting Started with Conda, Virtual Environments, and Python",
    "section": "conda and mamba",
    "text": "conda and mamba\nJust as there is with Python, there are multiple ways to install mamba and conda. For Windows, we recommend using the latest Miniforge3 installer and sticking with the recommended options. Mac OS and Linux users can copy and run the commands listed here within their terminal applications.\nIf you’ve installed mamba on Windows and stuck with the recommended options in the Miniforge installer, the mamba and conda commands will only be available from the Miniforge Prompt application that was included in the installation process. Use this application rather than Command Prompt or PowerShell. If you want to pair mamba with a more functional shell environment, we’ve included instructions for using mamba with a Bash shell on Windows in UNIX Shells and CLIs. For Mac OS and Linux users, the install script adds mamba to the PATH variable thus making it available from a terminal application running a Bash shell.\nOnce you have mamba installed on your machine, follow along with ‘Getting Started With Conda’ to familiarize yourself with some of basic functionalities of mamba/conda. As a reminder, Miniforge includes both tools, and they can be used interchangeably, so use whichever one makes sense to you.",
    "crumbs": [
      "Development Environments",
      "Conda - Getting Started"
    ]
  },
  {
    "objectID": "development-environments/conda-getting-started.html#virtual-environments",
    "href": "development-environments/conda-getting-started.html#virtual-environments",
    "title": "Getting Started with Conda, Virtual Environments, and Python",
    "section": "Virtual Environments",
    "text": "Virtual Environments\nWhen you install mamba with Miniforge, a virtual environment is automatically created for you, which includes an installation of Python along with mamba itself. This is your base environment, and it can give you quick access to Python in a pinch, but to ensure the stability of mamba, you’ll want to avoid making any changes to this environment apart from running the occasional upgrade to mamba. Before getting started with using mamba and Python, it’s important to have an understanding of virtual environments and how mamba works with them to install software and packages onto your machine. If you followed along with the Managing Environments section of ‘Getting Started With Conda’, then you got a very basic introduction to creating and using virtual environments.\nLet’s walkthrough the process again by creating a virtual environment that has a newer version of Python than the one that came pre-installed in the base environment along with the popular geospatial package, GeoPandas.\nTo start, you will need to open a terminal on your machine. If you are using Windows, use the Minforge Prompt as noted before.\nBefore we can start installing Python and other packages, we will need to create our new virtual environment. We will name it py-geo, which will remind us in the future that this is a Python environment with geospatial packages.\n$ mamba create --name py-geo\nNext we’ll want to activate our newly created virtual environment, so we can start interacting with it and installing our packages into it.\n$ mamba activate py-geo\nWhen you activate a virtual environment, the name of the environment will be prepended to your shell prompt, like so:\n(py-geo) $\nNow that we have our virtual environment activated, we can install the Python version that we would like to work with alongside GeoPandas.\n(py-geo) $ mamba install python=3.12 geopandas\nGeoPandas has a decent number of dependencies, many of which other Python geospatial packages also rely on. You will notice that mamba lists those dependencies and asks if you want to proceed with installing them along with Python. This is a good time to ensure you did not enter the install command with a typo and are about to accidentally install the wrong package. If everything looks correct, respond to the prompt to proceed.\nEvery package that you install with mamba includes information within it that lists all of its dependent packages and the range of versions that it will be compatible with. We can use mamba to review what dependencies were included with GeoPandas and Python itself.\n(py-geo) $ mamba list\n# packages in environment at ...\\miniforge3\\envs\\py-geo:\n#\n# Name      Version       Build                 Channel\nattrs       23.2.0        pyh71513ae_0          conda-forge\n...\npython      3.12.4        h889d299_0_cpython    conda-forge\n...\nzstd        1.5.6         h0ea2cb4_0            conda-forge\nYou will notice that I have abbreviated the list quite a bit with ellipses, but you can see that we have Python 3.12.4 installed along with GeoPandas’ many dependencies. You may see newer versions listed when you run this command. In a later step, we will look at how we can upgrade or downgrade Python to another version within this environment.\nTo ensure your virtual environment is reproducible for both yourself and collaborators, it’s important to keep an up-to-date file that lists the packages you used while running your code and store it alongside your code in a version control system, like Git. mamba includes the ability to generate this file for you by taking a snapshot of all the packages installed within an environment and store the name and versions of those packages into a text-based YAML (.yml) file using the following command.\n(py-geo) $ mamba env export --no-builds &gt; environment.yml\nTo test our file, let’s make one more environment using the environment file that we just generated to see how it works, and we will just list our environments again to verify that it was created.\n(py-geo) $ mamba deactivate\n$ mamba env create -n testenv -f environment.yml\n$ mamba info --envs\n# virtual environments:\n#\nbase                   * C:\\Users\\&lt;username&gt;\\miniforge3\npy-geo                   C:\\Users\\&lt;username&gt;\\miniforge3\\envs\\py-geo\ntestenv                  C:\\Users\\&lt;username&gt;\\miniforge3\\envs\\testenv\nNext let us just do a little cleanup by removing the testenv environment that we just created and verify that it is no longer on our machine.\n$ mamba remove -n testenv --all\n$ mamba info --envs\n# virtual environments:\n#\nbase                   * C:\\Users\\&lt;username&gt;\\miniforge3\npy-geo                   C:\\Users\\&lt;username&gt;\\miniforge3\\envs\\py-geo",
    "crumbs": [
      "Development Environments",
      "Conda - Getting Started"
    ]
  },
  {
    "objectID": "development-environments/conda-getting-started.html#python",
    "href": "development-environments/conda-getting-started.html#python",
    "title": "Getting Started with Conda, Virtual Environments, and Python",
    "section": "Python",
    "text": "Python\nNow I want to return to my py-geo environment, and rather than running Python 3.12.4, I want to downgrade my Python version to be one minor release older. We can use the following commands to activate the py-geo environment and then use the search tool to see what Python versions are available. One important thing to note is that conda and mamba can pull packages from different channels, which are managed by different groups and organizations. By default, mamba pulls from the conda-forge channel, which includes the most expansive and up-to-date set of packages. It’s often recommended that you start with the conda-forge channel and stick to it. Using multiple channels in a single environment can cause a range of conflicts that may break your environment.\n$ mamba activate py-geo\n(py-geo) $ mamba search python\nLoading channels: done\n#Name               Version         Build                  Channel\npython              2.7.12          0                      conda-forge\n...\npython              3.12.4          h889d299_0_cpython     conda-forge\nYou will notice that I have abbreviated the list quite a bit with an ellipsis, but I currently have access to a range of Python versions from 2.7.12 to 3.12.4.\nNow which Python version you install will heavily depend on the other packages and software that you intend to use within your environment. You will need to take a close look at what Python versions the packages that you want to use are compatible with.\nIt’s usually a good choice to select one minor version older than the newest version of Python. This ensures a broader level of compatibility with a range of packages. For many package maintainers, it can be quite a bit of work to update their code to make it compatible with the latest minor release of Python, so we can account for this by using an earlier minor release.\nTo downgrade my version of Python from 3.12.4 to 3.11.x, I can run the following command.\n(py-geo) $ mamba install python=3.11\nEvery October, Python releases a new minor version. In a variety of cases, it can be useful to upgrade to the next minor release either to access a new feature or to just keep your environment up-to-date. To update a package with mamba alongside all of its dependencies and the packages that depend on it, you can use the following command:\n(py-geo) $ mamba update python\nAnd voilà, I’m back to Python 3.12.4 and ready to go. A newer version of Python may have been released since writing this, so you might actually see a newer version installed in your environment. And remember to test your code after each package upgrade to ensure that it’s still running as expected.",
    "crumbs": [
      "Development Environments",
      "Conda - Getting Started"
    ]
  },
  {
    "objectID": "development-environments/jupyterhub-installing-packages.html",
    "href": "development-environments/jupyterhub-installing-packages.html",
    "title": "Installing Packages in UBC LT’s Jupyter Open or UBC Syzygy",
    "section": "",
    "text": "When using one of UBC’s remote Jupyter services, you will likely find that JupyterHub’s default environment does not include a range of packages needed to complete your research. While it is possible to install more packages within the default environment using package managers like, Pip, Conda, or Mamba, you may find yourself running into instances where you need to run an unsupported version of Python or R and/or need to install packages that conflict with the packages already included in the default environment. The following instructions provide walkthrough documentation for installing new packages and creating customized kernels within a JupyterHub environment.\nIf you need to share a reproducible Jupyter environment or would like to automate the creation and management of your kernels, copy the Makefile from this repository into your project and follow the instructions found in the repository’s README. Alternatively, if you want to bootstrap a Jupyter kernel with a range of geospatial packages and/or need to share a reproducible environment with others, review and clone the jupyter-py-geog-env repository or use it as a template to create a new repository with any modifications that you may need.",
    "crumbs": [
      "Development Environments",
      "JupyterHub - Installing Packages"
    ]
  },
  {
    "objectID": "development-environments/jupyterhub-installing-packages.html#mamba",
    "href": "development-environments/jupyterhub-installing-packages.html#mamba",
    "title": "Installing Packages in UBC LT’s Jupyter Open or UBC Syzygy",
    "section": "Mamba",
    "text": "Mamba\nIf you are simply running into issues with installing packages via Conda, try using Mamba instead. Mamba is a fast and lightweight alternative to Conda that is already included in JupyterHub’s default environment. It runs more efficiently in computing environments with fewer resources, such as UBC LT’s Jupyter Open or UBC Syzygy.",
    "crumbs": [
      "Development Environments",
      "JupyterHub - Installing Packages"
    ]
  },
  {
    "objectID": "development-environments/jupyterhub-installing-packages.html#virtual-environments",
    "href": "development-environments/jupyterhub-installing-packages.html#virtual-environments",
    "title": "Installing Packages in UBC LT’s Jupyter Open or UBC Syzygy",
    "section": "Virtual Environments",
    "text": "Virtual Environments\nWhile installing new packages within the default environment can initially seem quite convenient. You can often run into issues when attempting to install a package that shares a dependency with one of the packages already installed in the default environment. You will also find that packages that are installed within the default environment do not persist between sessions, requiring you to re-install them whenever you start a new Jupyter session. It is important to note that the default environment does not only contain a set of packages that JupyterHub administrators thought would be frequently useful to users, it also contains packages that run the Jupyter user interface (JupyterLab) and the pre-installed Jupyter kernels. To avoid some common issues, such as conflicts between package dependencies, you can use virtual environments located within your home directory, which help to isolate your computing environment from the default environment thus enabling you to more freely install, manage, and persist your packages.\nThe following instructions use Mamba as a drop in replacement for Conda and can be used for setting up either Python or R-based environments.\n\nNavigate to your preferred JupyterHub service, start a new Jupyter server, and launch a terminal.\nFrom the terminal, create a new virtual environment and install the appropriate kernel package. Replace &lt;environment_name&gt; with a unique name that can be used to easily identify your environment, like a project name. The name will be used to create a new directory in your home directory.\nPython:\n$ mamba create -p &lt;environment_name&gt; ipykernel -y\nR:\n$ mamba create -p &lt;environment_name&gt; r-irkernel -y\nBefore you can activate your virtual environment and start installing packages within it, you will need to initialize Mamba and restart your Bash shell.\n$ mamba init -q && source ~/.bashrc\nNext switch from the default environment to your newly created virtual environment by activating it. Replace &lt;username&gt; with the username displayed in the terminal’s shell prompt. On Open Jupyter, this would be jovyan, while on UBC Syzygy, it would be your CWL username. And again replace &lt;environment_name&gt; with the name you used earlier in Step 2.\n$ mamba activate /home/&lt;username&gt;/&lt;environment_name&gt;\nNow you can begin installing your packages into the virtual environment.\n$ mamba install &lt;package_names&gt; -y\nSome packages that you need may not be available via Mamba’s default package repository, conda-forge, and are only available from the Python Package Index (PyPI) or the Comprehensive R Archive Network (CRAN). To install those packages, you can use Python’s pip or R’s install.packages() with one of the following commands:\n$ python -m pip install &lt;python_package_names&gt;\n$ Rscript -e 'install.packages(c(\"&lt;r_package_names&gt;\"),repo=\"https://mirror.rcg.sfu.ca/mirror/CRAN/\",quiet=TRUE)'`\nWith your virtual environment setup and still active, you will need to register the environment as a Jupyter kernel using the kernel packages installed during the creation of your environment to make it available to your notebooks.\nPython:\n$ python -m ipykernel install --user --name &lt;environment_name&gt; --display-name \"Python (&lt;environment_name&gt;)\"\nR:\n$ Rscript -e 'IRkernel::installspec(name=\"&lt;environment_name&gt;\", displayname=\"R (&lt;environment_name&gt;)\")'`\n\nAfter a few seconds, the custom kernel built from you virtual environment will be listed in the JupyterLab launcher and you will be able to select it as the preferred kernel to run within your notebooks.\nTo install more packages into your environment, repeat steps 3-5 from a JupyterLab terminal.\nIt is important to note that the directories created for each virtual environment can be quite large as more packages are installed within them. Consider purging the environments when they are no longer needed and remember to include the directory in your .gitignore file if you are using Git/GitHub.\nYou can purge an environment from JupyterHub by completing the following steps:\n\nOpen a JupyterLab terminal.\nRemove the kernel from Jupyter.\n$ jupyter kernelspec remove &lt;environment_name&gt; -y\nRemove the virtual environment.\n$ mamba remove -p /home/&lt;username&gt;/&lt;environment_name&gt; --all",
    "crumbs": [
      "Development Environments",
      "JupyterHub - Installing Packages"
    ]
  },
  {
    "objectID": "version-control-systems/jupyterhub-private-repo.html",
    "href": "version-control-systems/jupyterhub-private-repo.html",
    "title": "Working with Private GitHub Repositories from JupyterHub",
    "section": "",
    "text": "This is the recommended method for cloning and pushing commits to a private GitHub repository while working within a JupyterHub server. A fine-grained personal access token, acts as a password for your account and provides a limited set of functionality when working with your GitHub repositories.\nYou can have multiple personal access tokens associated with your GitHub account with various levels of access to your account and specific repositories associated with it. You can generate tokens from within the developer settings of your personal account.\nWhen creating a new token, ensure that you provided it with the minimum privileges necessary to interact with your private repositories via JupyterHub. This includes setting a short expiration date for the token and restricting the token to only work with a specific repository.\n\n\n\nGitHub fine-grained personal access token - name, expiration, description, and repository access\n\n\nAlso set restrictive permissions for the token. If you only need to clone a private repository from JupyterHub, set the ‘Contents’ permissions to ‘Read-only’. Otherwise, if you plan to make changes to any files in the repository and push those changes to GitHub, set the ‘Contents’ permissions to ‘Read and write’.\n\n\n\nGitHub fine-grained personal access token - permissions\n\n\nOnce you click ‘Generate token’, GitHub will display your new personal access token. Be sure to copy and store the token in a secure place. From JupyterLab’s Git extension, you can now provide your GitHub username and your token in place of a password to clone and push changes to token’s associated repository.",
    "crumbs": [
      "Version Control Systems",
      "Working with Private GitHub Repositories from JupyterHub"
    ]
  },
  {
    "objectID": "version-control-systems/jupyterhub-private-repo.html#using-fine-grained-personal-access-tokens",
    "href": "version-control-systems/jupyterhub-private-repo.html#using-fine-grained-personal-access-tokens",
    "title": "Working with Private GitHub Repositories from JupyterHub",
    "section": "",
    "text": "This is the recommended method for cloning and pushing commits to a private GitHub repository while working within a JupyterHub server. A fine-grained personal access token, acts as a password for your account and provides a limited set of functionality when working with your GitHub repositories.\nYou can have multiple personal access tokens associated with your GitHub account with various levels of access to your account and specific repositories associated with it. You can generate tokens from within the developer settings of your personal account.\nWhen creating a new token, ensure that you provided it with the minimum privileges necessary to interact with your private repositories via JupyterHub. This includes setting a short expiration date for the token and restricting the token to only work with a specific repository.\n\n\n\nGitHub fine-grained personal access token - name, expiration, description, and repository access\n\n\nAlso set restrictive permissions for the token. If you only need to clone a private repository from JupyterHub, set the ‘Contents’ permissions to ‘Read-only’. Otherwise, if you plan to make changes to any files in the repository and push those changes to GitHub, set the ‘Contents’ permissions to ‘Read and write’.\n\n\n\nGitHub fine-grained personal access token - permissions\n\n\nOnce you click ‘Generate token’, GitHub will display your new personal access token. Be sure to copy and store the token in a secure place. From JupyterLab’s Git extension, you can now provide your GitHub username and your token in place of a password to clone and push changes to token’s associated repository.",
    "crumbs": [
      "Version Control Systems",
      "Working with Private GitHub Repositories from JupyterHub"
    ]
  },
  {
    "objectID": "version-control-systems/jupyterhub-private-repo.html#using-ssh",
    "href": "version-control-systems/jupyterhub-private-repo.html#using-ssh",
    "title": "Working with Private GitHub Repositories from JupyterHub",
    "section": "Using SSH",
    "text": "Using SSH\n\n\n\n\n\n\nImportant\n\n\n\nThis method is not recommended. While there are multiple precautions taken by UBC LT and Syzygy to isolate and secure your JupyterHub environment, if for any reason your environment were to be compromised, a bad actor could gain full access to your GitHub account.\n\n\nIf you must use this method, ensure you generate a new SSH key on JupyterHub with a strong passphrase. Never reuse/store an SSH private key that you have generated on your local machine within a JupyterHub environment.\nFrom a Terminal in your JupyterHub server, run the following command to generate a new SSH key.\njovyan@jupyter-&lt;your_cwl&gt;:~$ ssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/jovyan/.ssh/id_rsa):\nCreated directory '/home/jovyan/.ssh'.\nEnter passphrase (empty for no passphrase): &lt;enter_a_strong_passphrase_here&gt;\nEnter same passphrase again:\nYour identification has been saved in /home/jovyan/.ssh/id_rsa\nYour public key has been saved in /home/jovyan/.ssh/id_rsa.pub\nTo use your new SSH key, add a public key to your GitHub personal account from the SSH Key Settings page by copying the contents of /home/jovyan/.ssh/id_rsa.pub into the key field.\nFrom JupyterHub, you’ll then be able to clone and push changes to all of your private repositories within GitHub using the SSH protocol.",
    "crumbs": [
      "Version Control Systems",
      "Working with Private GitHub Repositories from JupyterHub"
    ]
  },
  {
    "objectID": "digital-communications/index.html",
    "href": "digital-communications/index.html",
    "title": "Digital Communications",
    "section": "",
    "text": "An open-source content management system that is used to host a wide range of websites and web applications.\n\n\nUBC Blogs is built on top of WordPress. UBC provides a large collection of documentation to help faculty and students get started with using it as a tool for scholarly communications.\n\nUBC Blogs Video Tutorials\nUBC Blogs Instructor Guide\nUBC Blogs FAQ\nWordPress Clinics at UBC CTLT\n\n\n\n\nHaving just a basic familiarity with WordPress’ user interface, is often enough to meet the needs of most use cases. The following resources can provide more-in-depth walkthroughs of WordPress’ for leveraging its more advanced functionalities.\n\nWordPress Documentation\nWordPress 101 – Video Course : UBC Library\nWordPress: The Missing Manual : UBC Library | WorldCat\nWordPress All-in-One for Dummies : UBC Library | WorldCat\nWordPress Styling with Blocks, Patterns, Templates, and Themes : UBC Library | WorldCat\n\n\n\n\nNote on UBC CMS\n\nUBC IT Shared Hosting\nDigital Research Alliance - Cloud - Persistent Instances (VMs)\nReclaim Hosting - Reclaim Press",
    "crumbs": [
      "Digital Communications"
    ]
  },
  {
    "objectID": "digital-communications/index.html#wordpress",
    "href": "digital-communications/index.html#wordpress",
    "title": "Digital Communications",
    "section": "",
    "text": "An open-source content management system that is used to host a wide range of websites and web applications.\n\n\nUBC Blogs is built on top of WordPress. UBC provides a large collection of documentation to help faculty and students get started with using it as a tool for scholarly communications.\n\nUBC Blogs Video Tutorials\nUBC Blogs Instructor Guide\nUBC Blogs FAQ\nWordPress Clinics at UBC CTLT\n\n\n\n\nHaving just a basic familiarity with WordPress’ user interface, is often enough to meet the needs of most use cases. The following resources can provide more-in-depth walkthroughs of WordPress’ for leveraging its more advanced functionalities.\n\nWordPress Documentation\nWordPress 101 – Video Course : UBC Library\nWordPress: The Missing Manual : UBC Library | WorldCat\nWordPress All-in-One for Dummies : UBC Library | WorldCat\nWordPress Styling with Blocks, Patterns, Templates, and Themes : UBC Library | WorldCat\n\n\n\n\nNote on UBC CMS\n\nUBC IT Shared Hosting\nDigital Research Alliance - Cloud - Persistent Instances (VMs)\nReclaim Hosting - Reclaim Press",
    "crumbs": [
      "Digital Communications"
    ]
  },
  {
    "objectID": "digital-communications/index.html#static-site-generators-ssg-web-publishing",
    "href": "digital-communications/index.html#static-site-generators-ssg-web-publishing",
    "title": "Digital Communications",
    "section": "Static Site Generators (SSG) / Web Publishing",
    "text": "Static Site Generators (SSG) / Web Publishing\nAs alternatives to WordPress, static site generators give up easy-to-use interfaces via a content management system and a range of other functionalities to create faster, more reliable, and more secure websites, where content is managed via a set of Markdown files. Additionally, these tools often have smaller sets of themes that users can install to customize the styling of their website, thus requiring a better understanding of HTML, CSS, and JavaScript to develop a more customized look and feel to a website.\nStatically generated sites can be hosted nearly anywhere on the web, including:\n\nDigital Research Alliance - Cloud / Arbutus Object Storage\nGitHub Pages - 1 GB Limit\n\n\nQuarto\nDeveloped by the same company working on RStudio, Quarto provides a powerful tool for publishing scientific and technical work in a range of formats and acts as a successor to R Markdown. Code in Python, R, Julia, and JavaScript can all be represented and rendered along with Pandoc-style Markdown. Interactive visualizations can also be embedded on supported formats using R Shiny, Observable JS, or Jupyter Widgets.\n\n\n\n\n\n\nNote\n\n\n\nIf rendering and exporting HTML files in RStudio using Quarto, ensure you also export the quarto_files directory, which holds necessary CSS and JavaScript files.\n\n\n\nQuarto Webinars\nQuarto Documentation\n\n\n\nR Markdown (R)\nR Markdown provides an authoring framework that enables code to be rendered alongside Markdown and published in multiple formats. See Quarto for a successor to R Markdown.\n\n\n\n\n\n\nNote\n\n\n\nHTML files rendered via R Markdown use inline scripts and CSS, which can create sometimes unnecessarily large files, but they ensure that the file displays consistently without external files.\n\n\n\nR Markdown Documentation\n\n\n\nJekyll (Ruby)\nOne of the most widely used static site generators. This SSG integrates smoothly with GitHub Pages to quickly render websites directly from a GitHub repository. Due to its wide adoption, it also includes a wide range of open-source themes that can be used to customize the style of a website.\n\nUBC Library Research Commons - Introduction to Jekyll\nUBC Library Research Commons - Intermediate Jekyll\nUBC Library Research Commons - Building a project website with Jekyll and GitHub Pages",
    "crumbs": [
      "Digital Communications"
    ]
  },
  {
    "objectID": "digital-communications/index.html#digital-exhibit-tools",
    "href": "digital-communications/index.html#digital-exhibit-tools",
    "title": "Digital Communications",
    "section": "Digital Exhibit Tools",
    "text": "Digital Exhibit Tools\nStoring and presenting digital assets, like images, audio, and video files, can be easily managed via content management systems and static site generators that specialize in digital exhibits.\n\nUBC Library Research Commons - Survey of Digital Exhibit Tools\n\n\nOmeka Classic & Omeka-S\nOmeka Classic is targeted towards small, individually developed projects, while Omeka-S focuses on developing larger scale, institutional-wide repositories with multiple collections or projects. Both are considered web applications as they include content management systems that provide an easy-to-use interface for developing and managing digital exhibits.\n\nUBC Library Research Commons - Building Digital Exhibits with Omeka\nOmeka: A User’s Guide: Introduction\n\n\nCloud Hosting\n\nUBC IT Shared Hosting\nOmeka Classic Documentation\nOmeka-S Documentation\n\n\n\n\nCollectionBuilder\nLacking a content management system, CollectionBuilder centers the development and management of digital exhibits around CSVs, which can be easily created and managed via a spreadsheet editor. Built as a theme around a static site generator (Jekyll), CollectionBuilder creates digital exhibits as easy-to-preserve websites. Paired with the Alliance’s Arbutus Object Storage, CollectionBuilder exhibits provide a fast and customizable method for publicly sharing a variety of media alongside a rich set of metadata. Additionally, CollectionBuilder automatically generates a Leaflet-based web map from any provided geospatial metadata.\n\nCollectionBuilder Documentation\nCollectionBuilder Example - Idaho Queered",
    "crumbs": [
      "Digital Communications"
    ]
  },
  {
    "objectID": "gis-software/index.html",
    "href": "gis-software/index.html",
    "title": "GIS Software",
    "section": "",
    "text": "The ecosystem of GIS software is quite large and complex, with new software being developed everyday. This page focuses on applications that integrate multiple pieces of GIS software to create a single desktop or server application. Resources for other GIS software, including spatial databases can be found here, while resources for geospatial libraries/packages can be found with their associated programming languages.",
    "crumbs": [
      "GIS Software"
    ]
  },
  {
    "objectID": "gis-software/index.html#arcgis-pro",
    "href": "gis-software/index.html#arcgis-pro",
    "title": "GIS Software",
    "section": "ArcGIS Pro",
    "text": "ArcGIS Pro\nDeveloped by Esri, ArcGIS Pro succeeded ArcMap and as the foundation to Esri’s extraordinarily popular ecosystem of GIS software. It is by far the most popular and commonly used commercial GIS desktop application.\nAll UBC Students can access ArcGIS Pro through the UBC Library labs documented in their GIS research guide. Students, who are enrolled in a geography course that requires access to ArcGIS Pro, will also have access to the Geography Computer Labs. Additionally, students may purchase a discounted one-year license for using ArcGIS Pro on their personal computers from the UBC GIS Software page.\nUBC Geography maintains a department license for all ArcGIS software, which is available to all faculty, staff, and researchers.\nArcGIS Pro is developed exclusively around Windows, so running it on a Mac or Linux device requires either the ability to dual boot into a Windows operating system or access to Windows from a virtual environment. Esri provides these instructions for Mac users.\n\nUBC Library Research Commons - Understanding Map Projections\nArcGIS Pro 3.x Cookbook : UBC Library | WorldCat\nLearning ArcGIS Pro 2 : UBC Library | WorldCat\nComputational Methods and GIS Applications in Social Science : UBC Library | WorldCat",
    "crumbs": [
      "GIS Software"
    ]
  },
  {
    "objectID": "gis-software/index.html#qgis",
    "href": "gis-software/index.html#qgis",
    "title": "GIS Software",
    "section": "QGIS",
    "text": "QGIS\nQGIS is a free and open-source GIS desktop application with broad support for Windows, Mac, and Linux operating systems. Its development is supported by the Open Source Geospatial Foundation (OSGeo), which also supports other popular geospatial software, like GDAL, GEOS, GrassGIS, PostGIS, and many others. You can download and install QGIS from their download page or, if you are a UBC Vancouver student, you can access it from one of the labs listed in UBC Library’s GIS Research Guide.\nThe UBC Library Research Commons frequently provides introductory workshops on QGIS for students, faculty, and staff. You can review previous workshop materials below and find upcoming workshops on their calendar.\n\nUBC Library Research Commons - Map Production with QGIS\nUBC Library Research Commons - Network Analysis with QGIS\nQGIS Quick Start Guide : UBC Library | WorldCat\nLearn QGIS : UBC Library | WorldCat\nQGIS: Becoming a GIS Power User : UBC Library | WorldCat",
    "crumbs": [
      "GIS Software"
    ]
  },
  {
    "objectID": "gis-software/index.html#geoserver",
    "href": "gis-software/index.html#geoserver",
    "title": "GIS Software",
    "section": "GeoServer",
    "text": "GeoServer\nGIS servers, like ArcGIS Server, QGIS Server, and GeoServer, enable users to publish and share their maps and geospatial data through web-hosted services. Similar to QGIS, GeoServer has been supported as an OSGeo project. It supports the following standard protocols: Web Feature Service (WFS), Web Map Service (WMS), and Web Coverage Service (WCS) alongside Web Processing Service (WPS) and Web Map Tile Service (WMTS) via added extensions. It also provides the back-end for the geospatial content management system, GeoNode.\nWhile GeoServer can be hosted on a virtual machine through the Alliance Cloud, we strongly recommend that you review and consider deploying cloud-optimized geospatial formats with object storage as an alternative if your use case allows it.\n\nGeoServer Beginner’s Guide : UBC Library | WorldCat\nExpert GeoServer : UBC Library | WorldCat",
    "crumbs": [
      "GIS Software"
    ]
  },
  {
    "objectID": "gis-software/index.html#web-mapping",
    "href": "gis-software/index.html#web-mapping",
    "title": "GIS Software",
    "section": "Web Mapping",
    "text": "Web Mapping\nWeb maps, like OpenStreetMap and Google Maps, provide one of the most effective methods for sharing geospatial data, as they enable users to render, view, and interact with that data directly through their web browsers either via a laptop, desktop, or mobile device without the need to install specialized GIS software.\nExamples:\n\nWayfinding at UBC\nCity of Vancouver - VanMap\nGovernment of BC - BC’s Map Hub\nbikemaps.org\nwalkrollmap.org\n\nThere are a large variety of tools that can assist in developing and distributing web maps. Prior to selecting a tool, be sure to closely analyze your use case and assess whether the tools you are reaching for can best addresses it. For particularly unique use cases, it may be necessary to develop a web map from the ground up using HTML, CSS, and JavaScript. You can find a few resources to assist that work in this site’s JavaScript page.\n\nArcGIS Online\nIndustry and government GIS experts, frequently rely on ESRI’s ArcGIS Online to provide access to their geospatial data via an extraordinarily powerful web mapping interface. Similar to ArcGIS Pro, UBC students and researchers can learn more about getting access to an ArcGIS Online account on the UBC GIS Software page.\n\nESRI - ArcGIS Online - Resources\n\nAn important item to note when assessing this tool for your use cases is that your web map cannot be exported from ArcGIS Online, which means the lifespan of your map will be heavily dependent on ESRI’s servers and your continued subscription to an ArcGIS Online account. If your web map will need to be supported and/or preserved well into the future, consider reviewing a few alternative options. Additionally, while the extensive number of features included in ArcGIS web maps can make them extraordinarily powerful, those same features can also make your map quite bloated, so if you are developing a web map to serve communities where internet access may be limited, consider working with a lighter web mapping tool that can be optimized to load more efficiently.\nAlternatives to ArcGIS Online that may fit similar use cases include: Mapbox and CARTO, which both run popular Software as a Service (SaaS) models for developing and hosting web maps.\n\n\nuMap\nBuilt on OpenStreetMap and open-source software, uMap provides a lighter, but less feature-rich alternative to ArcGIS Online. Public instances are available, which enable users to create and share their web maps for free. If you hope to embed images or other media on your map, checkout the Object Storage page for free/low-cost hosting options.\n\nOSM Wiki- uMap\n\n\n\nQGIS2WEB\nDistributed as a QGIS plugin, this tool enables users to generate web maps as static web sites. HTML, CSS, and JavaScript are exported alongside your data, which can then be hosted like any other static website through object storage or providers like GitHub Pages.\n\nqgis2web Wiki\n\n\n\nGeoNode\nAs noted earlier, GeoNode provides an open-source content management system atop a GeoServer instance, which works well for developing, collaborating, and sharing web maps. Unlike ArcGIS Online and uMap, GeoNode is only available via a self-hosted instance, so requesting a cloud-based virtual machine would be required to setup a server.\n\nGeoNode Documentation\n\n\n\nUshahidi\nDeveloped with a specific focus on creating web maps from crowdsourced data, Ushahidi provides a friendly user interface for quickly creating and distributing forms from which data can be collected and then mapped automatically. Ushahidi can either be self-hosted or managed via a SaaS account, but registering users should be avoided or discouraged in order to avoid collecting any personal identifiable information (PII), which could violate UBC policies and BC privacy regulations.\n\nUshahidi User Manual",
    "crumbs": [
      "GIS Software"
    ]
  },
  {
    "objectID": "development-environments/index.html",
    "href": "development-environments/index.html",
    "title": "Development Environments",
    "section": "",
    "text": "A development environment can often encompass both a code editor or integrated development environment (IDE) and the software necessary to compile and/or run code for debugging and testing purposes. Setting up a development environment is usually an important first step when getting started with a new programming language.\nMost geographers rely on programming languages that are interpreted rather than compiled. Meaning they have to install software that includes a runtime environment that can run their code along with any open-source packages that their code depends on. Packager managers and virtual environment managers or tools that provide a combination of both, like mamba, are extremely important for easing the process of setting up a development environment and ensuring that code can be shared with reproducible results.",
    "crumbs": [
      "Development Environments",
      "Overview"
    ]
  },
  {
    "objectID": "development-environments/index.html#coding-playgrounds",
    "href": "development-environments/index.html#coding-playgrounds",
    "title": "Development Environments",
    "section": "Coding Playgrounds",
    "text": "Coding Playgrounds\nCoding playgrounds are often used for instruction as well for testing code ideas on the fly. They essentially provide access to a pre-built development environment that runs a code editor or IDE in the user’s web browser while feeding code to a runtime environment that’s been installed on an isolated allotment of computing resources on a remote server.\n\nJupyterHub\nThe easiest way for a UBC faculty and student to access a development environment is through a JupyterHub service, like UBC LT’s Open Jupyter or UBC Syzygy. Both playgrounds enable users to connect to and interact with Jupyter Notebooks without having to install anything on their own machines. While convenient, these services come with significant drawbacks in computing power and will not work well in cases where computations are running on large datasets. For more intensive computational work either run JupyterLab on your local machine or access a JupyterHub instance running on one of the Alliance’s HPC clusters.\n\nUBC Open Jupyter\n\nUBC LT - JupyterHub Instructor Guide\n\nResources Per User:\n\n1 CPU\n2.5 GB RAM\n10 GB Storage\n\nSupported Software:\n\nPython 3.11.10\nR 4.3.3\nJulia 1.11.1\nAn assortment of pre-installed Python and R packages\n\nUBC LT provides access to a large collection of software within their JupyterHub instance, Open Jupyter, but many core geospatial packages are not currently installed in the environment. Review JupyterHub - Installing Packages for instructions on setting up alternative Jupyter kernels and installing additional packages.\n\n\nUBC Syzygy\n\nIntroduction to Syzygy - First Steps\n\nResources Per User:\n\n0.5 CPU\n2 GB RAM\n1 GB Storage\n\nSupported Software:\n\nPython 3.12.8\nR 4.4.1\nGDAL 3.9.1\nPROJ 9.4.1\nGEOS 3.12.1\nAn assortment of pre-installed Python and R packages\n\n\n\n\nJupyterLite\nAs an experimental alternative to JupyterHub, JupyterLite similarly runs JupyterLab, but rather than allocate resources on a remote server, JupyterLite leverages WebAssembly (WASM) to install runtime environments and various packages directly in the user’s web browser alongside JupyterLab.\nUBC Geography has setup an experimental deployment of JupyterLite. This is the fastest and easiest method for accessing a local instance of JupyterLab, but it’s also the least stable and lacks many of the same features that would be included in an instance that’s either running on JupyterHub or has been installed locally via pip or mamba, so it’s best used in more limited use cases.\n\nUBC Geography JupyterLite\n\n\n\nGoogle Earth Engine\nProviding a code editor, a basic JavaScript runtime environment, and access to a massive collection of remote sensing data via a well-documented API, Google Earth Engine is a highly-specialized coding playground for running complex analysis on the GEE datasets. To get started, researchers, instructors, and students will need a Google account and a noncommercial Google Cloud Project, which can be setup here. JavaScript - GEE provides resources for getting started with GEE through the code editor and API.\n\n\nGlitch\nSimilar to Google Earth Engine, Glitch provides a JavaScript-based coding playground, but focuses on web development rather than remote sensing. It can provide a quick way of setting up either a static website or a very basic web application. For geographers, Glitch is particularly useful for developing web maps and WebXR content. Glitch also provides a useful feature that enables users ‘remix’ each others projects by using another project as a template for new one.\nGlitch - Help Articles\nGlitch Example - Starter Leaflet",
    "crumbs": [
      "Development Environments",
      "Overview"
    ]
  },
  {
    "objectID": "development-environments/index.html#virtual-environment-managers",
    "href": "development-environments/index.html#virtual-environment-managers",
    "title": "Development Environments",
    "section": "Virtual Environment Managers",
    "text": "Virtual Environment Managers\nVirtual environments are an important tool for isolating multiple Python projects on a single machine, and their use is highly recommended when starting new projects. By isolating your projects, you can ensure that each project is only using the specific Python packages that were identified and installed for running it. This makes the project easier to reproduce while also reducing errors caused by clashing package dependencies.\nFor a very helpful introduction to virtual environments, check out the Environments Tutorial from the Opensource Computing for Earth Science Education (OCESE) Project.\n\nUBC ARC - Virtual Environment 2023 - Videos: Part 1 (virtualenv) & Part 2 (conda)\nGetting Started with Conda, Virtual Environments, and Python\n\n\nConda and Mamba\nconda and it’s faster alternative, mamba, are extremely powerful tools for both managing virtual environments and packages. Using conda install provides the ability to install a range of packages that either aren’t available or can’t be installed via Python’s built-in package manager, Pip. Additionally, Conda enables you to install and manage multiple versions of Python or a range of other programming languages on a single machine using virtual environments.\n\nConda User Guide\nMamba User Guide\n\nIf you need to install mamba and conda on your device, we suggest using the link below to install it using the Miniforge installer.\n\nMiniforge\n\n\n\nPython - venv and virtualenv\nvenv is a lightweight module that has been included with base installations of Python since version 3.3 and enables you to quickly setup and manage virtual environments for your project. virtualenv is a more powerful alternative to venv, but unlike venv it needs to be installed separately via pip. Both modules lack the same level of functionality as conda, but they can be helpful tools for those who are still getting comfortable with Python or don’t need conda’s added functionalities.\n\nvenv Documentation\nvirtualenv Documentation\n\n\n\nR - renv\nR’s equivalent to virtualenv is renv, which itself is a successor to another R-based virtual environment manager, packrat. renv has been developed by the same team behind RStudio and it’s usage is recommended within the RStudio user guide.\n\nIntroduction to renv\nRStudio User Guide - renv",
    "crumbs": [
      "Development Environments",
      "Overview"
    ]
  },
  {
    "objectID": "development-environments/index.html#integrated-development-environments-ide-and-code-editors",
    "href": "development-environments/index.html#integrated-development-environments-ide-and-code-editors",
    "title": "Development Environments",
    "section": "Integrated Development Environments (IDE) and Code Editors",
    "text": "Integrated Development Environments (IDE) and Code Editors\n\nJupyter\nProject Jupyter consists of an ecosystem of open-source projects that support the creation and distribution of computational notebooks. While Jupyter supports computational notebooks with kernels from a wide range of programming languages, it specializes in Python while also providing strong support for R and Julia. Much of the Jupyter ecosystem has been written in and/or runs on Python.\nFor those just getting started with Jupyter, UBC’s COMET Project has developed a great tutorial on creating and editing computational notebooks through JupyterLab.\nFor an in-depth walkthrough of the entire Project Jupyter ecosystem, review the Jupyter Tutorial.\n\nJupyter Notebooks\nThe term Jupyter Notebooks is used interchangeably to refer to the Jupyter Notebook file format for computational notebooks and the user interface that has most commonly been to used to author and edit them. In recent years, the Project Jupyter team has slowly encouraged users to migrate from the classic Jupyter Notebook interface to JupyterLab, its more feature-rich successor.\n\nJupyter Notebook (File) Format Documentation\nJupyter Notebook (User Interface) Documentation\n\n\n\nJupyterLab\nYou can install and run JupyterLab on your local machine in three separate ways. The traditional and most flexible approach is to install JupyterLab as a Python package using pip, conda, or mamba and starting it from the Jupyter command line interface. We recommend the latter using the following commands to create a new environment and install Jupyter along with it’s dependencies within that environment.\n$ mamba create -n jupyter jupyterlab -y\nThen activate the Jupyter environment and start Jupyter Lab.\n$ mamba activate jupyter\n(jupyter) $ jupyter lab\nThis will fire up a Jupyter server and open the JupyterLab interface directly in your web browser.\n\nJupyterLab Documentation\n\nFor those who may be uncomfortable with installing Python and/or using command line interfaces, Project Jupyter has also released a simple-to-use desktop application, named JupyterLab Desktop. This application comes packaged with Python and a default set of packages that are frequently used in scientific computing.\n\nJupyterLab Desktop - Installation\nJupyterLab Desktop User Guide\n\n\n\nJupyter Kernels\nJupyter uses the term kernel to refer to separate programming languages with their own interactive shells. It’s not uncommon to have multiple projects running in different programming languages. This is where kernels can come in handy as they enable you to have multiple, isolated computing environments available within a single IDE.\nWhen you installed JupyterLab with mamba, it came with its own preferred version of Python on which it is dependent. This installation of Python is immediately added as your default Jupyter kernel.\nand for R it is r-irkernel. You can find a comprehensive list of packages for various programming languages here.\n\nPython\nYou are more than welcome to start installing Python packages into the Jupyter virtual environment to make them available to the default Python kernel and the Jupyter notebooks that are running off that kernel, but we recommend taking the additional step of installing another kernel that runs in a separate environment from Jupyter. Yes, that means that you’ll have at least three different installations of Python running on your machine if you are using and that might feel a bit redundant, but the additional isolation and flexibility will be well worth it as they’ll ensure that you can customize your Jupyter environment with all the extensions you want alongside a Python environment that can run any package you need on whatever Python version you want (only stable and maintained versions please). This comes with the added benefit of being able to export and share your conda environment and ensuring that those you share it with only install the packages that they need to run your code.\nOpen a new terminal either in JupyterLab or a shell environment with access to mamba. Then create a new virtual environment with the version of Python that you’d like to install along with ipykernel.\n$ mamba create -n python_env python=3.12 ipykernel -y\nNext, use ipykernel to install the virtual environment as a Jupyter kernel.\n$ mamba run -n python_env python -m ipykernel install --user --name python_env --display-name \"Python (python_env)\"\nLaunch JupyterLab, if it isn’t already running, and you should see the python_env environment listed as a kernel in the launcher.\n\n\nR\nSimilar to Python, you have the option of running an R kernel within the same environment as Jupyter or from an entirely separate one. Unfortunately, setting up an R kernel can be a little more complex, so the recommended approach is to run your R kernels alongside Jupyter within their own separate virtual environments. This loses some of the convenience of operating within a single JupyterLab setup or alternatively running within an R-centered IDE, like RStudio, but it comes with the key benefit of enabling your R code to be shared in a larger Jupyter-based ecosystem.\n$ mamba create -n r_env r-base r-irkernel jupyter -y\n$ mamba run -n r_env Rscript -e \"IRkernel::installspec(name='r_env',displayname='R (r_env)')\"\n$ mamba run -n r_env jupyter lab\nIf you are keen to attempt to add an R kernel to an exiting Jupyter environment and don’t mind running into a few issues, you can try the following set of commands.\nCreate your R-based conda environment, which will be hosting the kernel, and activate it.\n$ mamba create -n r_env r-base r-irkernel -y\n$ mamba activate r_env\nThen from a Bash environment copy the kernelspec directory that is included with the r-irkernel package. This directory holds a default kernel config that is used when installing kernels in Jupyter.\n$ cp -r $CONDA_PREFIX/lib/R/library/IRkernel/kernelspec r_env_kernel\nFirst run one sed command on the Jupyter kernel config file that replaces the command that starts the default installation of R with a command that points to the installation of R running in your virtual environment.\n\nWindowsMac OS or Linux\n\n\nBe sure to replace &lt;user_name&gt; with your Windows username.\n$ sed -i 's/\"R\", /\"C:\\\\\\\\Users\\\\\\\\&lt;user_name&gt;\\\\\\\\miniforge3\\\\\\\\Scripts\\\\\\\\mamba.exe\", \"run\", \"-n\", \"r_env\", \"R\", /' r_env_kernel/kernel.json\n\n\n$ sed -i 's/\"R\", /\"mamba\", \"run\", \"-n\", \"r_env\", \"R\", /' r_env_kernel/kernel.json\n\n\n\nThen update the display name used in the Jupyter launcher to match your environment name.\n$ sed -i 's/\"display_name\":\"R\"/\"display_name\":\"R (r_env)\"/' r_env_kernel/kernel.json\nDeactivate your environment.\n$ mamba deactivate\nFinally, install the kernel into Jupyter and cleanup by deleting the temporary kernel config directory.\n$ mamba run -n jupyter jupyter kernelspec install r_env_kernel --user\n$ rm -rf r_env_kernel\nWhen you start up JupyterLab, the new R kernel should be listed in the launcher.\n\n\n\nHelpful Extensions\n\nRendering\n\nGeoJSON Extension\n\n\n\nGit & GitHub\n\nGit Extension\nGitHub Extension\nGit and Jupyter Notebooks: The Ultimate Guide\n\n\n\nLinting, Formatting, and Benchmarking\n\nLanguage Server Protocol Extension\nCode Formatter Extension\nExecution Time Extension\nSystem Monitor Extension\n\n\n\nSpellchecking\n\nSpellchecker Extension\n\n\n\nAI\n\nGenerative AI Extension\n\n\n\n\nGeospatial Libraries\n\nipyleaflet Documentation\n\n\n\n\nRStudio\nWhile JupyterLab excels at providing an intuitive environment for creating and editing Python-based computational notebooks, RStudio provides a similarly intuitive environment for working with the R programming language. It provides useful tools for writing R scripts, interacting with the R console, or developing R-based computational notebooks with Quarto or RMarkdown.\nYou can find installers for multiple operating systems here.\nWhen starting a new R project in RStudio, always checkmark ‘Use renv with this project’. This will ensure that you create an isolated and reproducible environment similar to those that are produced with mamba/conda or Python’s venv.\n\nRStudio UserGuide\n\n\n\nVisual Studio Code\nVisual Studio Code (VS Code) is a more general-purpose programming environment compared to JupyterLab and RStudio that excels for writing scripts and programs in Python, JavaScript, Julia, and a broad range of other programming languages. VS Code is also capable of running computational notebooks through well-supported extensions, but it can feel less intuitive for some users.\n\nUBC Library Research Commons - VS Code Overview in Setting Up a Dev Environment\nVS Code Documentation\nUsing Git source control in VSCode\nVisual Studio Code Distilled: Evolved Code Editing for Windows, macOS, and Linux\n\n\nHelpful Extensions\n\nRemote Development\n\nRemote Development Extension Overview\n\n\n\nJupyter\n\nJupyter Extension Overview\n\n\n\nQuarto\n\nQuarto Extension Overview\n\n\n\nPython\n\nPython in VS Code Documentation\nPython Extension Overview\n\n\n\nR\n\nR in VS Code Documentation\nR Extension Overview",
    "crumbs": [
      "Development Environments",
      "Overview"
    ]
  },
  {
    "objectID": "r/index.html",
    "href": "r/index.html",
    "title": "R",
    "section": "",
    "text": "Alongside Python, R is one of the commonly used programming languages in geography research due in part to the large collection of geospatial packages that have been developed with it along with it’s broad adoption for scientific computing.\nThe easiest and most recommended approach for getting started with R is to download the installer from CRAN:\nThe following resources will provide general introductions to R and its applications within data science, so any of them can be useful for learning the basics of R.",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "r/index.html#geospatial-books",
    "href": "r/index.html#geospatial-books",
    "title": "R",
    "section": "Geospatial Books",
    "text": "Geospatial Books\n\nGeographic Data Science with R: Visualizing and Analyzing Environmental Change\nLearning R for Geospatial Analysis : UBC Library | WorldCat\nIntroduction to R for Spatial Data Science\nSpatial Data Science with Applications in R\nGeocomputation with R : UBC Library | WorldCat\nRemote Sensing and Digital Image Processing with R : UBC Library | WorldCat\nSurveying with Geomatics and R : UBC Library | WorldCat\n\n\nExtending GIS Software\n\nHands-On Geospatial Analysis with R and QGIS : UBC Library | WorldCat",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "r/index.html#geospatial-librariespackages",
    "href": "r/index.html#geospatial-librariespackages",
    "title": "R",
    "section": "Geospatial Libraries/Packages",
    "text": "Geospatial Libraries/Packages\nA relatively comprehensive list of packages can be found in the R section of Awesome Geospatial.\nFor a closer analysis on some of the most popular R packages for spatial data analysis, consider reviewing the following journal article.\n\nR Packages for Analyzing Spatial Data: A Comparative Case Study with Areal Data\n\nAdditionally, you can identify packages from the following CRAN task views.\n\nCRAN Task View: Analysis of Ecological and Environmental Data\nCRAN Task View: Hydrological Data and Modeling\nCRAN Task View: Analysis of Spatial Data\nCRAN Task View: Handling and Analyzing Spatio-Temporal Data",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "r/index.html#interactive-visualizations-and-dashboards",
    "href": "r/index.html#interactive-visualizations-and-dashboards",
    "title": "R",
    "section": "Interactive Visualizations and Dashboards",
    "text": "Interactive Visualizations and Dashboards\nDashboard libraries enable you to create interactive visualizations from your R code, which can be hosted on sparsely-resourced servers and shared as web applications.\n\nShiny for R - Getting Started\nShiny for R - Leaflet: Interactive Web Maps with R\nMastering Shiny : UBC Library | WorldCat\nDash R User Guide\nDash - Plotly Maps",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "r/index.html#increasing-performance",
    "href": "r/index.html#increasing-performance",
    "title": "R",
    "section": "Increasing Performance",
    "text": "Increasing Performance\nSimilar to Python, R isn’t considered a particularly fast programming language. For many geography-based researchers, development workflows first focus on developing code that performs accurate analysis and then later improving the code as needed to ensure the analysis runs as efficiently as possible in both local and HTC environments before finally adapting code to work in an HPC environment if needed.\n\nIntroduction to high-performance research computing in R\nHigh-Performance Computing with R\nCRAN Task View: High-Performance and Parallel Computing with R\nfastverse: A Suite of High-Performance Packages for Statistics and Data Manipulation\nFutureverse: A Unifying Parallelization Framework in R for Everyone",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "r/index.html#code-quality-tools",
    "href": "r/index.html#code-quality-tools",
    "title": "R",
    "section": "Code Quality Tools",
    "text": "Code Quality Tools\n\nFormatting\nSometimes writing code can get a bit messy. Formatters, like styler, can automatically reformat your code to make it cleaner and easier to read while following a set of standards and best practices.\n\n\n\n\n\n\nNote\n\n\n\nRStudio includes a formatting tool via Code &gt; Reformat Code\n\n\n\nstyler Documentation\n\n\n\nLinting\nUsing a static analysis tool, or linter, is a common best practice among programmers that helps identify mistakes when writing code by ensuring that you follow the correct syntax and a guiding set of best practices.\n\nlintr Documentation\n\n\n\nTesting Framework\nSimilar to type checking, unit testing can be a helpful tool when writing large and complex scripts or programs. Testing frameworks enable you to define tests that can run over your functions and ensure they are following expected behavior in a range of practical scenarios.\n\ntestthat Documentation\n\n\n\nBenchmarking\nWhen writing code, there is often a wide array of ways in which a task can be completed. Benchmarking your functions can help you find the most efficient approach possible. This is particularly important when writing scripts and programs that will be used on large datasets and/or on high performance computers.\n\nbench Documentation",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "julia/index.html",
    "href": "julia/index.html",
    "title": "Julia",
    "section": "",
    "text": "Compared to Python and R, Julia is a relatively young language that is slowly seeing increased usage among various research fields. In relation to geospatial computing, the language lacks the same breadth of packages that can be found in Python or R, but it’s increased performance and support for parallelization may make it useful when analyzing extraordinarily large datasets, especially in HPC environments.",
    "crumbs": [
      "Julia"
    ]
  },
  {
    "objectID": "julia/index.html#geospatial-librariespackages",
    "href": "julia/index.html#geospatial-librariespackages",
    "title": "Julia",
    "section": "Geospatial Libraries/Packages",
    "text": "Geospatial Libraries/Packages\nA relatively comprehensive list of packages can be found in the Julia section of Awesome Geospatial.",
    "crumbs": [
      "Julia"
    ]
  },
  {
    "objectID": "julia/index.html#interactive-visualizations-and-dashboards",
    "href": "julia/index.html#interactive-visualizations-and-dashboards",
    "title": "Julia",
    "section": "Interactive Visualizations and Dashboards",
    "text": "Interactive Visualizations and Dashboards\nDashboard libraries enable you to create interactive visualizations from either Julia code or Jupyter Notebooks, which can be hosted on sparsely-resourced servers and shared as web applications.\n\nPlotly - Dash Julia User Guide\nGenie Framework Documentation",
    "crumbs": [
      "Julia"
    ]
  },
  {
    "objectID": "julia/index.html#code-quality-tools",
    "href": "julia/index.html#code-quality-tools",
    "title": "Julia",
    "section": "Code Quality Tools",
    "text": "Code Quality Tools\n\nFormatting\nSometimes writing code can get a bit messy. Formatters can automatically reformat your code to make it cleaner and easier to read while following a set of standards and best practices.\n\nJuliaFormatter.jl\n\n\n\nTesting\nUnit testing can be a helpful tool when writing large and complex scripts or programs. Julia enables you to define tests that can run over your functions and ensure they are following expected behavior in a range of practical scenarios via its standard library.\n\nJulia Documentation - Unit Testing",
    "crumbs": [
      "Julia"
    ]
  },
  {
    "objectID": "version-control-systems/index.html",
    "href": "version-control-systems/index.html",
    "title": "Version Control Systems",
    "section": "",
    "text": "Implementing version control is a commonly used best practice among programmers, and its usage is strongly encouraged for researchers whether they are working with large codebases or small Jupyter Notebooks. Version control is especially helpful when collaborating with other researchers or developers as it enables changes to be tracked coherently and new code to be explored separately from a stable, main codebase.",
    "crumbs": [
      "Version Control Systems",
      "Overview"
    ]
  },
  {
    "objectID": "version-control-systems/index.html#code",
    "href": "version-control-systems/index.html#code",
    "title": "Version Control Systems",
    "section": "Code",
    "text": "Code\n\nGit\nGit is by far the most popular version control system and comes with a powerful command line interface while integrating with code sharing and development platforms, like GitHub and GitLab.\nUBC Library Research Commons frequently runs introductory workshops on Git and GitHub, which are listed here. And the materials for these workshops are available below:\n\nUBC Library Research Commons - Introduction to Git and GitHub\n\nAdditionally, the SFU’s Research Computing Group provides a full-day workshop on Bash during their annual Summer School in early June with some of the materials for that course available below:\n\nDigital Research Alliance - Git Tutorial and Workshops\n\nAdditional Resources:\n\nSoftware Carpentry - Version Control with Git\nPro Git\nBeginning Git and GitHub: Version Control, Project Management and Teamwork for the New Developer : UBC Library | WorldCat\nMastering Git : UBC Library | WorldCat\n\n\n\nGitHub\nGitHub is a popular platform for sharing source code and Jupyter Notebooks as Git repositories. These can either be public and shared broadly with other users to copy (clone) or private for restricted sharing among invited users. UBC LT provides access to GitHub Enterprise which can securely share repositories among students within a single course or with other UBC collaborators.\n\nGitHub Documentation\nGitHub for Next-Generation Coders : UBC Library | WorldCat\nUBC LT - GitHub Instructor Guide\n\nGitHub Desktop provides a helpful graphical user interface that can help in managing local Git repositories and pushing changes to remote repositories on GitHub.\n\nGitHub Desktop Documentation\n\nAn additional benefit to storing your code in a public repository on GitHub is that your code can easily be archived into Zenodo, where it will receive a digital object identifier (DOI). This then makes it extraordinarily easy for your code to be cited and accessed by other researchers. If later you make revisions or improvements to your code, you can cut a new release and Zenodo will automatically update your archive and assign a new DOI.\n\nGitHub Documentation - Referencing and Citing Content\n\nWhile GitHub excels at displaying code on the web, it also supports some other interesting functionalities relevant to geospatial computing. For example, if you don’t mind storing your tabular data in CSV or GeoCSV, GitHub will render the data to an interactive table, which can be easily searched and edited.\n\nGitHub Documentation - Rendering CSV and TSV Data\n\nYou can also quickly generate basic web maps directly within your repository by storing your vector data in the GeoJSON or TopoJSON format. GitHub will render the vector data with Azure Maps and Leaflet.js. The generated web map can then be embedded on different sites with a simple snippet of JavaScript.\n\nGitHub Documentation - Mapping GeoJSON/TopoJSON files on GitHub\nGitHub GeoJSON Web Map Example\n\n\n\nIntegrations\n\nJupyterLab\n\nHow to Use the JupyterLab Git Extension\nJupyterLab-Git README\nWorking with Private GitHub Repositories from JupyterHub\n\n\n\nRStudio\n\nGitHub and RStudio\nHappy Git and GitHub for the UseR\n\n\n\nVS Code\n\nVS Code Extensions - Remote Repositories",
    "crumbs": [
      "Version Control Systems",
      "Overview"
    ]
  },
  {
    "objectID": "version-control-systems/index.html#data",
    "href": "version-control-systems/index.html#data",
    "title": "Version Control Systems",
    "section": "Data",
    "text": "Data\nWhile version control systems are most commonly applied to code, to further ensure the reproducibility of their work, researchers have been developing and improving systems that can work fluently with their data as well. These systems are often built to work atop or adjacent to Git while adding functionality to manage and store large datasets via Git LFS or object storage.\n\nThe Turing Way - Version Control for Data\n\n\nKart\nKart is a tool that has been built on top of Git and extends its functionality to work with vector datasets while also integrating with Git LFS (Large File Storage) to handle large raster datasets. The Kart developers also provide a QGIS plugin to ensure easy integration.\n\nKart Documentation\nKart QGIS Plugin\n\n\n\nDataLad\nDataLad provides a general-purpose data version control system that supports a broad set of storage options including Microsoft OneDrive, DRA’s Arbutus object storage (OpenStack Swift), and a range of other S3-compatible object storage providers.\n\nDatalad: The Handbook\nWestDRI - Distributed File Storage with git-annex\nWestDRI - Data Management with DataLad\nWestDRI - Distributed Datasets with Datalad\n\n\n\nDVC\nWhile applicable to a range of other data-intensive tasks, DVC is a data version control system that specializes in machine learning. It supports fewer storage options compared to Datalad, but can provide a smoother integration and setup experience in certain applications.\n\nDVC Documentation\nWestDRI - Version Control for Data Science and Machine Learning with DVC",
    "crumbs": [
      "Version Control Systems",
      "Overview"
    ]
  },
  {
    "objectID": "cloud-computing/index.html",
    "href": "cloud-computing/index.html",
    "title": "Cloud Computing",
    "section": "",
    "text": "Similar to high performance computing (HPC), cloud computing provides hardware for running resource-intensive computations, but while HPC focuses on completing a complex set of tasks as quickly as possible with an extraordinarily powerful set of resources, cloud computing has been traditionally dedicated to providing hardware through virtual machines (VM) of varying specifications over a long period of time in order to support ongoing services such as web applications and databases.",
    "crumbs": [
      "Cloud Computing",
      "Overview"
    ]
  },
  {
    "objectID": "cloud-computing/index.html#digital-research-alliance-dra",
    "href": "cloud-computing/index.html#digital-research-alliance-dra",
    "title": "Cloud Computing",
    "section": "Digital Research Alliance (DRA)",
    "text": "Digital Research Alliance (DRA)\nCanada-based faculty and librarians with DRA accounts can create a VMs at any time of the year by submitting a Rapid Access Service (RAS) request. If the RAS resource limits noted below and in more detail here are not sufficient, additional resources can be requested through the annual Resource Allocation Competition (RAC) with applications due between late September and early November. Upon approval, RAC resources are granted the following April.\n\nDRA - Cloud\n\nThe DRA Cloud provides access two different types of VMs with different use cases in mind, which are listed below.\n\nCompute Instances\nCompute VMs are ephemeral and provide a middle-ground between running batch jobs on the DRA HPC clusters and Persistent VMs. They can provide a powerful solution when using software that cannot be ran on the HPC clusters either natively or via Apptainer or when running computations that can run for days and/or weeks.\nFor geography-related research and instruction, these instances can be particularly useful for the following use cases:\n\nSpinning up a customized JupyterHub instance and/or RStudio server for a workshop or class\nWorking with massive datasets in a desktop application, such as QGIS, AliceVision, or Blender, where access to a GPU can be significantly beneficial\n\n\nRAS Resource Limits\n\n# of VMs: 20\nvCPUs: 80\n\n16 per VM\n\nRAM: 300 GB\n\n180 GB per VM\n\nStorage: 10 TB\nvGPU: 16 GB of memory on an Nvidia V100\n\n\n\n\nPersistent Instances\nThese VMs are ideal for running continuous computations, like web crawlers, and ongoing services, like web servers. It’s important to remember that services cannot be ran on the DRA Cloud indefinitely, so ensure you have a plan for your VM’s end-of-life (EOL) along with a set of clear security procedures before creating it. The DRA requires that researchers reassess their cloud services annually in April by submitting a form that restates the goals of the project and necessary resources for continued use.\nFor geography-related research and instruction, these instances can be particularly useful for the following use cases:\n\nHosting a PostgreSQL database with PostGIS\nRunning a web service with software like GeoServer, GeoNode, Ushahidi, uMap (OpenStreetMap), or QGIS server.\nDeveloping dynamic digital exhibits or publications with Omeka and/or Scalar.\n\nAgain, when working with persistent VMs, consider any implications that may come from preserving your work once the VM is no longer available by asking yourself whether your work can be easily exported, web archived, and/or hosted externally with minimal resources and no long-term maintenance.\n\nRAS Resource Limits\n\n# of VMs: 10\nvCPUs: 25\n\n16 per VM\n\nRAM: 50 GB\n\n32 GB per VM\n\nStorage: 10 TB",
    "crumbs": [
      "Cloud Computing",
      "Overview"
    ]
  },
  {
    "objectID": "cloud-computing/index.html#ubc-it",
    "href": "cloud-computing/index.html#ubc-it",
    "title": "Cloud Computing",
    "section": "UBC IT",
    "text": "UBC IT\nIn some cases, the DRA Cloud may not be the best option for a research project. In those cases, UBC IT can provide some budget-friendly alternatives. For minimal LAMP-stack web applications, like WordPress, Drupal, or Omeka, UBC IT provides simple shared hosting solutions, while more resource-intensive applications and computation tasks can run on EduCloud VMs. Additionally, UBC IT has begun a broker system that can enable resources to be requested from large commercial cloud providers like Amazon, Microsoft, and Google with additional support.\n\nUBC IT - EduCloud Server Service\nUBC IT - Hybrid Cloud Service",
    "crumbs": [
      "Cloud Computing",
      "Overview"
    ]
  },
  {
    "objectID": "cloud-computing/index.html#commercial-cloud-computing-providers",
    "href": "cloud-computing/index.html#commercial-cloud-computing-providers",
    "title": "Cloud Computing",
    "section": "Commercial Cloud Computing Providers",
    "text": "Commercial Cloud Computing Providers\nAlong with UBC IT’s Hybrid Cloud Services, resources from commercial cloud computing providers can be acquired with funding and credit opportunities via UBC Advanced Research Computing (ARC). Prior to using a commercial cloud computing provider, contact UBC ARC for consultation as they can provide guidance in selecting the best provider for your project and identifying any security concerns that may be applicable. This consultation can be crucial when navigating the massive and overwhelming number of services/options presented by commercial providers.\n\nAmazon Web Services (AWS)\nTo further support researchers in using allocated credits on AWS, UBC ARC provides access to RONIN, a web application that can simplify and increase the accessibility of clouding computing services.\n\nUBC ARC RONIN\nGeospatial Data Analytics on AWS\nAWS - Geospatial ML with Amazon SageMaker\n\n\n\nMicrosoft Azure\nAzure integrates smoothly with Esri software, making it a strong option for research that is heavily dependent on ArcGIS.\n\nAzure Maps\nAzure - Geospatial Data Processing and Analytics\nAzure - End-To-End Geospatial Storage, Analysis, and Visualization\n\n\n\nGoogle Cloud Platform (GCP)\n\nGoogle Cloud - Geospatial Analytics",
    "crumbs": [
      "Cloud Computing",
      "Overview"
    ]
  },
  {
    "objectID": "xr/index.html",
    "href": "xr/index.html",
    "title": "XR - Virtual Reality / Augmented Reality",
    "section": "",
    "text": "Extended reality (XR), which encompasses virtual reality (VR), augmented reality (AR), and mixed reality (MR), is a collection of rapidly growing technologies with an increasing number of applications being researched and deployed in the field of geography.",
    "crumbs": [
      "XR - Virtual Reality / Augmented Reality"
    ]
  },
  {
    "objectID": "xr/index.html#content",
    "href": "xr/index.html#content",
    "title": "XR - Virtual Reality / Augmented Reality",
    "section": "Content",
    "text": "Content\n\nGeography-related\n\nUBC Geography Tours - AR\nUBC Geography - Stanley Park Virtual Tour\nCamosun Bog 360\nBiogeography Teaching and Research Lab - Virtual Field Trips\nThe Stanford Ocean Acidification Experience\nGoogle Earth VR\nGoogle Arts & Culture - Exhibitions\nUsability of WebXR Visualizations in Urban Planning - VR Examples\n\n\n\nOthers\n\nWebXR Samples\nA-Frame Examples\nBabylon.js Community Examples\nPlayCanvas Games Demos\nSteam VR Games",
    "crumbs": [
      "XR - Virtual Reality / Augmented Reality"
    ]
  },
  {
    "objectID": "xr/index.html#development-tools-and-frameworks",
    "href": "xr/index.html#development-tools-and-frameworks",
    "title": "XR - Virtual Reality / Augmented Reality",
    "section": "Development Tools and Frameworks",
    "text": "Development Tools and Frameworks\nXR development has and continues to be a complex and specialized field often requiring in-depth knowledge of game development, 3D modelling, and programming languages like C# or JavaScript. When developing content for XR, identify the goals of your project and consult UBC’s Emerging Media Lab to get assistance with setting a clear scope and connecting with partners.\n360 images and video are often a good place to get started with VR content as they can be easier to create and accessible to wider audiences while still providing an immersive experience.\nFor AR, review platforms, like Echoes, which provide interfaces and support for developing and distributing virtual tours.\n\nWebXR\nWebXR is a new set of standards that enable VR and AR content to be rendered via web browsers. In some browsers, like Firefox and Safari, WebXR is still flagged as experimental, but it can be enabled through adjustments in settings.\n\nAR and VR Using the WebXR API: Learn to Create Immersive Content with WebGL, Three.js, and A-Frame : UBC Library | WorldCat\n\nDifferent frameworks have been in development over the past few years to help simplify developing VR content with WebXR and its now obsolete predecessor, WebVR.\n\nA-Frame - A popular, HTML and JavaScript-based framework for developing web-based VR content.\nBabylon.js A JavaScript-based framework for developing web-based VR Content\n\nGoing the Distance with Babylon.js : UBC Library | WorldCat\n\n\n\n\nUnity\nUnity provides provides a powerful editor game engine for developing AR and VR applications. Multiple software development kits have been developed to extend its functionality and cohesively work with GIS software.\n\nA Guide to Capturing and Preparing Photogrammetry for Unity\nUnity Development Cookbook : UBC Library | WorldCat\nUnity Virtual Reality Development with VRTK4 : UBC Library | WorldCat\n\n\nRelevant Plugins and SDK\n\nArcGIS Maps SDK for Unity\nMapBox Maps SDK for Unity\nCesium for Unity Quickstart\n\n\n\n\nGodot\nWhile lacking the maturity, ease-of-use, and breadth of GIS integrations compared to Unity, the Godot game engine is an increasingly popular choice for AR and VR development in both commercial and academic settings due to it being a powerful, free and open-source option. LandscapeLab provides an excellent example of how the engine can be applied within a geography-centered setting.\n\nGodot Engine Documentation\nGodot 4 Game Development Projects : UBC Library | WorldCat\nGodot 4 Game Development Cookbook : UBC Library | WorldCat\nGame Development with Blender and Godot : UBC Library | WorldCat\n\n\nRelevant Plugins and SDK\n\nGeodot-Plugin README\nHTerrain Plugin Documentation\n\n\n\n\nBlender\nBlender is a widely used piece of free and open source software with a range of applications, which include developing and editing 3D models, which can easily be exported to the VR development tools listed above.\n\nBlender All-in-One for Dummies : UBC Library | WorldCat\nBlender Reference Manual\n\n\nRelevant Plugins and SDK\n\nBlenderGIS\n\n\n\n\nOpenBrush\n\nOpenBrush Documentation\n\n\n\nOther Tools\n\nDEM.Net Elevation API - Quickly generate 3D Models from Open Street Map data\nQgis2threejs Plugin - Export DEM data in QGIS to 3D models",
    "crumbs": [
      "XR - Virtual Reality / Augmented Reality"
    ]
  }
]